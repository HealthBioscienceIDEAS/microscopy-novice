<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Image Analysis for Microscopy: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="manifest" href="../site.webmanifest">
<link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md navbar-light bg-white top-nav IDEAS"><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-6">
      <div class="large-logo">
        <img alt="IDEAS" src="../assets/images/IDEAS-logo.svg"><abbr class="badge badge-light" title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught." style="background-color: #FF4955; border-radius: 5px">
          <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link" style="color: #000">
            <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
            Pre-Alpha
          </a>
          <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
        </abbr>
        
      </div>
    </div>
    <div class="selector-container">
      
      
      <div class="dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl navbar-light bg-white bottom-nav IDEAS" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="IDEAS" src="../assets/images/IDEAS-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Image Analysis for Microscopy
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
      <i role="img" aria-label="search button" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Image Analysis for Microscopy
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
<input class="form-control me-2 searchbox" type="search" placeholder="Search" aria-label="Search"><button class="btn btn-outline-success tablet-search-button" type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="search button"></i>
        </button>
      </fieldset>
</form>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Image Analysis for Microscopy
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress IDEAS">
    <div class="progress-bar IDEAS" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->
      
            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="imaging-software.html">1. Imaging Software</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="what-is-an-image.html">2. What is an image?</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="image-display.html">3. Image display</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="multi-dimensional-images.html">4. Multi-dimensional images</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="filetypes-and-metadata.html">5. Filetypes and metadata</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="designing-a-light-microscopy-experiment.html">6. Designing a light microscopy experiment</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush8">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading8">
        <a href="choosing-acquisition-settings.html">7. Choosing acquisition settings</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush9">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading9">
        <a href="quality-control-and-manual-segmentation.html">8. Quality control and manual segmentation</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush10">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading10">
        <a href="filters-and-thresholding.html">9. Filters and thresholding</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush11">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading11">
        <a href="instance-segmentation-and-measurements.html">10. Instance segmentation and measurements</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush12">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading12">
        <a href="additional-resources.html">11. Additional resources</a>
    </div>
<!--/div.accordion-header-->
        
  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width resources">
<a href="../instructor/aio.html">See all in one page</a>
            

            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">
            
            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-imaging-software"><p>Content from <a href="imaging-software.html">Imaging Software</a></p>
<hr>
<p> Last updated on 2024-04-26 | 
        
        <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/edit/main/episodes/imaging-software.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 55 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are the different software options for viewing microscopy
images?</li>
<li>How can Napari be used to view images?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain the pros and cons of different image visualisation tools
(e.g. ImageJ, Napari and proprietary options)</li>
<li>Use Napari to open images</li>
<li>Navigate the Napari viewer (pan/zoom/swapping between 2D and 3D
views…)</li>
<li>Explain the main parts of the Napari user interface</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="choosing-the-right-tool-for-the-job"><h2 class="section-heading">Choosing the right tool for the job<a class="anchor" aria-label="anchor" href="#choosing-the-right-tool-for-the-job"></a>
</h2>
<hr class="half-width">
<p>Light microscopes can produce a very wide range of image data (we’ll
see some examples in the <a href="multi-dimensional-images.html">multi-dimensional images
episode</a>) - for example:</p>
<ul>
<li>2D or 3D</li>
<li>Time series or snapshots</li>
<li>Different channels</li>
<li>Small to large datasets</li>
</ul>
<figure><img src="../fig/images-mosaic.png" style="width:80.0%" alt="A mosaic of screenshots of some of Napari's  included sample data" class="figure mx-auto d-block"></figure><p>With such a wide range of data, there comes a huge variety of
software that can work with these images. Different software may be
specialised to specific types of image data, or to specific research
fields. There is no one ‘right’ software to use - it’s about choosing
the right tool for yourself, your data, and your research question!</p>
<p>Some points to consider when choosing software are:</p>
<ul>
<li><p><strong>What is common in your research field?</strong><br>
Having a good community around the software you work with can be
extremely helpful - so it’s worth considering what is popular in your
department, or in relevant papers in your field.</p></li>
<li><p><strong>Open source or proprietary?</strong><br>
We’ll look at this more in the next section, but it’s important to
consider if the software you are using is freely available, or requires
a one-off payment or a regular subscription fee to use.</p></li>
<li><p><strong>Support for image types?</strong><br>
For example, does it support 3D images, or timeseries?</p></li>
<li><p><strong>Can it be automated/customised/extended?</strong><br>
Can you automate certain steps with your own <em>scripts or
plugins</em>? This is useful to make sure your analysis steps can be
easily shared and reproduced by other researchers. It also enables you
to add extra features to a piece of software, and automate repetitive
steps for large numbers of images.</p></li>
</ul>
<div id="what-are-scripts-and-plugins" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="what-are-scripts-and-plugins" class="callout-inner">
<h3 class="callout-title">What are scripts and plugins?<a class="anchor" aria-label="anchor" href="#what-are-scripts-and-plugins"></a>
</h3>
<div class="callout-content">
<p>Scripts and plugins are ways to automate certain software steps or
add new features.</p>
<div class="section level4">
<h4 id="scripts">Scripts<a class="anchor" aria-label="anchor" href="#scripts"></a>
</h4>
<p>Scripts are lists of commands to be carried out by a piece of
software e.g.  load an image, then threshold it, then measure its size…
They are normally used to automate certain processing steps - for
example, rather than having to load each image individually and click
the same buttons again and again in the user interface, a script could
load each image automatically and run all those steps in one go. Not
only does this save time and reduce manual errors, but it also ensures
your workflow can easily be shared and reproduced by other
researchers.</p>
</div>
<div class="section level4">
<h4 id="plugins">Plugins<a class="anchor" aria-label="anchor" href="#plugins"></a>
</h4>
<p>Plugins, in contrast to scripts, are focused on adding optional new
features to a piece of software (rather than automating use of existing
features). They allow members of the community, outside the main team
that develops the software, to add features they need for a particular
image type or processing task. They’re designed to be re-useable so
other members of the community can easily benefit from these new
features.</p>
</div>
</div>
</div>
</div>
<p>A good place to look for advice on software is the <a href="https://forum.image.sc/" class="external-link">image.sc forum</a> - a popular forum for
image analysis (mostly related to biological or medical images).</p>
<div id="using-the-image.sc-forum" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="using-the-image.sc-forum" class="callout-inner">
<h3 class="callout-title">Using the image.sc forum<a class="anchor" aria-label="anchor" href="#using-the-image.sc-forum"></a>
</h3>
<div class="callout-content">
<p>Go to the <a href="https://forum.image.sc/" class="external-link">image.sc forum</a> and
take a look at the pinned post called ‘Welcome to the Image.sc
Forum!’</p>
<ul>
<li><p>Search for posts in the category ‘Announcements’ tagged with
‘napari’</p></li>
<li><p>Search for posts in the category ‘Image Analysis’ tagged with
‘napari’</p></li>
<li><p>Click on some posts to see how questions and replies are laid
out</p></li>
</ul>
</div>
</div>
</div>
</section><section id="open-source-vs-proprietary"><h2 class="section-heading">Open source vs proprietary<a class="anchor" aria-label="anchor" href="#open-source-vs-proprietary"></a>
</h2>
<hr class="half-width">
<p>A key factor to consider when choosing software is whether it is open
source or proprietary:</p>
<ul>
<li><p><strong>Open source</strong>: Software that is made freely
available to use and modify.</p></li>
<li><p><strong>Proprietary</strong>: Software that is owned by a company
and usually requires either a one-off fee or subscription to
use.</p></li>
</ul>
<p>Both can be extremely useful, and it is very likely that you will use
a mix of both to view and analyse your images. For example, proprietary
software is often provided by the manufacturer when a microscope is
purchased. You will likely use this during acquisition of your images
and for some processing steps after.</p>
<p>There are pros and cons to both, and it’s worth considering the
following:</p>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Pros and cons of open-source / proprietary</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler1" aria-labelledby="headingSpoiler1">
<div class="accordion-body">
<p><strong>Cost</strong><br>
One of the biggest advantages of open source software is that it is
free. This means it is always available, even if you move to a different
institution that may not have paid for other proprietary software.</p>
<p><strong>Development funding/team</strong><br>
Proprietary software is usually maintained by a large team of developers
that are funded full time. This may mean it is more stable and
thoroughly tested/validated than some open-source software. Some
open-source projects will be maintained by large teams with very
thorough testing, while others will only have a single developer
part-time.</p>
<p><strong>Flexibility/extension</strong><br>
Open-source software tends to be easier to extend with new features, and
more flexible to accommodate a wide variety of workflows. Although, many
pieces of proprietary software have a plugin system or scripting to
allow automation.</p>
<p><strong>Open file formats and workflows</strong><br>
Open-source software uses open file formats and workflows, so anyone can
see the details of how the analysis is done. Proprietary software tends
to keep the implementation details hidden and use file formats that
can’t be opened easily in other software.</p>
</div>
</div>
</div>
</div>
<p>As always, the right software to use will depend on your preference,
your data and your research question. This being said, we will only use
open-source software in this course, and we encourage using open-source
software where possible.</p>
</section><section id="fijiimagej-and-napari"><h2 class="section-heading">Fiji/ImageJ and Napari<a class="anchor" aria-label="anchor" href="#fijiimagej-and-napari"></a>
</h2>
<hr class="half-width">
<p>While there are many pieces of software to choose from, two of the
most popular open-source options are <a href="https://imagej.net/software/fiji/" class="external-link">Fiji/ImageJ</a> and <a href="https://napari.org/" class="external-link">Napari</a>. They are both:</p>
<ul>
<li>Freely available</li>
<li>‘General’ imaging software i.e. applicable to many different
research fields</li>
<li>Supporting a wide range of image types</li>
<li>Customisable with scripts + plugins</li>
</ul>
<p>Both are great options for working with a wide variety of images - so
why choose one over the other? Some of the main differences are listed
below if you are interested:</p>
<div id="accordionSpoiler2" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler2" aria-expanded="false" aria-controls="collapseSpoiler2">
  <h3 class="accordion-header" id="headingSpoiler2">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Differences between Fiji/ImageJ and Napari</h3>
</button>
<div id="collapseSpoiler2" class="accordion-collapse collapse" data-bs-parent="#accordionSpoiler2" aria-labelledby="headingSpoiler2">
<div class="accordion-body">
<p><strong>Python vs Java</strong><br>
A big advantage of Napari is that it is made with the Python programming
language (vs Fiji/ImageJ which is made with Java). In general, this
makes it easier to extend with scripts and plugins as Python tends to be
more widely used in the research community. It also means Napari can
easily integrate with other python tools e.g. Python’s popular machine
learning libraries.</p>
<p><strong>Maturity</strong><br>
Fiji/ImageJ has been actively developed <a href="https://imagej.net/software/imagej/" class="external-link">for many years now (&gt;20
years)</a>, while Napari is a more recent development <a href="https://napari.org/stable/community/team.html#project-history" class="external-link">starting
around 2018</a>. This difference in age comes with pros and cons - in
general, it means that the core features and layout of Fiji/ImageJ are
very well established, and less likely to change than Napari. With
Napari, you will likely have to adjust your image processing workflow
with new versions, or update any scripts/plugins more often. Equally, as
Napari is new and rapidly growing in popularity, it is quickly gaining
new features and attracting a wide range of new plugin developers.</p>
<p><strong>Built-in tools</strong><br>
Fiji/ImageJ comes with many image processing tools built-in by default -
e.g.  making image histograms, thresholding and gaussian blur (we will
look at these terms in the <a href="filters-and-thresholding.html">filters and thresholding
episode</a>). Napari, in contrast, is more minimal by default - mostly
focusing on image display. It requires installation of additional
plugins to add many of these features.</p>
<p><strong>Specific plugins</strong><br>
There are excellent plugins available for Fiji/ImageJ and Napari that
focus on specific types of image data or processing steps. The
availability of a specific plugin will often be a deciding factor on
whether to use Fiji/ImageJ or Napari for your project.</p>
<p><strong>Ease of installation and user interface</strong><br>
As Fiji/ImageJ has been in development for longer, it tends to be
simpler to install than Napari (especially for those with no prior
Python experience). In addition, as it has more built-in image
processing tools, it tends to be simpler to use fully from its user
interface. Napari meanwhile is often strongest when you combine it with
some Python scripting (although this isn’t required for many
workflows!)</p>
</div>
</div>
</div>
</div>
<p>For this lesson, we will use Napari as our software of choice. It’s
worth bearing in mind though that Fiji/ImageJ can be a useful
alternative - and many workflows will actually use both Fiji/ImageJ and
Napari together! Again, it’s about choosing the right tool for your data
and research question.</p>
</section><section id="opening-napari"><h2 class="section-heading">Opening Napari<a class="anchor" aria-label="anchor" href="#opening-napari"></a>
</h2>
<hr class="half-width">
<p>Let’s get started by opening a new Napari window - you should have
already followed the <a href="index.html#setup">installation instructions</a>.
Note this can take a while the first time, so give it a few minutes!</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">BASH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode bash" tabindex="0"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="ex">conda</span> activate napari-env</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="ex">napari</span></span></code></pre>
</div>
<figure><img src="../fig/blank-napari-ui.png" alt="A screenshot of the default Napari user  interface" class="figure mx-auto d-block"></figure></section><section id="opening-images"><h2 class="section-heading">Opening images<a class="anchor" aria-label="anchor" href="#opening-images"></a>
</h2>
<hr class="half-width">
<p>Napari comes with some example images - let’s open one now. Go to the
top menu-bar of Napari and select:<br><code>File &gt; Open Sample &gt; napari builtins &gt; Cells (3D+2Ch)</code></p>
<p>You should see a fluorescence microscopy image of some cells:</p>
<figure><img src="../fig/cells-napari.png" alt="A screenshot of a flourescence microscopy image  of some cells in Napari" class="figure mx-auto d-block"></figure></section><section id="naparis-user-interface"><h2 class="section-heading">Napari’s User interface<a class="anchor" aria-label="anchor" href="#naparis-user-interface"></a>
</h2>
<hr class="half-width">
<p>Napari’s user interface is split into a few main sections, as you can
see in the diagram below (note that on Macs the main menu will appear in
the upper ribbon, rather than inside the Napari window):</p>
<figure><img src="../fig/ui-sections-napari.png" alt="A screenshot of Napari with the main user  interface sections labelled" class="figure mx-auto d-block"></figure><p>Let’s take a brief look at each of these sections - for full
information see the <a href="https://napari.org/stable/tutorials/fundamentals/viewer.html" class="external-link">Napari
documentation</a>.</p>
</section><section id="main-menu"><h2 class="section-heading">Main menu<a class="anchor" aria-label="anchor" href="#main-menu"></a>
</h2>
<hr class="half-width">
<p>We already used the main menu in the last section to open a sample
image. The main menu contains various commands for opening images,
changing preferences and installing plugins (we’ll see more of these
options in later episodes).</p>
</section><section id="canvas"><h2 class="section-heading">Canvas<a class="anchor" aria-label="anchor" href="#canvas"></a>
</h2>
<hr class="half-width">
<p>The canvas is the main part of the Napari user interface. This is
where we display and interact with our images.</p>
<p>Try moving around the cells image with the following commands:</p>
<pre><code>Pan - Click and drag
Zoom - Scroll in/out (use the same gestures with your mouse 
                      that you would use to scroll up/down 
                      in a document)</code></pre>
</section><section id="dimension-sliders"><h2 class="section-heading">Dimension sliders<a class="anchor" aria-label="anchor" href="#dimension-sliders"></a>
</h2>
<hr class="half-width">
<p>Dimension sliders appear at the bottom of the canvas depending on the
type of image displayed. For example, here we have a 3D image of some
cells, which consists of a stack of 2D images. If we drag the slider at
the bottom of the image, we move up and down in this stack:</p>
<figure><img src="../fig/dim-slider.png" alt="Three screenshots of the cells image in napari, at  different z depths" class="figure mx-auto d-block"></figure><p>Pressing the arrow buttons at either end of the slider steps through
one slice at a time. Also, pressing the ‘play’ button at the very left
of the slider moves automatically through the stack until pressed
again.</p>
<figure><img src="../fig/dim-slider-closeup.png" style="width:80.0%" alt="Closeup of Napari's dimension slider with labels" class="figure mx-auto d-block"></figure><p>We will see in later episodes that more sliders can appear if our
image has more dimensions (e.g. time series, or further channels).</p>
</section><section id="viewer-buttons"><h2 class="section-heading">Viewer buttons<a class="anchor" aria-label="anchor" href="#viewer-buttons"></a>
</h2>
<hr class="half-width">
<p>The viewer buttons (the row of buttons at the bottom left of Napari)
control various aspects of the Napari viewer:</p>
<div class="section level3">
<h3 id="console">Console <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/console.svg" alt="A screenshot of Napari's console button" height="30" class="figure"><a class="anchor" aria-label="anchor" href="#console"></a>
</h3>
<p>This button opens Napari’s built-in python console - we’ll use the
console more in later episodes.</p>
</div>
<div class="section level3">
<h3 id="d3d">2D/3D <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/2D.svg" alt="A screenshot of Napari's 2D button" height="25" class="figure"> / <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/3D.svg" alt="A screenshot of Napari's 3D button" height="25" class="figure"><a class="anchor" aria-label="anchor" href="#d3d"></a>
</h3>
<p>This switches the canvas between 2D and 3D display. Try switching to
the 3D view for the cells image:</p>
<figure><img src="../fig/cells-3d-napari.png" alt="A screenshot of 3D cells in Napari" class="figure mx-auto d-block"></figure><p>The controls for moving in 3D are similar to those for 2D:</p>
<pre><code>Rotate - Click and drag
Pan - Shift + click and drag
Zoom - Scroll in/out</code></pre>
</div>
<div class="section level3">
<h3 id="roll-dimensions">Roll dimensions <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/roll.svg" alt="A screenshot of Napari's roll dimensions button" height="25" class="figure"><a class="anchor" aria-label="anchor" href="#roll-dimensions"></a>
</h3>
<p>This changes which image dimensions are displayed in the viewer. For
example, let’s switch back to the 2D view for our cells image and press
the roll dimensions button multiple times. You’ll see that it switches
between different orthogonal views (i.e. at 90 degrees to our starting
view). Pressing it 3 times will bring us back to the original
orientation.</p>
<figure><img src="../fig/roll-dims.png" alt="Three screenshots of the cells image in napari,  with different axes being visualised" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="transpose-dimensions">Transpose dimensions <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/transpose.svg" alt="A screenshot of Napari's transpose dimensions button" height="25" class="figure"><a class="anchor" aria-label="anchor" href="#transpose-dimensions"></a>
</h3>
<p>This button swaps the two currently displayed dimensions. Again
trying this for our cells image, we see that the image becomes flipped.
Pressing the button again brings us back to the original
orientation.</p>
<figure><img src="../fig/transpose-dim.png" alt="Two screenshots of the cells image in napari,  with dimensions swapped" class="figure mx-auto d-block"></figure>
</div>
<div class="section level3">
<h3 id="grid">Grid <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/grid.svg" alt="A screenshot of Napari's grid button" height="25" class="figure"><a class="anchor" aria-label="anchor" href="#grid"></a>
</h3>
<p>This button displays all image layers in a grid (+ any additional
layer types, as we’ll see <a href="#layer-buttons">later in the
episode</a>). Using this for our cells image, we see the nuclei (green)
displayed next to the cell membranes (purple), rather than on top of
each other.</p>
</div>
<div class="section level3">
<h3 id="home">Home <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/home.svg" alt="A screenshot of Napari's home button" height="25" class="figure"><a class="anchor" aria-label="anchor" href="#home"></a>
</h3>
<p>This button brings the canvas back to its default view. This is
useful if you have panned/zoomed to a specific region and want to
quickly get back to an overview of the full image.</p>
</div>
</section><section id="layer-list"><h2 class="section-heading">Layer list<a class="anchor" aria-label="anchor" href="#layer-list"></a>
</h2>
<hr class="half-width">
<p>Now that we’ve seen the main controls for the viewer, let’s look at
the layer list. ‘Layers’ are how Napari displays multiple items together
in the viewer. For example, currently our layer list contains two items
- ‘nuclei’ and ‘membrane’. These are both <code>Image</code> layers and
are displayed in order, with the nuclei on top and membrane
underneath.</p>
<figure><img src="../fig/layer-list.png" alt="A screenshot of Napari's layer list, showing two  image layers named 'nuclei' and 'membrane'" class="figure mx-auto d-block"></figure><p>We can show/hide each layer by clicking the eye icon on the left side
of their row. We can also rename them by double clicking on the row.</p>
<p>We can change the order of layers by dragging and dropping items in
the layer list. For example, try dragging the membrane layer above the
nuclei. You should see the nuclei disappear from the viewer (as they are
now hidden by the membrane image on top).</p>
<figure><img src="../fig/layer-reordering.png" alt="A screenshot of Napari with the nuclei and  membrane layer swapped" class="figure mx-auto d-block"></figure><p>Here we only have <code>Image</code> layers, but there are many more
types like <code>Points</code>, <code>Shapes</code> and
<code>Labels</code>, some of which we will see <a href="#layer-buttons">later in the episode</a>.</p>
</section><section id="layer-controls"><h2 class="section-heading">Layer controls<a class="anchor" aria-label="anchor" href="#layer-controls"></a>
</h2>
<hr class="half-width">
<p>Next let’s look at the layer controls - this area shows controls only
for the currently selected layer (i.e. the one that is highlighted in
blue in the layer list). For example, if we click on the nuclei layer
then we can see a <code>colormap</code> of green, while if we click on
the membrane layer we see a <code>colormap</code> of magenta.</p>
<p>Controls will also vary depending on layer type (like
<code>Image</code> vs <code>Points</code>) as we will see <a href="#layer-buttons">later in this episode</a>.</p>
<p>Let’s take a quick look at some of the main image layer controls:</p>
<div class="section level3">
<h3 id="opacity">Opacity<a class="anchor" aria-label="anchor" href="#opacity"></a>
</h3>
<p>This changes the opacity of the layer - lower values are more
transparent. For example, reducing the opacity of the membrane layer (if
it is still on top of the nuclei), allows us to see the nuclei
again.</p>
</div>
<div class="section level3">
<h3 id="contrast-limits">Contrast limits<a class="anchor" aria-label="anchor" href="#contrast-limits"></a>
</h3>
<p>We’ll discuss this in detail in the <a href="image-display.html#brightness-and-contrast">image display
episode</a>, but briefly - the contrast limits adjust what parts of the
image we can see and how bright they appear in the viewer. Moving the
left node adjusts what is shown as fully black, while moving the right
node adjusts what is shown as fully bright.</p>
</div>
<div class="section level3">
<h3 id="colormap">Colormap<a class="anchor" aria-label="anchor" href="#colormap"></a>
</h3>
<p>Again, we’ll discuss this in detail in the <a href="image-display.html#colormaps-luts">image display episode</a>, but
briefly - the colormap determines what colours an image is displayed
with. Clicking in the dropdown shows a wide range of options that you
can swap between.</p>
</div>
<div class="section level3">
<h3 id="blending">Blending<a class="anchor" aria-label="anchor" href="#blending"></a>
</h3>
<p>This controls how multiple layers are blended together to give the
final result in the viewer. There are <a href="https://napari.org/stable/guides/layers.html#blending-layers" class="external-link">many
different options to choose from</a>. For example, let’s put the nuclei
layer back on top of the membrane and change its blending to ‘opaque’.
You should see that it now completely hides the membrane layer
underneath. Changing the blending back to ‘additive’ will allow both the
nucleus and membrane layers to be seen together again.</p>
<div id="using-image-layer-controls" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="using-image-layer-controls" class="callout-inner">
<h3 class="callout-title">Using image layer controls<a class="anchor" aria-label="anchor" href="#using-image-layer-controls"></a>
</h3>
<div class="callout-content">
<p>Adjust the layer controls for both nuclei and membrane to give the
result below:</p>
<figure><img src="../fig/layer-controls-task.png" alt="Cells image with blue nuclei and bright  red membranes" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<ul>
<li>Click on the nuclei in the layer list</li>
<li>Change the colormap to cyan</li>
<li>Click on the membrane in the layer list</li>
<li>Change the colormap to red</li>
<li>Move the right contrast limits node to the left to make the
membranes appear brighter</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section><section id="layer-buttons"><h2 class="section-heading">Layer buttons<a class="anchor" aria-label="anchor" href="#layer-buttons"></a>
</h2>
<hr class="half-width">
<p>So far we have only looked at <code>Image</code> layers, but there
are many more types supported by Napari. The layer buttons allow us to
add additional layers of these new types:</p>
<div class="section level3">
<h3 id="points">Points <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/new_points.svg" alt="A screenshot of Napari's point layer button" height="30" class="figure"><a class="anchor" aria-label="anchor" href="#points"></a>
</h3>
<p>This button creates a new <a href="https://napari.org/stable/howtos/layers/points.html" class="external-link">points
layer</a>. This can be used to mark specific locations in an image.</p>
</div>
<div class="section level3">
<h3 id="shapes">Shapes <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/new_shapes.svg" alt="A screenshot of Napari's shape layer button" height="30" class="figure"><a class="anchor" aria-label="anchor" href="#shapes"></a>
</h3>
<p>This button creates a new <a href="https://napari.org/stable/howtos/layers/shapes.html" class="external-link">shapes
layer</a>. Shapes can be used to mark regions of interest e.g. with
rectangles, ellipses or lines.</p>
</div>
<div class="section level3">
<h3 id="labels">Labels <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/new_labels.svg" alt="A screenshot of Napari's labels layer button" height="30" class="figure"><a class="anchor" aria-label="anchor" href="#labels"></a>
</h3>
<p>This button creates a new <a href="https://napari.org/stable/howtos/layers/labels.html" class="external-link">labels
layer</a>. This is usually used to label specific regions in an image
e.g. to label individual nuclei.</p>
</div>
<div class="section level3">
<h3 id="remove-layer">Remove layer <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/delete.svg" alt="A screenshot of Napari's delete layer button" height="30" class="figure"><a class="anchor" aria-label="anchor" href="#remove-layer"></a>
</h3>
<p>This button removes the currently selected layer (highlighted in
blue) from the layer list.</p>
<div id="other-layer-types" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="other-layer-types" class="callout-inner">
<h3 class="callout-title">Other layer types<a class="anchor" aria-label="anchor" href="#other-layer-types"></a>
</h3>
<div class="callout-content">
<p>Note that there are some layer types that can’t be added via clicking
buttons in the user interface, like <a href="https://napari.org/stable/howtos/layers/surface.html" class="external-link">surfaces</a>,
<a href="https://napari.org/stable/howtos/layers/tracks.html" class="external-link">tracks</a>
and <a href="https://napari.org/stable/howtos/layers/vectors.html" class="external-link">vectors</a>.
These require calling python commands in Napari’s console or an external
python script.</p>
</div>
</div>
</div>
<div id="point-layers" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="point-layers" class="callout-inner">
<h3 class="callout-title">Point layers<a class="anchor" aria-label="anchor" href="#point-layers"></a>
</h3>
<div class="callout-content">
<p>Let’s take a quick look at one of these new layer types - the
<code>Points</code> layer.</p>
<p>Add a new points layer by clicking the points button. Investigate the
different layer controls - what do they do? Note that hovering over
buttons will usually show a summary tooltip.</p>
<p>Add points and adjust settings to give the result below:</p>
<figure><img src="../fig/points-task.png" alt="Cells image with points marking multiple nuclei" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<ul>
<li>Click the ‘add points’ button <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/add.svg" alt="Screenshot of Napari's add points button" height="30" class="figure">
</li>
<li>Click on nuclei to add points on top of them</li>
<li>Click the ‘select points’ button <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/select.svg" alt="Screenshot of Napari's select points button" height="30" class="figure">
</li>
<li>Click on the point over the dividing nucleus</li>
<li>Increase the point size slider</li>
<li>Change its symbol to star</li>
<li>Change its face colour to purple</li>
<li>change its edge colour to white</li>
</ul>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>There are many software options for light microscopy images</li>
<li>Napari and Fiji/ImageJ are popular open-source options</li>
<li>Napari’s user interface is split into a few main sections including
the canvas, layer list, layer controls…</li>
<li>Layers can be of different types e.g. <code>Image</code>,
<code>Point</code>, <code>Label</code>
</li>
<li>Different layer types have different layer controls</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</section></section><section id="aio-what-is-an-image"><p>Content from <a href="what-is-an-image.html">What is an image?</a></p>
<hr>
<p> Last updated on 2024-04-26 | 
        
        <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/edit/main/episodes/what-is-an-image.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 55 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How are images represented in the computer?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li><p>Explain how a digital image is made of pixels</p></li>
<li><p>Find the value of different pixels in an image in Napari</p></li>
<li><p>Determine an image’s dimensions (numpy ndarray
<code>.shape</code>)</p></li>
<li><p>Determine an image’s data type (numpy ndarray
<code>.dtype</code>)</p></li>
<li><p>Explain the coordinate system used for images</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>In the last episode, we looked at how to view images in Napari. Let’s
take a step back now and try to understand how Napari (or ImageJ or any
other viewer) understands how to display images properly. To do that we
must first be able to answer the fundamental question - what is an
image?</p>
<section id="pixels"><h2 class="section-heading">Pixels<a class="anchor" aria-label="anchor" href="#pixels"></a>
</h2>
<hr class="half-width">
<p>Let’s start by removing all the layers we added to the Napari viewer
last episode. Then we can open a new sample image:</p>
<ul>
<li><p>Click on the top layer in the layer list and <kbd>shift</kbd> +
click the bottom layer. This should highlight all layers in
blue.</p></li>
<li><p>Press the remove layer button <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/delete.svg" alt="A screenshot of Napari's delete layer button" height="30" class="figure"></p></li>
<li><p>Go to the top menu-bar of Napari and select:<br><code>File &gt; Open Sample &gt; napari builtins &gt; Human Mitosis</code></p></li>
</ul>
<figure><img src="../fig/human-mitosis-napari.png" alt="A screenshot of a 2D image of human cells  undergoing mitosis in Napari" class="figure mx-auto d-block"></figure><p>This 2D image shows the nuclei of human cells undergoing mitosis. If
we really zoom in up-close by scrolling, we can see that this image is
actually made up of many small squares with different brightness values.
These squares are the image’s pixels (or ‘picture elements’) and are the
individual units that make up all digital images.</p>
<p>If we hover over these pixels with the mouse cursor, we can see that
each pixel has a specific value. Try hovering over pixels in dark and
bright areas of the image and see how the value changes in the bottom
left of the viewer:</p>
<figure><img src="../fig/pixel-value.png" alt="A screenshot of Napari - with the mouse cursor  hovering over a pixel and highlighting the corresponding pixel value" class="figure mx-auto d-block"></figure><p>You should see that brighter areas have higher values than darker
areas (we’ll see exactly how these values are converted to colours in
the <a href="image-display.html">image display episode</a>).</p>
</section><section id="images-are-arrays-of-numbers"><h2 class="section-heading">Images are arrays of numbers<a class="anchor" aria-label="anchor" href="#images-are-arrays-of-numbers"></a>
</h2>
<hr class="half-width">
<p>We’ve seen that images are made of individual units called pixels
that have specific values - but how is an image really represented in
the computer? Let’s dig deeper into Napari’s <code>Image</code>
layers…</p>
<p>First, open Napari’s built-in Python console by pressing the console
button <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/console.svg" alt="A screenshot of Napari's console button" height="30" class="figure">. Note
this can take a few seconds to open, so give it some time:</p>
<figure><img src="../fig/console.png" alt="A screenshot of Napari's console" class="figure mx-auto d-block"></figure><div id="console-readability" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="console-readability" class="callout-inner">
<h3 class="callout-title">Console readability<a class="anchor" aria-label="anchor" href="#console-readability"></a>
</h3>
<div class="callout-content">
<p>You can increase the font size in the console by clicking inside it,
then pressing <kbd>Ctrl</kbd> and <kbd>+</kbd> together. The font size
can also be decreased with <kbd>Ctrl</kbd> and <kbd>-</kbd>
together.</p>
<p>Note that you can also pop the console out into its own window by
clicking the small <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/pop_out.svg" alt="A screenshot of Napari's float panel button" height="30" class="figure">
icon on the left side.</p>
</div>
</div>
</div>
<p>Let’s look at the human mitosis image more closely - copy the text in
the ‘Python’ cell below into Napari’s console and then press the
<kbd>Enter</kbd> key. You should see it returns text that matches the
‘Output’ cell below in response.</p>
<p>All of the information about the Napari viewer can be accessed
through the console with a variable called <code>viewer</code>. A
<code>viewer</code> has 1 to many layers, and here we access the top
(first) layer with <code>viewer.layers[0]</code>. Then, to access the
actual image data stored in that layer, we retrieve it with
<code>.data</code>:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co"># Get the image data for the first layer in Napari</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>image <span class="op">=</span> viewer.layers[<span class="dv">0</span>].data</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># Print the image values and type</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="bu">print</span>(image)</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(image))</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[[ 8  8  8 ... 63 78 75]
 [ 8  8  7 ... 67 71 71]
 [ 9  8  8 ... 53 64 66]
 ...
 [ 8  9  8 ... 17 24 59]
 [ 8  8  8 ... 17 22 55]
 [ 8  8  8 ... 16 18 38]]
 
 &lt;class 'numpy.ndarray'&gt;</code></pre>
</div>
<p>You should see that a series of numbers are printed out that are
stored in a Python data type called a <code>numpy.ndarray</code>.
Fundamentally, this means that all images are really just arrays of
numbers (one number per pixel). Arrays are just rectangular grids of
numbers, much like a spreadsheet. Napari is reading those values and
converting them into squares of particular colours for us to see in the
viewer, but this is only to help us interpret the image contents - the
numbers are the real underlying data.</p>
<p>For example, look at the simplified image of an arrow below. On the
left is the array of numbers, with the corresponding image display on
the right. This is called a 4 by 4 image, as it has 4 rows and 4
columns:</p>
<figure><img src="../fig/array.png" alt="A diagram comparing the array of numbers and image  display for a simplified image of an arrow" class="figure mx-auto d-block"></figure><p>In Napari this array is a <code>numpy.ndarray</code>. <a href="https://numpy.org/" class="external-link">NumPy</a> is a popular python package that
provides ‘n-dimensional arrays’ (or ‘ndarray’ for short). N-dimensional
just means they can support any number of dimensions - for example, 2D
(squares/rectangles of numbers), 3D (cubes/cuboids of numbers) and
beyond (like time series, images with many channels etc. where we would
have multiple rectangles or cuboids of data which provide further
information all at the same location).</p>
</section><section id="creating-an-image"><h2 class="section-heading">Creating an image<a class="anchor" aria-label="anchor" href="#creating-an-image"></a>
</h2>
<hr class="half-width">
<p>Where do the numbers in our image array come from? The exact details
of how an image is created will depend on the type of microscope you are
using e.g.  widefield, confocal, superresolution etc. In general though,
we have 3 main parts:</p>
<ul>
<li>
<strong>Sample:</strong> the object we want to image e.g. some
cells</li>
<li>
<strong>Objective lens:</strong> the lens that gathers the light and
focuses it for detection</li>
<li>
<strong>Detector:</strong> the device that detects the light to form
the digital image e.g. a CCD camera</li>
</ul>
<p>To briefly summarise for a fluorescence microscopy image:<br>
an excitation light source (e.g. a laser) illuminates the sample, and
this light is absorbed by a fluorescent label. This causes it to emit
light which is then gathered and focused by the objective lens, before
hitting the detector. The detector might be a single element (e.g. in a
laser-scanning microscope) or composed of an array of many small, light
sensitive areas - these are physical pixels, that will correspond to the
pixels in the final image. When light hits one of the detector elements
it is converted into electrons, with more light resulting in more
electrons and a higher final value for that pixel.</p>
<p>The important factor to understand is that the final pixel value is
only ever an approximation of the real sample. Many factors will affect
this final result including the microscope optics, detector performance
etc.</p>
<div id="discussion1" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Discussion<a class="anchor" aria-label="anchor" href="#discussion1"></a>
</h3>
<div class="callout-content">
<p>Read the <a href="https://bioimagebook.github.io/chapters/1-concepts/1-images_and_pixels/images_and_pixels.html#a-simple-microscope" class="external-link">‘A
simple microscope’ section of Pete Bankhead’s bioimage book</a>.</p>
<ul>
<li>What are some factors that influence pixel values?</li>
<li>Can you come up with suggestions for any more?</li>
</ul>
</div>
</div>
</div>
</section><section id="image-dimensions"><h2 class="section-heading">Image dimensions<a class="anchor" aria-label="anchor" href="#image-dimensions"></a>
</h2>
<hr class="half-width">
<p>Let’s return to our human mitosis image and explore some of the key
features of its image array. First, what size is it?</p>
<p>We can find this out by running the following in Napari’s
console:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>image.shape</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(512, 512)</code></pre>
</div>
<p>The array size (also known as its dimensions) is stored in the
<code>.shape</code>. Here we see that it is <code>(512, 512)</code>
meaning this image is 512 pixels high and 512 pixels wide. Two values
are printed as this image is two dimensional (2D), for a 3D image there
would be 3, for a 4D image (e.g. with an additional time series) there
would be 4 and so on…</p>
</section><section id="image-data-type"><h2 class="section-heading">Image data type<a class="anchor" aria-label="anchor" href="#image-data-type"></a>
</h2>
<hr class="half-width">
<p>The other key feature of an image array is its ‘data type’ - this
controls which values can be stored inside of it. For example, let’s
look at the data type for our human mitosis image - this is stored in
<code>.dtype</code>:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>image.dtype</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span></span>
<span><span class="fu">dtype</span><span class="op">(</span><span class="st">'uint8'</span><span class="op">)</span></span></code></pre>
</div>
<p>We see that the data type (or ‘dtype’ for short) is
<code>uint8</code>. This is short for ‘unsigned integer 8-bit’. Let’s
break this down further into two parts - the type (unsigned integer) and
the bit-depth (8-bit).</p>
</section><section id="type"><h2 class="section-heading">Type<a class="anchor" aria-label="anchor" href="#type"></a>
</h2>
<hr class="half-width">
<p>The type determines what kind of values can be stored in the array,
for example:</p>
<ul>
<li>Unsigned integer: positive whole numbers</li>
<li>Signed integer: positive and negative whole numbers</li>
<li>Float: positive and negative numbers with a decimal point
e.g. 3.14</li>
</ul>
<p>For our mitosis image, ‘unsigned integer’ means that only positive
whole numbers can be stored inside. You can see this by hovering over
the pixels in the image again in Napari - the pixel value down in the
bottom left is always a positive whole number.</p>
</section><section id="bit-depth"><h2 class="section-heading">Bit depth<a class="anchor" aria-label="anchor" href="#bit-depth"></a>
</h2>
<hr class="half-width">
<p>The bit depth determines the range of values that can be stored
e.g. only values between 0 and 255. This is directly related to how the
array is stored in the computer.</p>
<p>In the computer, each pixel value will ultimately be stored in some
binary format as a series of ones and zeros. Each of these ones or zeros
is known as a ‘bit’, and the ‘bit depth’ is the number of bits used to
store each value. For example, our mitosis image uses 8 bits to store
each value (i.e.  a series of 8 ones or zeros like
<code>00000000</code>, or <code>01101101</code>…).</p>
<p>The reason the bit depth is so important is that it dictates the
number of different values that can be stored. In fact it is equal
to:</p>
<p><span class="math display">\[\large \text{Number of values} =
2^{\text{(bit depth)}}\]</span></p>
<p>Going back to our mitosis image, since it is stored as integers with
a bit-depth of 8, this means that it can store <span class="math inline">\(2^8 = 256\)</span> different values. This is equal
to a range of 0-255 for unsigned integers.</p>
<p>We can verify this by looking at the maximum value of the mitosis
image:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="bu">print</span>(image.<span class="bu">max</span>())</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span></span>
<span><span class="fl">255</span></span></code></pre>
</div>
<p>You can also see this by hovering over the brightest nuclei in the
viewer and examining their pixel values. Even the brightest nuclei won’t
exceed the limit of 255.</p>
<div id="dimensions-and-data-types" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="dimensions-and-data-types" class="callout-inner">
<h3 class="callout-title">Dimensions and data types<a class="anchor" aria-label="anchor" href="#dimensions-and-data-types"></a>
</h3>
<div class="callout-content">
<p>Let’s open a new image by removing all layers from the Napari viewer,
then copying and pasting the following lines into the Napari
console:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="im">from</span> skimage <span class="im">import</span> data</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>viewer.add_image(data.brain()[<span class="dv">9</span>, :, :], name<span class="op">=</span><span class="st">"brain"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>image <span class="op">=</span> viewer.layers[<span class="st">"brain"</span>].data</span></code></pre>
</div>
<p>This opens a new 2D image of part of a human head X-ray.</p>
<ul>
<li><p>What are the dimensions of this image?</p></li>
<li><p>What type and bit depth is this image?</p></li>
<li><p>What are the possible min/max values of this image array, based
on the bit depth?</p></li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="section level3">
<h3 id="dimensions">Dimensions<a class="anchor" aria-label="anchor" href="#dimensions"></a>
</h3>
<p>The image’s dimensions are (256, 256)</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>image.shape</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>
(256, 256)</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="type-and-bit-depth">Type and bit depth<a class="anchor" aria-label="anchor" href="#type-and-bit-depth"></a>
</h3>
<p>The image’s type and bit depth are: unsigned integer 16-bit</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a>image.dtype</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span></span>
<span><span class="fu">dtype</span><span class="op">(</span><span class="st">'uint16'</span><span class="op">)</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="min-and-max">Min and max<a class="anchor" aria-label="anchor" href="#min-and-max"></a>
</h3>
<p>Based on a bit depth of 16, this image can store <span class="math inline">\(2^{16} = 65536\)</span> values. As it is of type
‘unsigned integer’ this corresponds to a min and max of 0 and 65535.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="common-data-types"><h2 class="section-heading">Common data types<a class="anchor" aria-label="anchor" href="#common-data-types"></a>
</h2>
<hr class="half-width">
<p>NumPy supports a <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#sized-aliases" class="external-link">very
wide range of data types</a>, but there are a few that are most common
for image data:</p>
<table class="table">
<thead><tr class="header">
<th align="left">NumPy datatype</th>
<th align="left">Full name</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left"><code>uint8</code></td>
<td align="left">Unsigned integer 8-bit</td>
</tr>
<tr class="even">
<td align="left"><code>uint16</code></td>
<td align="left">Unsigned integer 16-bit</td>
</tr>
<tr class="odd">
<td align="left"><code>float32</code></td>
<td align="left">Float 32-bit</td>
</tr>
<tr class="even">
<td align="left"><code>float64</code></td>
<td align="left">Float 64-bit</td>
</tr>
</tbody>
</table>
<p><code>uint8</code> and <code>uint16</code> are most common for images
from light microscopes. <code>float32</code> and <code>float64</code>
are common during image processing (as we will see in later
episodes).</p>
</section><section id="choosing-a-bit-depth"><h2 class="section-heading">Choosing a bit depth<a class="anchor" aria-label="anchor" href="#choosing-a-bit-depth"></a>
</h2>
<hr class="half-width">
<p>Most images are either 8-bit or 16-bit - so how to choose which to
use? A higher bit depth will allow a wider range of values to be stored,
but it will also result in larger file sizes for the resulting images.
In general, a 16-bit image will have a file size that is about twice as
large as an 8-bit image if no compression is used (we’ll discuss
compression in the <a href="filetypes-and-metadata.html#compression">filetypes and metadata
episode</a>).</p>
<p>The best bit depth choice will depend on your particular imaging
experiment and research question. For example, if you know you have to
recognise features that only differ slightly in their brightness, then
you will likely need 16-bit to capture this. Equally, if you know that
you will need to collect a very large number of images and 8-bit is
sufficient to see your features of interest, then 8-bit may be a better
choice to reduce the required file storage space. As always it’s about
choosing the best fit for your specific project!</p>
<p>For more information on bit depths and types - we highly recommend <a href="https://bioimagebook.github.io/chapters/1-concepts/3-bit_depths/bit_depths.html" class="external-link">the
‘Types &amp; bit-depths’ chapter from Pete Bankhead’s free bioimage
book</a>.</p>
<div id="clipping-and-overflow" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="clipping-and-overflow" class="callout-inner">
<h3 class="callout-title">Clipping and overflow<a class="anchor" aria-label="anchor" href="#clipping-and-overflow"></a>
</h3>
<div class="callout-content">
<p>It’s important to be aware of what image type and bit depth you are
using. If you try to store values outside of the valid range, this can
lead to <em>clipping</em> and <em>overflow</em>.</p>
<ul>
<li><p>Clipping: Values outside the valid range are changed to the
closest valid value. For example, storing 1000 in a <code>uint8</code>
image may result in 255 being stored instead (the max value)</p></li>
<li><p>Overflow: For NumPy arrays, values outside the valid range are
‘wrapped around’ to give the new result. For example, storing 256 in a
<code>uint8</code> image (max 255) would give 0, 257 would give 1 and so
on…</p></li>
</ul>
<p>Clipping and overflow result in data loss - you can’t get the
original values back! So it’s always good to keep the data type in mind
when doing image processing operations (as we will see in later
episodes), and also when converting between different bit depths.</p>
</div>
</div>
</div>
</section><section id="coordinate-system"><h2 class="section-heading">Coordinate system<a class="anchor" aria-label="anchor" href="#coordinate-system"></a>
</h2>
<hr class="half-width">
<p>We’ve seen that images are arrays of numbers with a specific shape
(dimensions) and data type. How do we access specific values from this
array? What coordinate system is Napari using?</p>
<p>To look into this, let’s hover over pixels in our mitosis image and
examine the coordinates that appear to the left of the pixel value. If
you closed the mitosis image, then open it again by removing all layers
and selecting:
<code>File &gt; Open Sample &gt; napari builtins &gt; Human Mitosis</code>:</p>
<figure><img src="../fig/coordinates.png" alt="A screenshot of Napari - with the mouse cursor  hovering over a pixel and highlighting the corresponding coordinates" class="figure mx-auto d-block"></figure><p>As you move around, you should see that the lowest coordinate values
are at the top left corner, with the first value increasing as you move
down and the second value increasing as you move to the right. This is
different to the standard coordinate systems you may be used to (for
example, from making graphs):</p>
<figure><img src="../fig/coordinate-system.png" alt="Diagram comparing a standard graph  coordinate system (left) and the image coordinate system (right)" class="figure mx-auto d-block"></figure><p>Note that Napari lists coordinates as [y, x] or [rows, columns], so
e.g. [1,3] would be the pixel in row 1 and column 3. Remember that these
coordinates always start from 0 as you can see in the diagram below:</p>
<figure><img src="../fig/coordinates-on-image.png" style="width:50.0%" alt="A diagram showing how pixel coordinates change over a simple 4x4 image" class="figure mx-auto d-block"></figure><p>For the mitosis image, these coordinates are in pixels, but we’ll see
in the <a href="filetypes-and-metadata.html#pixel-size">filetypes and
metadata episode</a> that images can also be scaled based on resolution
to represent distances in the physical world (e.g. in micrometres).
Also, bear in mind that images with more dimensions (e.g. a 3D image)
will have longer coordinates like [z, y, x]…</p>
<div id="reading-and-modifying-pixel-values" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="reading-and-modifying-pixel-values" class="callout-inner">
<h3 class="callout-title">Reading and modifying pixel values<a class="anchor" aria-label="anchor" href="#reading-and-modifying-pixel-values"></a>
</h3>
<div class="callout-content">
<p>First, make sure you only have the human mitosis image open (close
any others). Run the following line in the console to ensure you are
referencing the correct image:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="co"># Get the image data for the layer called 'human_mitosis'</span></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>image <span class="op">=</span> viewer.layers[<span class="st">"human_mitosis"</span>].data</span></code></pre>
</div>
<p>Pixel values can be read by hovering over them in the viewer, or by
running the following in the console:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="co"># Replace y and x with the correct y and x coordinate e.g. image[3, 5]</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="bu">print</span>(image[y, x])</span></code></pre>
</div>
<p>Pixel values can be changed by running the following in the
console:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="co"># Replace y and x with the correct y and x coordinate, and</span></span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="co"># 'pixel_value' with the desired new pixel value e.g. image[3, 5] = 10</span></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>image[y, x] <span class="op">=</span> pixel_value</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a>viewer.layers[<span class="st">"human_mitosis"</span>].refresh()</span></code></pre>
</div>
<p>Given this information:</p>
<ol style="list-style-type: decimal">
<li>What is the pixel value at x=213 and y=115?</li>
<li>What is the pixel value at x=25 and y=63?</li>
<li>Change the value of the pixel at x=10 and y=15 to 200. Check the new
value - is it correct? If not, why not?</li>
<li>Change the value of the pixel at x=10 and y=15 to 300. Check the new
value - is it correct? If not, why not?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="section level3">
<h3 id="section">1<a class="anchor" aria-label="anchor" href="#section"></a>
</h3>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>image[<span class="dv">115</span>, <span class="dv">213</span>]</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span></span>
<span><span class="fl">162</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="section-1">2<a class="anchor" aria-label="anchor" href="#section-1"></a>
</h3>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>image[<span class="dv">63</span>, <span class="dv">25</span>]</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span></span>
<span><span class="fl">9</span></span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="section-2">3<a class="anchor" aria-label="anchor" href="#section-2"></a>
</h3>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>image[<span class="dv">15</span>, <span class="dv">10</span>] <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>viewer.layers[<span class="st">"human_mitosis"</span>].refresh()</span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a><span class="bu">print</span>(image[<span class="dv">15</span>, <span class="dv">10</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span></span>
<span><span class="fl">200</span></span></code></pre>
</div>
<p>The new value is correct. If you zoom into the top left corner of the
image, you should see the one bright pixel you just created.</p>
</div>
<div class="section level3">
<h3 id="section-3">4<a class="anchor" aria-label="anchor" href="#section-3"></a>
</h3>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a></span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>image[<span class="dv">15</span>, <span class="dv">10</span>] <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a>viewer.layers[<span class="st">"human_mitosis"</span>].refresh()</span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a><span class="bu">print</span>(image[<span class="dv">15</span>, <span class="dv">10</span>])</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span></span>
<span><span class="fl">44</span></span></code></pre>
</div>
<p>The new value is not correct. This is because 300 exceeds the maximum
value for this 8-bit image (max 255). The value therefore overflows and
‘wraps around’ to give 44 - an incorrect value.</p>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Digital images are made of pixels</li>
<li>Digital images store these pixels as arrays of numbers</li>
<li>Light microscopy images are only an approximation of the real
sample</li>
<li>Napari (and Python more widely) use NumPy arrays to store images -
these have a <code>shape</code> and <code>dtype</code>
</li>
<li>Most images are 8-bit or 16-bit unsigned integer</li>
<li>Images use a coordinate system with (0,0) at the top left, x
increasing to the right, and y increasing down</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-image-display"><p>Content from <a href="image-display.html">Image display</a></p>
<hr>
<p> Last updated on 2024-04-26 | 
        
        <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/edit/main/episodes/image-display.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 55 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How are pixel values converted into colours for display?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Create a histogram for an image</li>
<li>Install plugins from Napari Hub</li>
<li>Change colormap (LUT) in Napari</li>
<li>Adjust brightness and contrast in Napari</li>
<li>Explain the importance of always retaining a copy of the original
pixel values</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="image-array-and-display"><h2 class="section-heading">Image array and display<a class="anchor" aria-label="anchor" href="#image-array-and-display"></a>
</h2>
<hr class="half-width">
<p>Last episode we saw that images are arrays of numbers with specific
dimensions and data type. Napari reads these numbers (pixel values) and
converts them into colours on our display, allowing us to view the
image. Exactly how this conversion is done can vary greatly, and is the
topic of this episode.</p>
<p>For example, take the image array shown below. Depending on the
display settings, it can look very different inside Napari:</p>
<figure><img src="../fig/same-array-diff-display.png" style="width:70.0%" alt="Diagram showing an image array (top) with three different colormap  options (bottom)" class="figure mx-auto d-block"></figure><p>Equally, image arrays with different pixel values can look the same
in Napari depending on the display settings:</p>
<figure><img src="../fig/diff-array-same-display.png" style="width:50.0%" alt="Diagram showing two image arrays - 8-bit vs 16-bit (top) with the same  display (bottom)" class="figure mx-auto d-block"></figure><p>In summary, we can’t rely on appearance alone to understand the
underlying pixel values. Display settings (like the colormap, brightness
and contrast - as we will see below) have a big impact on how the final
image looks.</p>
</section><section id="napari-plugins"><h2 class="section-heading">Napari plugins<a class="anchor" aria-label="anchor" href="#napari-plugins"></a>
</h2>
<hr class="half-width">
<p>How can we quickly assess the pixel values in an image? We could
hover over pixels in Napari, or print the array into Napari’s console
(as we saw last episode), but these are hard to interpret at a glance. A
much better option is to use an image histogram.</p>
<p>To do this, we will have to install a new plugin for Napari. Remember
from the <a href="imaging-software.html">Imaging Software episode</a>
that plugins add new features to a piece of software. Napari has
hundreds of plugins available on the <a href="https://www.napari-hub.org/" class="external-link">napari hub</a> website.</p>
<p>Let’s start by going to the napari hub and searching for
‘matplotlib’:</p>
<figure><img src="../fig/napari-hub.png" alt="Screenshot of searching 'matplotlib' on napari hub" class="figure mx-auto d-block"></figure><p>You should see ‘napari Matplotlib’ appear in the list (if not, try
scrolling further down the page). If we click on
<code>napari matplotlib</code> this opens a summary of the plugin with
links to the documentation. There’s also a useful ‘Activity’ tab that
summarises the number of installs and maintenance history of the
plugin:</p>
<figure><img src="../fig/napari-hub-activity.png" alt="Screenshot of napari-matplotlib's activity tab on napari hub" class="figure mx-auto d-block"></figure><p>Now that we’ve found the plugin we want to use, let’s go ahead and
install it in Napari. Note that some plugins have special requirements
for installation, so it’s always worth checking their napari hub page
for any extra instructions. In the top menu bar of Napari select:<br><code>Plugins &gt; Install/Uninstall Plugins...</code></p>
<figure><img src="../fig/plugin-installation.png" alt="Screenshot of plugin installation window in Napari" class="figure mx-auto d-block"></figure><p>This should open a window summarising all installed plugins (at the
top) and all available plugins to install (at the bottom). If we search
for ‘matplotlib’ in the top searchbar, then ‘napari-matplotlib’ will
appear under ‘Available Plugins’. Press the blue install button and wait
for it to finish. <strong>You’ll then need to close and re-open
Napari.</strong></p>
<p>If all worked as planned, you should see a new option in the top
menubar under:<br><code>Plugins &gt; napari Matplotlib</code></p>
<div id="finding-plugins" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="finding-plugins" class="callout-inner">
<h3 class="callout-title">Finding plugins<a class="anchor" aria-label="anchor" href="#finding-plugins"></a>
</h3>
<div class="callout-content">
<p>Napari hub contains hundreds of plugins with varying quality, written
by many different developers. It can be difficult to choose which
plugins to use!</p>
<ul>
<li>Search for cell tracking plugins on <a href="https://www.napari-hub.org/" class="external-link">Napari hub</a>
</li>
<li>Look at some of the plugin summaries and ‘Activity’ tabs</li>
<li>What factors could help you decide if the plugin is well
maintained?</li>
<li>What factors could help you decide if the plugin is popular with
Napari users?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<div class="section level3">
<h3 id="is-a-plugin-well-maintained">Is a plugin well maintained?<a class="anchor" aria-label="anchor" href="#is-a-plugin-well-maintained"></a>
</h3>
<p>Some factors to look for:</p>
<p><strong>Last updated</strong><br>
Check when the plugin was last updated - was it recently? This is shown
in the search list summary and under ‘Maintenance’ in the activity tab.
In the activity tab, you can also look at a graph summarising the
‘commits’ over the past year - ‘commits’ are made when someone updates
their plugin with new changes.</p>
<p><strong>Documentation</strong><br>
Is the plugin summary (+ any linked documentation) detailed enough to
explain how to use the plugin?</p>
</div>
<div class="section level3">
<h3 id="is-a-plugin-popular">Is a plugin popular?<a class="anchor" aria-label="anchor" href="#is-a-plugin-popular"></a>
</h3>
<p>Some factors to look for:</p>
<p><strong>Installs</strong><br>
Check how many times a plugin has been installed - a higher number of
installs usually means it’s more popular in the Napari community. The
installs are shown in the search list summary and under ‘Usage’ in the
activity tab. In the activity tab, you can also look at a graph
summarising the installs over the past year.</p>
<p><strong>Image.sc</strong><br>
It can also be useful to search the plugin’s name on the <a href="https://forum.image.sc/" class="external-link">image.sc</a> forum to browse relevant
posts and see if other people had good experiences using it. Image.sc is
also a great place to get help and advice from other plugin users, or
the plugin’s developers.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="image-histograms"><h2 class="section-heading">Image histograms<a class="anchor" aria-label="anchor" href="#image-histograms"></a>
</h2>
<hr class="half-width">
<p>Let’s use our newly installed plugin to look at the human mitosis
image. If you don’t have it open, go the top menubar and select:<br><code>File &gt; Open Sample &gt; napari builtins &gt; Human Mitosis</code></p>
<p>Then open the image histogram with:<br><code>Plugins &gt; napari Matplotlib &gt; Histogram</code></p>
<p>You should see a histogram open on the right side of the image:</p>
<figure><img src="../fig/mitosis-histogram.png" style="width:70.0%" alt="Screenshot of image histogram for mitosis image" class="figure mx-auto d-block"></figure><p>This histogram summarises the pixel values of the entire image. On
the x axis is the pixel value which run from 0-255 for this 8-bit image.
This is split into a number of ‘bins’ of a certain width (for example,
it could be 0-10, 11-20 and so on…). Each bin has a blue bar whose
height represents the number of pixels with values in that bin. So, for
example, for our mitosis image we see the highest bars to the left, with
shorter bars to the right. This means this image has a lot of very dark
(low intensity) pixels and fewer bright (high intensity) pixels.</p>
<p>The vertical white lines at 0 and 255 represent the current ‘contrast
limits’ - we’ll look at this in detail in a <a href="#brightness-and-contrast">later section of this episode</a>.</p>
<p>Let’s quickly compare to another image. Open the ‘coins’ image
with:<br><code>File &gt; Open Sample &gt; napari builtins &gt; Coins</code></p>
<figure><img src="../fig/coins-histogram.png" style="width:68.0%" alt="Screenshot of image histogram for coins image" class="figure mx-auto d-block"></figure><p>From the histogram, we can see that this image has a wider spread of
pixel values. There are bars of similar height across many different
values (rather than just one big peak at the left hand side).</p>
<p>Image histograms are a great way to quickly summarise and compare
pixel values of different images.</p>
<div id="histograms" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="histograms" class="callout-inner">
<h3 class="callout-title">Histograms<a class="anchor" aria-label="anchor" href="#histograms"></a>
</h3>
<div class="callout-content">
<p>Match each of these small test images to their corresponding
histogram. You can assume that all images are displayed with a standard
gray colormap and default contrast limits of the min/max possible pixel
values:</p>
<figure><img src="../fig/exercise-hist-images.png" style="width:68.0%" alt="Screenshot of 4 small grayscale test images" class="figure mx-auto d-block"></figure><figure><img src="../fig/exercise-histograms.png" alt="Screenshot of 4 histograms, corresponding to the test images" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<ul>
<li>a - 3</li>
<li>b - 4</li>
<li>c - 2</li>
<li>d - 1</li>
</ul>
</div>
</div>
</div>
</div>
</section><section id="changing-display-settings"><h2 class="section-heading">Changing display settings<a class="anchor" aria-label="anchor" href="#changing-display-settings"></a>
</h2>
<hr class="half-width">
<p>What happens to pixel values when we change the display settings? Try
changing the contrast limits or colormap in the layer controls. You
should see that the blue bars of the histogram stay the same, no matter
what settings you change i.e. the display settings don’t affect the
underlying pixel values.</p>
<p>This is one of the reasons it’s <em>important to use software
designed for scientific analysis</em> to work with your light microscopy
images. Software like Napari and ImageJ will try to ensure that your
pixel values remain unchanged, while other image editing software
(designed for working with photographs) may change the pixel values in
unexpected ways. Also, even with scientific software, some image
processing steps (that we’ll see in later episodes) will change the
pixel values.</p>
<p>Keep this in mind and make sure you always retain a copy of your
original data, in its original file format! We’ll see in the <a href="filetypes-and-metadata.html">‘Filetypes and metadata’ episode</a>
that original image files contain important metadata that should be kept
for future reference.</p>
</section><section id="colormaps-luts"><h2 class="section-heading">Colormaps / LUTs<a class="anchor" aria-label="anchor" href="#colormaps-luts"></a>
</h2>
<hr class="half-width">
<p>Let’s dig deeper into Napari’s colormaps. As we saw in the <a href="what-is-an-image.html">‘What is an image?’ episode</a>, images are
represented by arrays of numbers (pixel values) with certain dimensions
and data type. Napari (or any other image viewer) has to convert these
numbers into coloured squares on our screen to allow us to view and
interpret the image. Colormaps (also known as lookup tables or LUTs) are
a way to convert pixel values into corresponding colours for display.
For example, remember the image at the beginning of this episode,
showing an image array using three different colormaps:</p>
<figure><img src="../fig/same-array-diff-display.png" style="width:70.0%" alt="Diagram showing an image array (top) with three different colormap  options (bottom)" class="figure mx-auto d-block"></figure><p>Napari supports a wide range of colormaps that can be selected from
the ‘colormap’ menu in the layer controls (as we saw in the <a href="imaging-software.html">Imaging Software episode</a>). For example,
see the diagram below showing the ‘gray’ colormap, where every pixel
value from 0-255 is matched to a shade of gray:</p>
<figure><img src="../fig/gray-colorbar.png" style="width:50.0%" alt="Grey colormap shown as a colorbar with corresponding pixel values" class="figure mx-auto d-block"></figure><p>See the diagram below for examples of 4 different colormaps applied
to the ‘coins’ image from Napari, along with corresponding image
histograms:</p>
<figure><img src="../fig/colorbar-comparison.png" style="width:90.0%" alt="Diagram showing histograms, colorbars and images for the gray, green,  viridis and inferno colormap applied on the coins image" class="figure mx-auto d-block"></figure><p>Why would we want to use different colormaps?</p>
<ul>
<li>to highlight specific features in an image</li>
<li>to help with overlaying multiple images, as we saw in the <a href="imaging-software.html">Imaging Software episode</a> when we
displayed green nuclei and magenta membranes together.</li>
<li>to help interpretation of an image. For example, if we used a red
fluorescent label in an experiment, then using a red colormap might help
people understand the image quickly.</li>
</ul></section><section id="brightness-and-contrast"><h2 class="section-heading">Brightness and contrast<a class="anchor" aria-label="anchor" href="#brightness-and-contrast"></a>
</h2>
<hr class="half-width">
<p>As the final section of this episode, let’s learn more about the
‘contrast limits’ in Napari. As we saw in the <a href="imaging-software.html">Imaging Software episode</a>, adjusting the
contrast limits in the layer controls changes how bright different parts
of the image are displayed. What is really going on here though?</p>
<p>In fact, the ‘contrast limits’ are adjusting how our colormap gets
applied to the image. For example, consider the standard gray
colormap:</p>
<figure><img src="../fig/contrast-comparison-0-255.png" style="width:90.0%" alt="Histogram, colorbar and image corresponding to coins coloured by the  gray colormap. Contrast limits 0 and 255." class="figure mx-auto d-block"></figure><p>For an 8-bit image, the range of colours from black to white are
normally spread from 0 (the minimum pixel value) to 255 (the maximum
pixel value). If we move the left contrast limits node, we change where
the colormap starts from e.g.  for a value of 150 we get:</p>
<figure><img src="../fig/contrast-comparison-150-255.png" style="width:90.0%" alt="Histogram, colorbar and image corresponding to coins coloured by the  gray colormap. Contrast limits 150 and 255." class="figure mx-auto d-block"></figure><p>Now all the colours from black to white are spread over a smaller
range of pixel values from 150-255 and everything below 150 is set to
black. Note that in Napari you can set specific values for the contrast
limits by right clicking on the contrast limits slider. As you adjust
the contrast limits, the vertical white lines on the
<code>napari-matplotlib</code> histogram will move to match.</p>
<p>If we move the right contrast limits node, we change where the
colormap ends (i.e. where pixels are fully white). For example, for
contrast limits of 150 and 200:</p>
<figure><img src="../fig/contrast-comparison-150-200.png" style="width:90.0%" alt="Histogram, colorbar and image corresponding to coins coloured by the  gray colormap. Contrast limits 150 and 200." class="figure mx-auto d-block"></figure><p>Now the range of colours from black to white only cover the pixel
values from 150-200, everything below is black and everything above is
white.</p>
<p>Why do we need to adjust contrast limits?</p>
<ul>
<li><p>to allow us to see low contrast features. Some parts of your
image may only differ slightly in their pixel value (low contrast).
Bringing the contrast limits closer together allows a small change in
pixel value to be represented by a bigger change in colour from the
colormap.</p></li>
<li><p>to focus on specific features. For example, increasing the lower
contrast limit will remove any low intensity parts of the image from the
display.</p></li>
</ul>
<div id="adjusting-contrast" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="adjusting-contrast" class="callout-inner">
<h3 class="callout-title">Adjusting contrast<a class="anchor" aria-label="anchor" href="#adjusting-contrast"></a>
</h3>
<div class="callout-content">
<p>Open the Napari console with the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/console.svg" alt="A screenshot of Napari's console button" height="30" class="figure"> button
and copy and paste the code below:</p>
<pre><code>import numpy as np
from skimage.draw import disk
image = np.full((100,100), 10, dtype="uint8")
image[:, 50:100] = 245
image[disk((50, 25),15)] = 11
image[disk((50, 75),15)] = 247
viewer.add_image(image)</code></pre>
<p>This 2D image contains two hidden circles:</p>
<ul>
<li>Adjust Napari’s contrast limits to view both</li>
<li>What contrast limits allow you to see each circle? (remember you can
right click on the contrast limit bar to see specific values). Why do
these limits work?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<p>The left circle can be seen with limits of e.g. 0 and 33</p>
<p>These limits work because the background on the left side of the
image has a pixel value of 10 and the circle has a value of 11. By
moving the upper contrast limit to the left we force the colormap to go
from black to white over a smaller range of pixel values. This allows
this small difference in pixel value to be represented by a larger
difference in colour and therefore made visible.</p>
<p>The right circle can be seen with limits of e.g. 231 and 255. This
works for a very similar reason - the background has a pixel value of
245 and the circle has a value of 247.</p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>The same image array can be displayed in many different ways in
Napari</li>
<li>Image histograms provide a useful overview of the pixel values in an
image</li>
<li>Plugins can be searched for on Napari hub and installed via
Napari</li>
<li>Colormaps (or LUTs) map pixel values to colours</li>
<li>Contrast limits change the range of pixel values the colormap is
spread over</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-multi-dimensional-images"><p>Content from <a href="multi-dimensional-images.html">Multi-dimensional images</a></p>
<hr>
<p> Last updated on 2024-04-26 | 
        
        <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/edit/main/episodes/multi-dimensional-images.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 60 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do we visualise and work with images with more than 2
dimensions?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain how different axes (xyz, channels, time) are stored in image
arrays and displayed</li>
<li>Open and navigate images with different dimensions in Napari</li>
<li>Explain what RGB images are and how they are handled in Napari</li>
<li>Split and merge channels in Napari</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="image-dimensions-axes"><h2 class="section-heading">Image dimensions / axes<a class="anchor" aria-label="anchor" href="#image-dimensions-axes"></a>
</h2>
<hr class="half-width">
<p>As we saw in the <a href="what-is-an-image.html">‘What is an image?’
episode</a>, image pixel values are stored as arrays of numbers with
certain dimensions and data type. So far we have focused on grayscale 2D
images that are represented by a 2D array:</p>
<figure><img src="../fig/array.png" style="width:80.0%" alt="A diagram comparing the array of numbers and image  display for a simplified image of an arrow" class="figure mx-auto d-block"></figure><p>Light microscopy data varies greatly though, and often has more
dimensions representing:</p>
<ul>
<li><p><strong>Time (t):</strong><br>
Multiple images of the same sample over a certain time period - also
known as a ‘time series’. This is useful to evaluate dynamic
processes.</p></li>
<li><p><strong>Channels (c):</strong><br>
Usually, this is multiple images of the same sample under different
wavelengths of light (e.g. red, green, blue channels, or wavelengths
specific to certain fluorescent markers). Note that channels can
represent many more values though, as we will <a href="#channels">see
below</a>.</p></li>
<li><p><strong>Depth (z):</strong><br>
Multiple images of the same sample, taken at different depths. This will
produce a 3D volume of images, allowing the shape and position of
objects to be understood in full 3D.</p></li>
</ul>
<p>These images will be stored as arrays that have more than two
dimensions. Let’s start with our familiar human mitosis image, and work
up to some more complex imaging data.</p>
</section><section id="d"><h2 class="section-heading">2D<a class="anchor" aria-label="anchor" href="#d"></a>
</h2>
<hr class="half-width">
<p>Go to the top menu-bar of Napari and select:<br><code>File &gt; Open Sample &gt; napari builtins &gt; Human Mitosis</code></p>
<figure><img src="../fig/human-mitosis-napari.png" alt="A screenshot of a 2D image of human cells  undergoing mitosis in Napari" class="figure mx-auto d-block"></figure><p>We can see this image only has two dimensions (or two ‘axes’ as
they’re also known) due to the lack of sliders under the image, and by
checking its shape in the Napari console. Remember that we looked at <a href="what-is-an-image.html#images-are-arrays-of-numbers">how to open
the console</a> and how to check the <a href="what-is-an-image.html#image-dimensions"><code>.shape</code></a> in
detail in the <a href="what-is-an-image.html">‘What is an image?’
episode</a>:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>image <span class="op">=</span> viewer.layers[<span class="st">"human_mitosis"</span>].data</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="bu">print</span>(image.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># (y, x)
(512, 512)</code></pre>
</div>
<p>Note that comments have been added to all output sections in this
episode (the lines starting with #). These state what the dimensions
represent (e.g. (y, x) for the y and x axes). These comments won’t
appear in the output in your console.</p>
</section><section id="d-1"><h2 class="section-heading">3D<a class="anchor" aria-label="anchor" href="#d-1"></a>
</h2>
<hr class="half-width">
<p>Let’s remove the mitosis image by clicking the remove layer button
<img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/delete.svg" alt="A screenshot of Napari's delete layer button" height="30" class="figure"> at
the top right of the layer list. Then, let’s open a new 3D image:<br><code>File &gt; Open Sample &gt; napari builtins &gt; Brain (3D)</code></p>
<figure><img src="../fig/brain-napari.png" alt="A screenshot of a head X-ray in Napari" class="figure mx-auto d-block"></figure><p>This image shows part of a human head acquired using X-ray
Computerised Tomography (CT). We can see it has three dimensions due to
the slider at the base of the image, and the shape output:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>image <span class="op">=</span> viewer.layers[<span class="st">"brain"</span>].data</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="bu">print</span>(image.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># (z, y, x)
(10, 256, 256)</code></pre>
</div>
<p>This 3D image can be thought of as a stack of ten 2D images, with
each image containing a 256x256 array. The x/y axes point along the
width/height of the first 2D image, and the z axis points along the
stack. It is stored as a 3D array:</p>
<figure><img src="../fig/2d-3d-arrays.png" style="width:80.0%" alt="A diagram comparing 2D and  3D image arrays" class="figure mx-auto d-block"></figure><p>In Napari (and Python in general), dimensions are referred to by
their index e.g. here dimension 0 is the z axis, dimension 1 is the y
axis and dimension 2 is the x axis. We can check this in Napari by
looking at the number at the very left of the slider. Here it’s labelled
‘0’, showing that it controls movement along dimension 0 (i.e. the z
axis).</p>
<div id="axis-labels" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="axis-labels" class="callout-inner">
<h3 class="callout-title">Axis labels<a class="anchor" aria-label="anchor" href="#axis-labels"></a>
</h3>
<div class="callout-content">
<p>By default, sliders will be labelled by the index of the dimension
they move along e.g. 0, 1, 2… Note that it is possible to re-name these
though! For example, if you click on the number at the left of the
slider, you can freely type in a new value. This can be useful to label
sliders with informative names like ‘z’, or ‘time’.</p>
<p>You can also check which labels are currently being used with:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>viewer.dims.axis_labels</span></code></pre>
</div>
</div>
</div>
</div>
</section><section id="channels"><h2 class="section-heading">Channels<a class="anchor" aria-label="anchor" href="#channels"></a>
</h2>
<hr class="half-width">
<p>Next, let’s look at a 3D image where an additional (fourth) dimension
contains data from different ‘channels’. Remove the brain image and
select:<br><code>File &gt; Open Sample &gt; napari builtins &gt; Cells (3D+2Ch)</code></p>
<figure><img src="../fig/cells-napari.png" alt="A screenshot of a flourescence microscopy image  of some cells in Napari" class="figure mx-auto d-block"></figure><p>Image channels can be used to store data from multiple sources for
the same location. For example, consider the diagram below. On the left
is shown a 2D image array overlaid on a simple cell with a nucleus. Each
of the pixel locations e.g. (0, 0), (1, 1), (2, 1)… can be considered as
sampling locations where different measurements can be made. For
example, this could be the intensity of light detected with different
wavelengths (like red, green…) at each location, or it could be
measurements of properties like the surface height and elasticity at
each location (like from <a href="https://www.nature.com/articles/s43586-021-00033-2" class="external-link">scanning probe
microscopy</a>). These separate data sources are stored as ‘channels’ in
our image.</p>
<figure><img src="../fig/channel-arrays.png" style="width:80.0%" alt="A diagram showing different kinds of channels  for a 4x4 image of a cell e.g. red / green / surface height / elasticity" class="figure mx-auto d-block"></figure><p>This diagram shows an example of a 2D image with channels, but this
can also be extended to 3D images, as we will see now.</p>
<p>Fluorescence microscopy images, like the one we currently have open
in Napari, are common examples of images with multiple channels. In
flourescence microscopy, different ‘flourophores’ are used that target
specific features (like the nucleus or cell membrane) and emit light of
different wavelengths. Images are taken filtering for each of these
wavelengths in turn, giving one image channel per fluorophore. In this
case, there are two channels - one for a flourophore targeting the
nucleus, and one for a fluorophore targeting the cell membrane. These
are shown as separate image layers in Napari’s layer list:</p>
<figure><img src="../fig/layer-list.png" alt="A screenshot of Napari's layer list, showing two  image layers named 'nuclei' and 'membrane'" class="figure mx-auto d-block"></figure><p>Recall from the <a href="imaging-software.html">imaging software
episode</a> that ‘layers’ are how Napari displays multiple items
together in the viewer. Each layer could be an entire image, part of an
image like a single channel, a series of points or shapes etc. Each
layer is displayed as a named item in the layer list, and can have
various display settings adjusted in the layer controls.</p>
<p>Let’s check the shape of both layers:</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>nuclei <span class="op">=</span> viewer.layers[<span class="st">"nuclei"</span>].data</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>membrane <span class="op">=</span> viewer.layers[<span class="st">"membrane"</span>].data</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="bu">print</span>(nuclei.shape)</span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="bu">print</span>(membrane.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># (z, y, x)
(60, 256, 256)
(60, 256, 256)</code></pre>
</div>
<p>This shows that each layer contains a 3D image array of size
60x256x256. Each 3D image represents the intensity of light detected at
each (z, y, x) location with a wavelength corresponding to the given
fluorophore.</p>
<p>Here, Napari has automatically recognised that this image contains
different channels and separated them into different image layers. This
is not always the case though, and sometimes it <a href="#when-to-split-channels-into-layers-in-napari">may be preferable
to not split the channels into separate layers</a>! We can merge our
channels again by selecting both image layers (shift + click so they’re
both highlighted in blue), right clicking on them and selecting:<br><code>Merge to stack</code></p>
<p>You should see the two image layers disappear, and a new combined
layer appear in the layer list (labelled ‘membrane’). This image has an
extra slider that allows switching channels - try moving both sliders
and see how the image display changes.</p>
<p>We can check the image’s dimensions with:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a></span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>image <span class="op">=</span> viewer.layers[<span class="st">"membrane"</span>].data</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="bu">print</span>(image.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># (c, z, y, x)
(2, 60, 256, 256)</code></pre>
</div>
<p>Notice how these dimensions match combining the two layers shown
above. Two arrays with 3 dimensions (60, 256, 256) are combined to give
one array with four dimensions (2, 60, 256, 256) with the first axis
representing the 2 channels. See the diagram below for a visualisation
of how these 3D and 4D arrays compare:</p>
<figure><img src="../fig/3d-4d-arrays.png" style="width:80.0%" alt="A diagram comparing image arrays with three  (z, y, x) and four (c, z, y, x) dimensions" class="figure mx-auto d-block"></figure><p>As we’ve seen before, the labels on the left hand side of each slider
in Napari matches the index of the dimension it moves along. The top
slider (labelled 1) moves along the z axis, while the bottom slider
(labelled 0) switches channels.</p>
<p>We can separate the channels again by right clicking on the
‘membrane’ image layer and selecting:<br><code>Split Stack</code></p>
<p>Note that this resets the contrast limits for membrane and nuclei to
defaults of the min/max possible values. You’ll need to adjust the
contrast limits on the membrane layer to see it clearly again after
splitting.</p>
<div id="reading-channels-in-napari" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="reading-channels-in-napari" class="callout-inner">
<h3 class="callout-title">Reading channels in Napari<a class="anchor" aria-label="anchor" href="#reading-channels-in-napari"></a>
</h3>
<div class="callout-content">
<p>When we open the cells image through the Napari menu, it is really
calling something like:</p>
<pre><code>from skimage import data
viewer.add_image(data.cells3d(), channel_axis=1)</code></pre>
<p>This adds the cells 3D image (which is stored as zcyx), and specifies
that dimension 1 is the channel axis. This allows Napari to split the
channels automatically into different layers.</p>
<p>Usually when loading your own images into Napari e.g. with the
napari-aicsimageio plugin (as we will see in the <a href="filetypes-and-metadata.html">filetypes and metadata episode</a>),
the channel axis should be recognised automatically. If not, you may
need to add the image via the console as above, manually stating the
channel axis.</p>
</div>
</div>
</div>
<div id="when-to-split-channels-into-layers-in-napari" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="when-to-split-channels-into-layers-in-napari" class="callout-inner">
<h3 class="callout-title">When to split channels into layers in
Napari?<a class="anchor" aria-label="anchor" href="#when-to-split-channels-into-layers-in-napari"></a>
</h3>
<div class="callout-content">
<p>As we saw above, we have two choices when loading images with
multiple channels into Napari:</p>
<ul>
<li><p>Load the image as one Napari layer, and use a slider to change
channel</p></li>
<li><p>Load each channel as a separate Napari layer, e.g. using the
<code>channel_axis</code></p></li>
</ul>
<p>Both are useful ways to view multichannel images, and which you
choose will depend on your image and preference. Some pros and cons are
shown below:</p>
<div id="accordionSpoiler1" class="accordion spoiler-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button spoiler-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSpoiler1" aria-expanded="false" aria-controls="collapseSpoiler1">
  <h3 class="accordion-header" id="headingSpoiler1">
<div class="note-square"><i aria-hidden="true" class="callout-icon" data-feather="eye"></i></div>Pros and cons</h3>
</button>
<div id="collapseSpoiler1" class="accordion-collapse collapse" aria-labelledby="headingSpoiler1" data-bs-parent="#accordionSpoiler1">
<div class="accordion-body">
<p><strong>Channels as separate layers</strong></p>
<ol style="list-style-type: decimal">
<li><p>Channels can be overlaid on top of each other (rather than only
viewed one at a time)</p></li>
<li><p>Channels can be easily shown/hidden with the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility.svg" alt="A screenshot of Napari's eye button" height="20" class="figure">
icons</p></li>
<li><p>Display settings like contrast limits and colormaps can be
controlled independently for each channel (rather than only for the
entire image)</p></li>
</ol>
<p><strong>Entire image as one layer</strong></p>
<ol style="list-style-type: decimal">
<li><p>Useful when handling a very large number of channels. For
example, if we have hundreds of channels, then moving between them on a
slider may be simpler and faster.</p></li>
<li><p>Napari can become slow, or even crash with a very large number of
layers. Keeping the entire image as one layer can help prevent
this.</p></li>
<li><p>Keeping the full image as one layer can be helpful when running
certain processing operations across multiple channels at once</p></li>
</ol>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</section><section id="time"><h2 class="section-heading">Time<a class="anchor" aria-label="anchor" href="#time"></a>
</h2>
<hr class="half-width">
<p>You should have already downloaded the MitoCheck dataset as part of
the <a href="index.html#setup">setup instructions</a> - if not, you can
download it by clicking on ‘00001_01.ome.tiff’ on <a href="https://docs.openmicroscopy.org/ome-model/5.6.3/ome-tiff/data.html#mitocheck" class="external-link">this
page of the OME website</a>.</p>
<p>To open it in Napari, remove any existing image layers, then drag and
drop the file over the canvas. A popup may appear asking you to choose a
‘reader’ - you can select either ‘napari builtins’ or
‘napari-aicsimageio’. We’ll see in the next episode that the
‘napari-aicsimageio’ reader gives us access to useful image
metadata.</p>
<p>Note this image can take a while to open, so give it some time!
Alternatively, you can select in the top menu-bar:<br><code>File &gt; Open File(s)...</code></p>
<figure><img src="../fig/cells-time-napari.png" alt="A screenshot of a 2D time series in Napari" class="figure mx-auto d-block"></figure><p>This image is a 2D time series (tyx) of some human cells undergoing
mitosis. The slider at the bottom now moves through time, rather than z
or channels. Try moving the slider from left to right - you should see
some nuclei divide and the total number of nuclei increase. You can also
press the small <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/right_arrow.svg" alt="A screenshot of Napari's play button" height="20" class="figure"> icon at
the left side of the slider to automatically move along it. The icon
will change into a <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/square.svg" alt="A screenshot of Napari's stop button" height="25" class="figure">- pressing
this will stop the movement.</p>
<p>We can again check the image dimensions by running the following:</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>image <span class="op">=</span> viewer.layers[<span class="st">"00001_01.ome"</span>].data</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="bu">print</span>(image.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># (t, y, x)
(93, 1024, 1344)</code></pre>
</div>
<p>Note that this image has a total of 3 dimensions, and so it will also
be stored in a 3D array:</p>
<figure><img src="../fig/tyx-array.png" style="width:40.0%" alt="A diagram of a tyx image array" class="figure mx-auto d-block"></figure><p>This makes the point that the dimensions don’t always represent the
same quantities. For example, a 3D image array with shape (512, 512,
512) could represent a zyx, cyx or tyx image. We’ll discuss this more in
the next section.</p>
</section><section id="dimension-order"><h2 class="section-heading">Dimension order<a class="anchor" aria-label="anchor" href="#dimension-order"></a>
</h2>
<hr class="half-width">
<p>As we’ve seen so far, we can check the number and size of an image’s
dimensions by running:</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>image.shape</span></code></pre>
</div>
<p>Napari reads this array and displays the image appropriately, with
the correct number of sliders, based on these dimensions. It’s worth
noting though that Napari doesn’t usually know what these different
dimensions represent e.g.  consider a 4 dimensional image with shape
(512, 512, 512, 512). This could be tcyx, czyx, tzyx etc… Napari will
just display it as an image with 2 additional sliders, not caring about
exactly what each represents.</p>
<p>Python has certain conventions for the order of image axes (like <a href="https://scikit-image.org/docs/stable/user_guide/numpy_images.html#coordinate-conventions" class="external-link">scikit-image’s
‘coordinate conventions’</a> and <a href="https://allencellmodeling.github.io/aicsimageio/aicsimageio.aics_image.AICSImage.html" class="external-link">aicsimageio’s
reader</a>) - but this tends to vary based on the library or plugin
you’re using. These are not firm rules!</p>
<p>Therefore, it’s always worth checking you understand which axes are
being shown in any viewer and what they represent! Check against your
prior knowledge of the experimental setup, and check the metadata in the
original image (we’ll look at this in the next episode). If you want to
change how axes are displayed, remember you can use the roll or
transpose dimensions buttons as discussed in the <a href="imaging-software.html">imaging software episode</a>. Also, if
loading the image manually from the console, you can provide some extra
information like the <code>channel_axis</code> parameter discussed
above.</p>
<div id="interpreting-image-dimensions" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="interpreting-image-dimensions" class="callout-inner">
<h3 class="callout-title">Interpreting image dimensions<a class="anchor" aria-label="anchor" href="#interpreting-image-dimensions"></a>
</h3>
<div class="callout-content">
<p>Remove all image layers, then open a new image by copying and pasting
the following into Napari’s console:</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="im">from</span> skimage <span class="im">import</span> data</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>image <span class="op">=</span> viewer.add_image(data.kidney(), rgb<span class="op">=</span><span class="va">False</span>).data</span></code></pre>
</div>
<p>This is a fluorescence microscopy image of mouse kidney tissue.</p>
<ol style="list-style-type: decimal">
<li><p>How many dimensions does it have?</p></li>
<li><p>What do each of those dimensions represent? (e.g. t, c, z, y, x)
<strong>Hint:</strong> try using the roll dimensions button <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/roll.svg" alt="A screenshot of Napari's roll dimensions button" height="25" class="figure">
to view different combinations of axes.</p></li>
<li><p>Once you know which dimension represent channels, remove the
image and load it again with:</p></li>
</ol>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="co"># Replace ? with the correct channel axis</span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>viewer.add_image(data.kidney(), rgb<span class="op">=</span><span class="va">False</span>, channel_axis<span class="op">=</span>?)</span></code></pre>
</div>
<p>Note that using the wrong channel axis may cause Napari to crash. If
this happens to you just restart Napari and try again. Bear in mind, as
we saw in the <a href="#when-to-split-channels-into-layers-in-napari">channel splitting
section</a> that large numbers of layers can be difficult to handle, so
it isn’t usually advisable to use ‘channel_axis’ on dimensions with a
large size.</p>
<ol start="4" style="list-style-type: decimal">
<li>How many channels does the image have?</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="section level3">
<h3 id="section">1<a class="anchor" aria-label="anchor" href="#section"></a>
</h3>
<p>The image has 4 dimensions, which we can see with:</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>image.shape</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(16, 512, 512, 3)</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="section-1">2<a class="anchor" aria-label="anchor" href="#section-1"></a>
</h3>
<p>If we press the roll dimensions button <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/roll.svg" alt="A screenshot of Napari's roll dimensions button" height="25" class="figure">
once, we can see an image of various cells and nuclei. Moving the slider
labelled ‘0’ seems to move up and down in this image (i.e. the z axis),
while moving the slider labelled ‘3’ changes between highlighting
different features like nuclei and cell edges (i.e. channels).
Therefore, the remaining two axes (1 and 2) must be y and x. This means
the image’s 4 dimensions are (z, y, x, c)</p>
</div>
<div class="section level3">
<h3 id="section-2">3<a class="anchor" aria-label="anchor" href="#section-2"></a>
</h3>
<p>The channel axis is 3 (remember that the numbering always starts form
0!)</p>
</div>
<div class="section level3">
<h3 id="section-3">4<a class="anchor" aria-label="anchor" href="#section-3"></a>
</h3>
<p>There are 3 channels which we can see from the <code>.shape</code>
output, or from the number of layers in the layer list.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="rgb"><h2 class="section-heading">RGB<a class="anchor" aria-label="anchor" href="#rgb"></a>
</h2>
<hr class="half-width">
<p>For the final part of this episode, let’s look at RGB images. RGB
images can be considered as a special case of a 2D image with channels
(yxc). In this case, there are always 3 channels - with one representing
red (R), one representing green (G) and one representing blue (B).</p>
<p>Let’s open an example RGB image with the command below. Make sure you
remove any existing image layers first!<br><code>File &gt; Open Sample &gt; napari builtins &gt; Skin (RGB)</code></p>
<figure><img src="../fig/skin-napari.png" alt="A screenshot of an H+E slide of skin layers  in Napari" class="figure mx-auto d-block"></figure><p>This image is a hematoxylin and eosin stained slide of dermis and
epidermis (skin layers). Let’s check its shape:</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>image <span class="op">=</span> viewer.layers[<span class="st">"skin"</span>].data</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="bu">print</span>(image.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># (y, x, c)
(960, 1280, 3)</code></pre>
</div>
<p>Notice that the channels aren’t separated out into different image
layers, as they were for the multichannel images above. Instead, they
are shown combined together as a single image. If you hover your mouse
over the image, you should see three pixel values printed in the bottom
left representing (R, G, B).</p>
<figure><img src="../fig/rgb-values.png" alt="A screenshot of an H+E slide of skin layers in  Napari, highlighting the (R,G,B) values" class="figure mx-auto d-block"></figure><p>We can see these different channels more clearly if we right click on
the ‘skin’ image layer and select:<br><code>Split RGB</code></p>
<p>This shows the red, green and blue channels as separate image layers.
Try inspecting each one individually by clicking the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility.svg" alt="A screenshot of Napari's eye button" height="30" class="figure"> icons to
hide the other layers.</p>
<p>We can understand these RGB pixel values better by opening a
different sample image. Remove all layers, then select:<br><code>File &gt; Open Sample &gt; napari builtins &gt; Colorwheel (RGB)</code></p>
<figure><img src="../fig/colorwheel-napari.png" alt="A screenshot of a colorwheel in Napari" class="figure mx-auto d-block"></figure><p>This image shows an RGB colourwheel - try hovering your mouse over
different areas, making note of the (R, G, B) values shown in the bottom
left of Napari. You should see that moving into the red area gives high
values for R, and low values for B and G. Equally, the green area shows
high values for G, and the blue area shows high values for B. The red,
green and blue values are mixed together to give the final colour.</p>
<p>Recall from the <a href="image-display.html">image display
episode</a> that for most microscopy images the colour of each pixel is
determined by a colormap (or LUT). Each pixel usually has a single value
that is then mapped to a corresponding colour via the colormap, and
different colormaps can be used to highlight different features. RGB
images are different - here we have three values (R, G, B) that
unambiguously correspond to a colour for display. For example, (255, 0,
0) would always be fully red, (0, 255, 0) would always be fully green
and so on… This is why there is no option to select a colormap in the
layer controls when an RGB image is displayed. In effect, the colormap
is hard-coded into the image, with a full colour stored for every pixel
location.</p>
<p>These kinds of images are common when we are trying to replicate what
can be seen with the human eye. For example, photos taken with a
standard camera or phone will be RGB. They are less common in
microscopy, although there are certain research fields and types of
microscope that commonly use RGB. A key example is imaging of tissue
sections for histology or pathology. You will also often use RGB images
when preparing figures for papers and presentations - RGB images can be
opened in all imaging software (not just scientific research focused
ones), so are useful when preparing images for display. As usual, always
make sure you keep a copy of your original raw data!</p>
<div id="reading-rgb-in-napari" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="reading-rgb-in-napari" class="callout-inner">
<h3 class="callout-title">Reading RGB in Napari<a class="anchor" aria-label="anchor" href="#reading-rgb-in-napari"></a>
</h3>
<div class="callout-content">
<p>How does Napari know an image is RGB? If an image’s final dimension
has a length of 3 or 4, Napari will assume it is RGB and <a href="https://napari.org/stable/howtos/layers/image.html#viewing-rgb-vs-luminance-grayscale-images" class="external-link">display
it as such</a>. If loading an image from the console, you can also
manually set it to load as rgb:</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="im">from</span> skimage <span class="im">import</span> data</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>viewer.add_image(data.astronaut(), rgb<span class="op">=</span><span class="va">True</span>)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="rgb-histograms" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="rgb-histograms" class="callout-inner">
<h3 class="callout-title">RGB histograms<a class="anchor" aria-label="anchor" href="#rgb-histograms"></a>
</h3>
<div class="callout-content">
<p>Make a histogram of the Skin (RGB) image using
<code>napari-matplotlib</code> (as covered in the <a href="image-display.html">image display episode</a>)</p>
<ul>
<li>How does it differ from the image histograms we looked at in the <a href="image-display.html">image display episode</a>?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p>If you haven’t completed the <a href="image-display.html">image
display episode</a>, you will need to <a href="image-display.html#napari-plugins">install the
<code>napari-matplotlib</code> plugin</a>.</p>
<p>Then open a histogram with:<br><code>Plugins &gt; napari Matplotlib &gt; Histogram</code></p>
<figure><img src="../fig/rgb-histogram.png" alt="RGB histogram of the Napari Skin sample image" class="figure mx-auto d-block"></figure><p>This histogram shows separate lines for R, G and B - this is because
each displayed pixel is represented by three values (R, G, B). This
differs from the histograms we looked at previously, where there was
only one line as each displayed pixel was only represented by one
value.</p>
</div>
</div>
</div>
</div>
<div id="understanding-rgb-colours" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="understanding-rgb-colours" class="callout-inner">
<h3 class="callout-title">Understanding RGB colours<a class="anchor" aria-label="anchor" href="#understanding-rgb-colours"></a>
</h3>
<div class="callout-content">
<p>What colour would you expect the following (R, G, B) values to
produce? Each value has a minimum of 0 and a maximum of 255.</p>
<ul>
<li>(0, 0, 255)</li>
<li>(255, 255, 255)</li>
<li>(0, 0, 0)</li>
<li>(255, 255, 0)</li>
<li>(100, 100, 100)</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<figure><img src="../fig/rgb-exercise.png" style="width:50.0%" alt="Diagram of (R, G, B) values next to corresponding  colours" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Microscopy images can have many dimensions, usually representing
time (t), channels (c), and spatial axes (z, y, x)</li>
<li>Napari can open images with any number of dimensions</li>
<li>Napari (and python in general) has no firm rules for axis order.
Different libraries and plugins will often use different
conventions.</li>
<li>RGB images always have 3 channels - red (R), green (G) and blue (B).
These channels aren’t separated into different image layers - they’re
instead combined together to give the final image display.</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-filetypes-and-metadata"><p>Content from <a href="filetypes-and-metadata.html">Filetypes and metadata</a></p>
<hr>
<p> Last updated on 2024-04-26 | 
        
        <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/edit/main/episodes/filetypes-and-metadata.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 55 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Which file formats should be used for microscopy images?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain the pros and cons of some popular image file formats</li>
<li>Explain the difference between lossless and lossy compression</li>
<li>Inspect image metadata with the napari-aicsimageio plugin</li>
<li>Inspect and set an image’s scale in Napari</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="image-file-formats"><h2 class="section-heading">Image file formats<a class="anchor" aria-label="anchor" href="#image-file-formats"></a>
</h2>
<hr class="half-width">
<p>Images can be saved in a wide variety of file formats. You may be
familiar with many of these like JPEG (files ending with .jpg or .jpeg
extension), PNG (.png extension) or TIFF (.tiff or .tif extension).
Microscopy images have an especially wide range of options, with
hundreds of different formats in use, often specific to particular
microscope manufacturers. With this being said, how do we choose which
format(s) to use in our research? It’s worth bearing in mind that the
best format to use will vary depending on your research question and
experimental workflow - so you may need to use different formats on
different research projects.</p>
</section><section id="metadata"><h2 class="section-heading">Metadata<a class="anchor" aria-label="anchor" href="#metadata"></a>
</h2>
<hr class="half-width">
<p>First, let’s look closer at what gets stored inside an image
file.</p>
<p>There are two main things that get stored inside image files:
<em>pixel values</em> and <em>metadata</em>. We’ve looked at pixel
values in previous episodes (<a href="what-is-an-image.html">What is an
image?</a>, <a href="image-display.html">Image display</a> and <a href="multi-dimensional-images.html">Multi-dimensional images</a>
episodes) - this is the raw image data as an array of numbers with
specific dimensions and data type. The metadata, on the other hand, is a
wide variety of additional information about the image and how it was
acquired.</p>
<p>For example, let’s take a look at the metadata in the
‘Plate1-Blue-A-12-Scene-3-P3-F2-03.czi’ file we downloaded as part of
the <a href="index.html#setup">setup instructions</a>. To browse the metadata
we will use a Napari plugin called <a href="https://www.napari-hub.org/plugins/napari-aicsimageio" class="external-link">napari-aicsimageio</a>.
This plugin allows a wide variety of file formats to be opened in Napari
that aren’t supported by default. This plugin was already installed in
the <a href="index.html#setup">setup instructions</a>, so you should be able
to start using it straight away.</p>
<p>Let’s open the ‘Plate1-Blue-A-12-Scene-3-P3-F2-03.czi’ file by
removing any existing image layers, then dragging and dropping it onto
the canvas. In the pop-up menu that appears, select
‘napari-aicsimageio’. Note this can take a while to open, so give it
some time! Alternatively, you can select in Napari’s top menu-bar:<br><code>File &gt; Open File(s)...</code></p>
<figure><img src="../fig/plate1-czi-napari.png" alt="A screenshot of yeast sample data shown in  Napari" class="figure mx-auto d-block"></figure><p>This image is part of a <a href="https://idr.openmicroscopy.org/search/?query=Name:idr0011-ledesmafernandez-dad4/screenD" class="external-link">published
dataset on Image Data Resource</a> (accession number idr0011), and comes
from the <a href="https://downloads.openmicroscopy.org/images/Zeiss-CZI/idr0011/" class="external-link">OME
sample data</a>. It is a 3D fluorescence microscopy image of yeast with
three channels (we’ll explore what these channels represent in the <a href="#exploring-metadata">Exploring metadata exercise</a>). Napari has
automatically recognised these three channels and split them into three
separate image layers. Recall that we looked at how Napari handles
channels in the <a href="multi-dimensional-images.html#channels">multi-dimensional images
episode</a>.</p>
<p>We can browse its metadata by selecting in the top menu-bar:<br><code>Plugins &gt; OME Tree Widget (ome-types)</code></p>
<figure><img src="../fig/ome-tree-widget.png" alt="A screenshot of napari-aicsimageio's OME Tree  Widget" class="figure mx-auto d-block"></figure><p>This opens a panel on the right-hand side of Napari listing different
metadata stored in the file. In this case, we can see it is split into
categories like <code>experimenters</code>, <code>images</code> and
<code>instruments</code>… You can click on the different categories to
expand them and see more detail. For example, under
<code>images &gt; Image:0</code>, we see useful metadata about this
specific image. We can see the <code>acquisition_date</code> when this
image was acquired. Also, under <code>pixels</code> we can see
information about the <code>dimension_order</code>, the size of
different dimensions (<code>size_c</code>, <code>size_t</code>,
<code>size_x</code>, <code>size_y</code> and <code>size_z</code>), the
type and bit-depth (<code>type</code>) and importantly the pixel size
(<code>physical_size_x</code>, <code>physical_size_x_unit</code> etc.).
The pixel size is essential for making accurate quantitative
measurements from our images, and will be discussed <a href="#pixel-size">in the next section of this episode</a>.</p>
<p>This metadata is a vital record of exactly how the image was acquired
and what it represents. As we’ve mentioned in previous episodes - it’s
important to maintain this metadata as a record for the future. It’s
also essential to allow us to make quantitative measurements from our
images, by understanding factors like the pixel size. Converting between
file formats can result in loss of metadata, so it’s always worthwhile
keeping a copy of your image safely in its original raw file format. You
should take extra care to ensure that additional processing steps don’t
overwrite this original image.</p>
<div id="exploring-metadata-in-the-console" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="exploring-metadata-in-the-console" class="callout-inner">
<h3 class="callout-title">Exploring metadata in the console<a class="anchor" aria-label="anchor" href="#exploring-metadata-in-the-console"></a>
</h3>
<div class="callout-content">
<p>Note that you can also inspect some metadata via the console (as
below). Here we use python’s <a href="https://github.com/Textualize/rich" class="external-link">rich</a> library to add
colour-coding to make the output easier to read:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> rich</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>rich.<span class="bu">print</span>(viewer.layers[<span class="dv">0</span>].metadata)</span></code></pre>
</div>
<figure><img src="../fig/metadata-console.png" alt="Screenshot of metadata printed to Napari's  console" class="figure mx-auto d-block"></figure><p>See the <a href="https://github.com/AllenCellModeling/napari-aicsimageio?tab=readme-ov-file#access-to-the-aicsimage-object-and-metadata" class="external-link">napari-aicsimageio
readme</a> for details.</p>
</div>
</div>
</div>
<div id="exploring-metadata" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exploring-metadata" class="callout-inner">
<h3 class="callout-title">Exploring metadata<a class="anchor" aria-label="anchor" href="#exploring-metadata"></a>
</h3>
<div class="callout-content">
<p>Explore the metadata in the OME Tree Widget panel to answer the
following questions:</p>
<ul>
<li>What manufacturer made the microscope used to take this image?</li>
<li>What detector was used to take this image?</li>
<li>What does each channel show? For example, which fluorophore is used?
What is its excitation and emission wavelength?</li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="section level3">
<h3 id="microscope-model">Microscope model<a class="anchor" aria-label="anchor" href="#microscope-model"></a>
</h3>
<p>Under <code>instruments &gt; Instrument:0 &gt; microscope</code>, we
can see that the manufacturer was Zeiss.</p>
</div>
<div class="section level3">
<h3 id="detector">Detector<a class="anchor" aria-label="anchor" href="#detector"></a>
</h3>
<p>Under
<code>instruments &gt; Instrument:0 &gt; detectors &gt; Detector:HDCam</code>,
we can see that this used an HDCamC10600-30B (ORCA-R2) detector.</p>
</div>
<div class="section level3">
<h3 id="channels">Channels<a class="anchor" aria-label="anchor" href="#channels"></a>
</h3>
<p>Under <code>images &gt; Image:0 &gt; pixels &gt; channels</code>, we
can see one entry per channel - <code>Channel:1-0</code>,
<code>Channel:2-0</code> and <code>Channel:3-0</code>.</p>
<p>Expanding the first, we can see that its <code>fluor</code> is
TagYFP, a fluorophore with <code>emission_wavelength</code> of 524nm and
<code>excitation_wavelength</code> of 508nm.</p>
<p>Expanding the second, we can see that its <code>fluor</code> is
mRFP1.2, a fluorophore with <code>emission_wavelength</code> of 612nm
and <code>excitation_wavelength</code> of 590nm.</p>
<p>Expanding the third, we see that it has no <code>fluor</code>,
<code>emission_wavelength</code> or <code>excitation_wavelength</code>
listed. Its <code>illumination_type</code> is listed as ‘transmitted’,
so this is simply an image of the yeast cells with no fluorophores
used.</p>
</div>
</div>
</div>
</div>
</div>
<div id="napari-aicsimagio-image-metadata-support" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="napari-aicsimagio-image-metadata-support" class="callout-inner">
<h3 class="callout-title">Napari-aicsimagio image / metadata
support<a class="anchor" aria-label="anchor" href="#napari-aicsimagio-image-metadata-support"></a>
</h3>
<div class="callout-content">
<p>Only certain file types will support browsing metadata via the ‘OME
Tree Widget’ in napari-aicsimageio. The plugin is still under
development - so more formats are likely to be supported in future!</p>
<p>Napari-aicsimageio also has <a href="https://github.com/AllenCellModeling/napari-aicsimageio?tab=readme-ov-file#reading-mode-threshold" class="external-link">limits
on the size of images</a> it will load directly into memory. Only images
with a filesize less than 4GB and less than 30% of available memory will
be loaded directly. Otherwise, images are loaded as smaller chunks as
required.</p>
<p>If you have difficulty opening a specific file format with
napari-aicsimageio, it’s worth trying to open it in <a href="https://imagej.net/software/fiji/" class="external-link">Fiji</a> also. Fiji has very
well established integration with Bio-Formats, and so tends to support a
wider range of formats. Note that you can always save your image (or
part of your image) as another format like .tiff via Fiji to open in
Napari later (making sure you still retain a copy of the original file
and its metadata!)</p>
</div>
</div>
</div>
</section><section id="pixel-size"><h2 class="section-heading">Pixel size<a class="anchor" aria-label="anchor" href="#pixel-size"></a>
</h2>
<hr class="half-width">
<p>One of the most important pieces of metadata is the pixel size. In
our .czi image, this is stored under
<code>images &gt; Image:0 &gt; pixels</code> as
<code>physical_size</code> and <code>physical_size_unit</code> for each
axis (x, y and z). The pixel size states how large a pixel is in
physical units i.e. ‘real world’ units of measurement like micrometre,
or millimetre. In this case the unit given is ‘μm’ (micrometre). This
means that each pixel has a size of 0.20μm (x axis), 0.20μm (y axis) and
0.35μm (z axis). As this image is 3D, you will sometimes hear the pixels
referred to as ‘voxels’, which is just a term for a 3D pixel.</p>
<p>The pixel size is important to ensure that any measurements made on
the image are correct. For example, how long is a particular cell? Or
how wide is each nucleus? These answers can only be correct if the pixel
size is properly set. It’s also useful when we want to overlay different
images on top of each other (potentially acquired with different kinds
of microscope) - setting the pixel size appropriately will ensure their
overall size matches correctly in the Napari viewer.</p>
<p>How do we set the pixel size in Napari? Most of the time, if the
pixel size is provided in the image metadata, napari-aicsimageio will
set it automatically in a property called <code>scale</code>. We can
check this by running the following in Napari’s console:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># Get the first image layer</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>image_layer <span class="op">=</span> viewer.layers[<span class="dv">0</span>]</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co"># Print its scale</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="bu">print</span>(image_layer.scale)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code># [z y x]
[0.35 0.2047619 0.2047619]</code></pre>
</div>
<p>Each image layer in Napari has a <code>.scale</code> which is
equivalent to the pixel size. Here we can see that it is already set to
values matching the image metadata.</p>
<p>If the pixel size isn’t listed in the metadata, or napari-aicsimagio
doesn’t read it correctly, you can set it manually like so:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>image_layer.scale <span class="op">=</span> (<span class="fl">0.35</span>, <span class="fl">0.2047619</span>, <span class="fl">0.2047619</span>)</span></code></pre>
</div>
<div id="pixel-size-scale" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="pixel-size-scale" class="callout-inner">
<h3 class="callout-title">Pixel size / scale<a class="anchor" aria-label="anchor" href="#pixel-size-scale"></a>
</h3>
<div class="callout-content">
<p>Copy and paste the following into Napari’s console to get
<code>image_layer_1</code>, <code>image_layer_2</code> and
<code>image_layer_3</code> of the yeast image:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>image_layer_1 <span class="op">=</span> viewer.layers[<span class="dv">0</span>]</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>image_layer_2 <span class="op">=</span> viewer.layers[<span class="dv">1</span>]</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>image_layer_3 <span class="op">=</span> viewer.layers[<span class="dv">2</span>]</span></code></pre>
</div>
<ol style="list-style-type: decimal">
<li>Check the <code>.scale</code> of each layer - are they the
same?</li>
<li>Set the scale of layer 3 to (0.35, 0.4, 0.4) - what changes in the
viewer?</li>
<li>Set the scale of layer 3 to (0.35, 0.4, 0.2047619) - what changes in
the viewer?</li>
<li>Set the scale of all layers so they are half as wide and half as
tall as their original size in the viewer</li>
</ol>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="section level3">
<h3 id="section">1<a class="anchor" aria-label="anchor" href="#section"></a>
</h3>
<p>All layers have the same scale</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="bu">print</span>(image_layer_1.scale)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="bu">print</span>(image_layer_2.scale)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="bu">print</span>(image_layer_3.scale)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[0.35 0.2047619 0.2047619]
[0.35 0.2047619 0.2047619]
[0.35 0.2047619 0.2047619]</code></pre>
</div>
</div>
<div class="section level3">
<h3 id="section-1">2<a class="anchor" aria-label="anchor" href="#section-1"></a>
</h3>
<figure><img src="../fig/yeast-exercise-2.png" alt="Yeast image shown in Napari with layer 3  twice as big in y and x" class="figure mx-auto d-block"></figure><div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>image_layer_3.scale <span class="op">=</span> (<span class="fl">0.35</span>, <span class="fl">0.4</span>, <span class="fl">0.4</span>)</span></code></pre>
</div>
<p>You should see that layer 3 becomes about twice as wide and twice as
tall as the other layers. This is because we set the pixel size in y and
x (which used to be 0.2047619μm) to about twice its original value (now
0.4μm).</p>
</div>
<div class="section level3">
<h3 id="section-2">3<a class="anchor" aria-label="anchor" href="#section-2"></a>
</h3>
<figure><img src="../fig/yeast-exercise-3.png" alt="Yeast image shown in Napari with layer 3  twice as big in y" class="figure mx-auto d-block"></figure><div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>image_layer_3.scale <span class="op">=</span> (<span class="fl">0.35</span>, <span class="fl">0.4</span>, <span class="fl">0.2047619</span>)</span></code></pre>
</div>
<p>You should see that layer 3 appears squashed - with the same width as
other layers, but about twice the height. This is because we set the
pixel size in y (which used to be 0.2047619μm) to about twice its
original value (now 0.4μm). Bear in mind that setting the pixel sizes
inappropriately can lead to stretched or squashed images like this!</p>
</div>
<div class="section level3">
<h3 id="section-3">4<a class="anchor" aria-label="anchor" href="#section-3"></a>
</h3>
<figure><img src="../fig/yeast-exercise-4.png" alt="Yeast image shown in Napari with all layers  half size in y/x" class="figure mx-auto d-block"></figure><p>We set the pixel size in y/x to half its original value of
0.2047619μm:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a>image_layer_1.scale <span class="op">=</span> (<span class="fl">0.35</span>, <span class="fl">0.10238095</span>, <span class="fl">0.10238095</span>)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a>image_layer_2.scale <span class="op">=</span> (<span class="fl">0.35</span>, <span class="fl">0.10238095</span>, <span class="fl">0.10238095</span>)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>image_layer_3.scale <span class="op">=</span> (<span class="fl">0.35</span>, <span class="fl">0.10238095</span>, <span class="fl">0.10238095</span>)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
</section><section id="choosing-a-file-format"><h2 class="section-heading">Choosing a file format<a class="anchor" aria-label="anchor" href="#choosing-a-file-format"></a>
</h2>
<hr class="half-width">
<p>Now that we’ve seen an example of browsing metadata in Napari, let’s
look more closely into how we can decide on a file format. There are
many factors to consider, including:</p>
<div class="section level3">
<h3 id="dimension-support">Dimension support<a class="anchor" aria-label="anchor" href="#dimension-support"></a>
</h3>
<p>Some file formats will only support certain dimensions e.g. 2D, 3D, a
specific number of channels… For example, .png and .jpg only support 2D
images (either grayscale or RGB), while .tiff can support images with
many more dimensions (including any number of channels, time, 2D and 3D
etc.).</p>
</div>
<div class="section level3">
<h3 id="metadata-support">Metadata support<a class="anchor" aria-label="anchor" href="#metadata-support"></a>
</h3>
<p>As mentioned above, image metadata is a very important record of how
an image was acquired and what it represents. Different file formats
have different standards for how to store metadata, and what kind of
metadata they accept. This means that converting between file formats
often results in loss of some metadata.</p>
</div>
<div class="section level3">
<h3 id="compatibility-with-software">Compatibility with software<a class="anchor" aria-label="anchor" href="#compatibility-with-software"></a>
</h3>
<p>Some image analysis software will only support certain image file
formats. If a format isn’t supported, you will likely need to make a
copy of your data in a new format.</p>
</div>
<div class="section level3">
<h3 id="proprietary-vs-open-formats">Proprietary vs open formats<a class="anchor" aria-label="anchor" href="#proprietary-vs-open-formats"></a>
</h3>
<p>Many light microscopes will save data automatically into their own
proprietary file formats (owned by the microscope company). For example,
Zeiss microscopes often save files to a format with a .czi extension,
while Leica microscopes often use a format with a .lif extension. These
formats will retain all the metadata used during acquisition, but are
often difficult to open in software that wasn’t created by the same
company.</p>
<p><a href="https://www.openmicroscopy.org/bio-formats/" class="external-link">Bio-Formats</a>
is an open-source project that helps to solve this problem - allowing
over 100 file formats to be opened in many pieces of open source
software. Napari-aicsimageio (that we used earlier in this episode)
integrates with Bio-Formats to allow many different file formats to be
opened in Napari. Bio-Formats is really essential to allow us to work
with these multitude of formats! Even so, it won’t support absolutely
everything, so you will likely need to convert your data to another file
format sometimes. If so, it’s good practice to use an open file format
whose specification is freely available, and can be opened in many
different pieces of software e.g. OME-TIFF.</p>
</div>
<div class="section level3">
<h3 id="compression">Compression<a class="anchor" aria-label="anchor" href="#compression"></a>
</h3>
<p>Different file formats use different types of ‘compression’.
Compression is a way to reduce image file sizes, by changing the way
that the image pixel values are stored. There are many different
compression algorithms that compress files in different ways.</p>
<p>Many compression algorithms rely on finding areas of an image with
similar pixel values that can be stored in a more efficient way. For
example, imagine a row of 30 pixels from an 8-bit grayscale image:</p>
<figure><img src="../fig/image-line.png" alt="Diagram of a line of 30 pixels - 10 with pixel  value 50, then 10 with pixel value 100, then 10 with pixel value 150" class="figure mx-auto d-block"></figure><p>Normally, this would be stored as 30 individual pixel values, but we
can reduce this greatly by recognising that many of the pixel values are
the same. We could store the exact same data with only 6 values: 10 50
10 100 10 150, showing that there are 10 values with pixel value 50,
then 10 with value 100, then 10 with value 150. This is the general idea
behind ‘run-length encoding’ and many compression algorithms use similar
principles to reduce file sizes.</p>
<p>There are two main types of compression:</p>
<ul>
<li><p><strong>Lossless compression</strong> algorithms are reversible -
when the file is opened again, it can be reversed perfectly to give the
exact same pixel values.</p></li>
<li><p><strong>Lossy compression</strong> algorithms reduce the size by
irreversibly altering the data. When the file is opened again, the pixel
values will be different to their original values. Some image quality
may be lost via this process, but it can achieve much smaller file
sizes.</p></li>
</ul>
<p>For microscopy data, you should therefore use a file format with no
compression, or lossless compression. Lossy compression should be
avoided as it degrades the pixel values and may alter the results of any
analysis that you perform!</p>
<div id="compression-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="compression-1" class="callout-inner">
<h3 class="callout-title">Compression<a class="anchor" aria-label="anchor" href="#compression-1"></a>
</h3>
<div class="callout-content">
<p>Let’s remove all layers from the Napari viewer, and open the
‘00001_01.ome.tiff’ dataset. You should have already downloaded this to
your working directory as part of the <a href="index.html#setup">setup
instructions</a>.</p>
<p>Run the code below in Napari’s console. This will save a specific
timepoint of this image (time = 30) as four different file formats
(.tiff, .png + low and high quality .jpg). Note: these files will be
written to the same folder as the ‘00001_01.ome.tiff’ image!</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">from</span> skimage.io <span class="im">import</span> imsave</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="co"># Get the 00001_01.ome layer, and get timepoint = 30</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>layer <span class="op">=</span> viewer.layers[<span class="st">"00001_01.ome"</span>]</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>image <span class="op">=</span> layer.data[<span class="dv">30</span>, :, :]</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a><span class="co"># Save as different file formats in same folder as 00001_01.ome</span></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>folder_path <span class="op">=</span> Path(layer.source.path).parent</span>
<span id="cb11-10"><a href="#cb11-10" tabindex="-1"></a>imsave( folder_path <span class="op">/</span> <span class="st">"test-tiff.tiff"</span>, image)</span>
<span id="cb11-11"><a href="#cb11-11" tabindex="-1"></a>imsave( folder_path <span class="op">/</span> <span class="st">"test-png.png"</span>, image)</span>
<span id="cb11-12"><a href="#cb11-12" tabindex="-1"></a>imsave( folder_path <span class="op">/</span> <span class="st">"test-jpg-high-quality.jpg"</span>, image, quality<span class="op">=</span><span class="dv">75</span>)</span>
<span id="cb11-13"><a href="#cb11-13" tabindex="-1"></a>imsave( folder_path <span class="op">/</span> <span class="st">"test-jpg-low-quality.jpg"</span>, image, quality<span class="op">=</span><span class="dv">30</span>)</span></code></pre>
</div>
<ul>
<li><p>Go to the folder where ‘00001_01.ome.tiff’ was saved and look at
the file sizes of the newly written images (<code>test-tiff</code>,
<code>test-png</code>, <code>test-jpg-high-quality</code> and
<code>test-jpg-low-quality</code>). Which is biggest? Which is
smallest?</p></li>
<li><p>Open all four images in Napari. Zoom in very close to a bright
nucleus, and try showing / hiding different layers with the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility.svg" alt="A screenshot of Napari's eye button" height="30" class="figure"> icon. How
do they differ? How does each compare to timepoint 30 of the original
‘00001_01.ome’ image?</p></li>
<li><p>Which file formats use lossy compression?</p></li>
</ul>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<div class="section level3">
<h3 id="file-sizes">File sizes<a class="anchor" aria-label="anchor" href="#file-sizes"></a>
</h3>
<p>‘test_tiff’ is largest, followed by ‘test-png’, then
‘test-jpg-high-quality’, then ‘test-jpg-low-quality’.</p>
</div>
<div class="section level3">
<h3 id="differences-to-original">Differences to original<a class="anchor" aria-label="anchor" href="#differences-to-original"></a>
</h3>
<p>By showing/hiding different layers, you should see that ‘test-tiff’
and ‘test-png’ look identical to the original image (when its slider is
set to timepoint 30). In contrast, both jpg files show blocky artefacts
around the nuclei - the pixel values have clearly been altered. This
effect is worse in the low quality jpeg than the high quality one.</p>
</div>
<div class="section level3">
<h3 id="lossy-compression">Lossy compression<a class="anchor" aria-label="anchor" href="#lossy-compression"></a>
</h3>
<p>Both jpg files use lossy compression, as they alter the original
pixel values. The low quality jpeg compresses the file more than the
high quality jpeg, resulting in smaller file sizes, but also worse
alteration of the pixel values. The rest of the file formats (.tiff /
.png) use no compression, or lossless compression - so their pixel
values are identical to the original values.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="handling-of-large-image-data">Handling of large image data<a class="anchor" aria-label="anchor" href="#handling-of-large-image-data"></a>
</h3>
<p>If your images are very large, you may need to use a pyramidal file
format that is specialised for handling them. Pyramidal file formats
store images at multiple resolutions (and usually in small chunks) so
that they can be browsed smoothly without having to load all of the
full-resolution data. This is similar to how google maps allows browsing
of its vast quantities of map data. Specialised software like <a href="https://qupath.github.io/" class="external-link">QuPath</a>, <a href="https://imagej.net/plugins/bdv/" class="external-link">Fiji’s BigDataViewer</a> and <a href="https://www.openmicroscopy.org/omero/" class="external-link">OMERO’s viewer</a> can
provide smooth browsing of these kinds of images.</p>
<p>See below for an example image pyramid (using napari’s Cells (3D +
2Ch) sample image) with three different resolution levels stored. Each
level is about twice as small the last in x and y:</p>
<figure><img src="../fig/image-pyramid.png" style="width:40.0%" alt="Diagram of an image pyramid with three  resolution levels" class="figure mx-auto d-block"></figure>
</div>
</section><section id="common-file-formats"><h2 class="section-heading">Common file formats<a class="anchor" aria-label="anchor" href="#common-file-formats"></a>
</h2>
<hr class="half-width">
<p>As we’ve seen so far, there are many different factors to consider
when choosing file formats. As different file formats have various pros
and cons, it is very likely that you will use different formats for
different purposes. For example, having one file format for your raw
acquisition data, another for some intermediate analysis steps, and
another for making diagrams and figures for display. Pete Bankhead’s
bioimage book has a great chapter on <a href="https://bioimagebook.github.io/chapters/1-concepts/6-files/files.html" class="external-link">Files
&amp; file formats</a> that explores this in detail.</p>
<p>To finish this episode, let’s look at some common file formats you
are likely to encounter (this table is from <a href="https://bioimagebook.github.io/chapters/1-concepts/6-files/files.html#table-file-formats" class="external-link">Pete
Bankhead’s bioimage book</a> which is released under a CC-BY 4.0
license):</p>
<table class="table">
<colgroup>
<col width="14%">
<col width="17%">
<col width="17%">
<col width="17%">
<col width="34%">
</colgroup>
<thead><tr class="header">
<th align="left">Format</th>
<th align="left">Extensions</th>
<th align="left">Main use</th>
<th align="left">Compression</th>
<th align="left">Comment</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">TIFF</td>
<td align="left">.tif, .tiff</td>
<td align="left">Analysis, display (print)</td>
<td align="left">None, lossless, lossy</td>
<td align="left">Very general image format</td>
</tr>
<tr class="even">
<td align="left">OME-TIFF</td>
<td align="left">.ome.tif, .ome.tiff</td>
<td align="left">Analysis, display (print)</td>
<td align="left">None, lossless, lossy</td>
<td align="left">TIFF, with standardized metadata for microscopy</td>
</tr>
<tr class="odd">
<td align="left">Zarr</td>
<td align="left">.zarr</td>
<td align="left">Analysis</td>
<td align="left">None, lossless, lossy</td>
<td align="left">Emerging format, great for big datasets – but limited
support currently</td>
</tr>
<tr class="even">
<td align="left">PNG</td>
<td align="left">.png</td>
<td align="left">Display (web, print)</td>
<td align="left">Lossless</td>
<td align="left">Small(ish) file sizes without compression
artefacts</td>
</tr>
<tr class="odd">
<td align="left">JPEG</td>
<td align="left">.jpg, .jpeg</td>
<td align="left">Display (web)</td>
<td align="left">Lossy (usually)</td>
<td align="left">Small file sizes, but visible artefacts</td>
</tr>
</tbody>
</table>
<p>Note that there are many, many proprietary microscopy file formats in
addition to these! You can get a sense of how many by browsing
Bio-Formats list of <a href="https://bio-formats.readthedocs.io/en/v7.1.0/supported-formats.html" class="external-link">supported
formats</a>.</p>
<p>You’ll also notice that many file formats support different types of
compression e.g. none, lossless or lossy (as well as different
compression settings like the ‘quality’ we saw on jpeg images earlier).
You’ll have to make sure you are using the right kind of compression
when you save images into these formats.</p>
<p>To summarise some general recommendations:</p>
<ul>
<li><p>During acquisition, it’s usually a good idea to use whatever the
standard proprietary format is for that microscope. This will ensure you
retain as much metadata as possible, and have maximum compatibility with
that company’s acquisition and analysis software.</p></li>
<li><p>During analysis, sometimes you can directly use the format you
acquired your raw data in. If it’s not supported, or you need to save
new images of e.g.  sub-regions of an image, then it’s a good idea to
switch to one of the formats in the table above specialised for analysis
(TIFF and OME-TIFF are popular choices).</p></li>
<li><p>Finally, for display, you will need to use common file formats
that can be opened in any imaging software (not just scientific) like
png, jpeg or tiff. Note that jpeg usually uses lossy compression, so
it’s only advisable if you need very small file sizes (for example, for
displaying many images on a website).</p></li>
</ul>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Image files contain pixel values and metadata.</li>
<li>Metadata is an important record of how an image was acquired and
what it represents.</li>
<li>Napari-aicsimagio allows many more image file formats to be opened
in Napari, along with providing easy browsing of some metadata.</li>
<li>Pixel size states how large a pixel is in physical units
(e.g. micrometre).</li>
<li>Compression can be lossless or lossy - lossless is best for
microscopy images.</li>
<li>There are many, many different microscopy file formats. The best
format to use depends on your use-case e.g. acquisition, analysis or
display.</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-designing-a-light-microscopy-experiment"><p>Content from <a href="designing-a-light-microscopy-experiment.html">Designing a light microscopy experiment</a></p>
<hr>
<p> Last updated on 2024-04-26 | 
        
        <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/edit/main/episodes/designing-a-light-microscopy-experiment.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 55 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are the key factors to consider when designing a light
microscopy experiment?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li><p>Explain the main steps of designing a light microscopy
experiment</p></li>
<li><p>Describe a few examples of common microscopy methods
e.g. widefield, confocal</p></li>
<li><p>Explain factors to consider when choosing a microscopy
technique</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>In previous episodes we focused on the fundamentals of how images are
stored and displayed, with Napari as our main example. In the next few
episodes, we will instead focus on how to design an experiment using
light microscopy and how to analyse the resulting image data.</p>
<section id="steps-to-designing-a-light-microscopy-experiment"><h2 class="section-heading">Steps to designing a light microscopy experiment<a class="anchor" aria-label="anchor" href="#steps-to-designing-a-light-microscopy-experiment"></a>
</h2>
<hr class="half-width">
<p>As we’ve said throughout the course, there is no one ‘right’ way of
analysing image data. It depends on the specific images and research
question, with different solutions being best for different situations.
The same holds true for designing a light microscopy experiment - here
we’ll discuss some general guidelines, but the specifics will depend on
your particular research project.</p>
<p>For the sake of this episode, let’s imagine a scenario where we are
investigating the effects of a specific chemical on cells grown in
culture. What steps would we take to design a light microscopy
experiment for this?</p>
<p>A general workflow could be:</p>
<ol style="list-style-type: decimal">
<li><p>Define your research question</p></li>
<li><p>Define what you need to observe to answer that question</p></li>
<li><p>Define what you need to measure to answer that question</p></li>
<li><p>Choose a light microscopy method that fits your data
needs</p></li>
<li><p>Choose acquisition settings that fit your data needs</p></li>
</ol></section><section id="define-your-research-question"><h2 class="section-heading">Define your research question<a class="anchor" aria-label="anchor" href="#define-your-research-question"></a>
</h2>
<hr class="half-width">
<p>Clearly defining your research question is the essential starting
point. What do you want to find out from your microscopy experiment?</p>
<p>For our chemical example, the main aim could be: Does the chemical
affect the number, size or shape of cell nuclei over time?</p>
<p>Then we could form more specific research questions e.g.</p>
<ul>
<li><p>Does the addition of the chemical result in an increase in the
number of cell nuclei over ten hours?</p></li>
<li><p>Does the addition of the chemical result in an increase in the
average nucleus diameter over ten hours?</p></li>
<li><p>Does the addition of the chemical result in an increase in the
roundness of cell nuclei over ten hours?</p></li>
</ul>
<p>For the rest of the episode, we will stick to our very broad research
aim of: ‘Does the chemical affect the number, size or shape of cell
nuclei over time?’. This will allow us to have a broad discussion of
many different approaches to designing a light microscopy experiment.
Bear in mind that in a real experiment you should be as specific as
possible with your research question!</p>
</section><section id="define-what-you-need-to-observe"><h2 class="section-heading">Define what you need to observe<a class="anchor" aria-label="anchor" href="#define-what-you-need-to-observe"></a>
</h2>
<hr class="half-width">
<p>The next step is to figure out what you need to observe to answer
your research question. For example:</p>
<ul>
<li><p>What structures/events do you need to see?</p></li>
<li><p>Do you need fluorescent labels? If so, how many?</p></li>
<li><p>Over what spatial scale? E.g. large tissue sections vs small 3D
volumes</p></li>
<li><p>Over what timescale? E.g. do you need live cell imaging to follow
events over time, or is a snapshot of fixed cells enough?</p></li>
<li><p>2D or 3D?</p></li>
</ul>
<div id="what-do-you-need-to-observe" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="what-do-you-need-to-observe" class="callout-inner">
<h3 class="callout-title">What do you need to observe?<a class="anchor" aria-label="anchor" href="#what-do-you-need-to-observe"></a>
</h3>
<div class="callout-content">
<p>What would you need to observe to answer our research question? -
‘Does the chemical affect the number, size or shape of cell nuclei over
time?’</p>
<p>Think about the points above and make a list of potential answers. It
may be best to discuss in a group, so you can share different ideas.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<div class="section level3">
<h3 id="structures-events">Structures / events<a class="anchor" aria-label="anchor" href="#structures-events"></a>
</h3>
<p>We need to see individual nuclei, with enough resolution to see their
size and overall shape.</p>
</div>
<div class="section level3">
<h3 id="fluorescent-labels">Fluorescent labels<a class="anchor" aria-label="anchor" href="#fluorescent-labels"></a>
</h3>
<p>We will probably need a fluorescent label for the nuclei. This will
allow them to be easily identified in our images so the number, size and
shape can be measured.</p>
<p>Note that it is possible to see and measure nuclei from un-labelled
cells also (e.g. from phase contrast or DIC images, as we will cover in
the <a href="#widefield">widefield section of this episode</a>). The
downside is that it’s much harder to automatically recognise and measure
nuclei from these kinds of images. Unlike images of fluorescently
labelled nuclei, there is a much less clear separation between nuclei
and the rest of the cell, making analysis more challenging.</p>
</div>
<div class="section level3">
<h3 id="spatial-scale">Spatial scale<a class="anchor" aria-label="anchor" href="#spatial-scale"></a>
</h3>
<p>We need to observe many nuclei at once, to get a good estimate of
their number and average shape and size. Therefore, our spatial scale
should encompass an area large enough to image many nuclei for each
timepoint (say 100 nuclei).</p>
</div>
<div class="section level3">
<h3 id="timescale">Timescale<a class="anchor" aria-label="anchor" href="#timescale"></a>
</h3>
<p>As we are interested in how the nuclei number, shape and size changes
over time, we will need to use live cell imaging. We will need to image
for long enough to see multiple cell divisions, so we can assess how the
number of nuclei increases. The length of time required will depend on
the cell line you are using, as they each take a different length of
time to divide.</p>
</div>
<div class="section level3">
<h3 id="d-or-3d">2D or 3D<a class="anchor" aria-label="anchor" href="#d-or-3d"></a>
</h3>
<p>This will depend on how we want to measure nucleus size and shape (as
we’ll look at in the next section of this episode). For example, if we
want to accurately characterise the volume and 3D shape of the nuclei
then we will need to image in 3D. If we’re instead happy to estimate
from 2D measures like nucleus diameter, then 2D is sufficient. This
again highlights the need for a specific research question! In a real
life experiment, we should probably define more clearly exactly what
aspects of nucleus size and shape we are interested in, as this will
help to inform the need for 2D or 3D.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="define-what-you-need-to-measure"><h2 class="section-heading">Define what you need to measure<a class="anchor" aria-label="anchor" href="#define-what-you-need-to-measure"></a>
</h2>
<hr class="half-width">
<p>Once we have a good idea of what we need to observe, we need to
clearly define what measurements are required.</p>
<ul>
<li><p>What do you need to quantify from your images to answer your
question? E.g. cell length, nucleus volume, the number of cells with a
specific fluorescent label…</p></li>
<li><p>How will you quantify them? E.g. which image processing software?
What kind of analysis methods?</p></li>
<li><p>How many objects/events do you need to quantify?</p></li>
</ul>
<p>As the second point indicates, it’s always worthwhile to consider how
you will quantify your images before you collect them. This will make
the image analysis steps much faster and ensure that you are collecting
images optimised for the given method. We’ll look at image processing
methods more in later episodes.</p>
<p>For the last point, it’s important to consider any statistical tests
you will use to answer your research question, and the sample size
required to give them sufficient statistical ‘power’. Statistical power
is the likelihood of a statistical test detecting an effect when one is
actually present. For example, consider testing if there is a difference
of X% in the number of cells when grown with and without our chemical of
interest. If there really is a difference, a statistical power of 0.8
(80%) would mean that out of 100 different studies, 80 of them would
detect it. Having a high statistical power is vital to ensure that our
experiment has a good chance of detecting the effects we are interested
in. There is a clear relationship between sample size and power, with
larger sample sizes resulting in higher power to detect the same effect.
Conducting a ‘power analysis’ is a good way to assess a reasonable
minimum sample size for your experiment. A full discussion of this is
outside of the scope of this course, but there are links to further
resources in the <a href="additional-resources.html">final
episode</a>.</p>
<div id="what-do-you-need-to-measure" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="what-do-you-need-to-measure" class="callout-inner">
<h3 class="callout-title">What do you need to measure?<a class="anchor" aria-label="anchor" href="#what-do-you-need-to-measure"></a>
</h3>
<div class="callout-content">
<p>Consider the two points below and make a list of potential answers
for our research question ‘Does the chemical affect the number, size or
shape of cell nuclei over time?’:</p>
<ul>
<li>What do you need to quantify from your images to answer your
question?</li>
<li>How will you quantify them?</li>
</ul>
<p>It may be best to discuss in a group, so you can share different
ideas.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="section level3">
<h3 id="what-to-quantify">What to quantify?<a class="anchor" aria-label="anchor" href="#what-to-quantify"></a>
</h3>
<p>For nucleus number, we will need to count the number of nuclei
present at each timepoint.</p>
<p>For nucleus size, there are many different options. If we’re imaging
in 2D, we could measure the nucleus area, or the width at its widest
point. For 3D, we could measure nucleus volume, or again the width at
its widest point…</p>
<p>For nucleus shape, there are even more options. In 2D we could, for
example, measure the nucleus ‘roundness’ (a measure of how circular it
is). In 3D, we could measure nucleus ‘sphericity’ (a measure of how
spherical it is). Which measures you use will often depend on which
image analysis software you use. Many analysis packages have a wide
range of shape (morphological) features built-in e.g. Napari has a
plugin called ‘napari-skimage-regionprops’ that offers <a href="https://www.napari-hub.org/plugins/napari-skimage-regionprops#features" class="external-link">many
different features</a>.</p>
</div>
<div class="section level3">
<h3 id="how-to-quantify">How to quantify?<a class="anchor" aria-label="anchor" href="#how-to-quantify"></a>
</h3>
<p>Here again there is no one correct answer - a lot will depend on
which image analysis software you use and your personal preference. For
example, let’s say you decided on imaging in 3D and measuring nucleus
number, volume and sphericity with Napari. Before we can make any
measurements from the cells, we first need to ‘segment’ the nuclei
i.e. identify which pixels in the image correspond to each nucleus
(we’ll look at this in detail in later episodes). For some tasks, this
could be as simple as drawing a contour or boundary around the cell.
However, as we are looking to quantify many nuclei at many different
timepoints, it’s not feasible to do this manually - we’ll need to
develop an automated workflow to segment the nuclei and measure their
volume and sphericity. We’ll look at some techniques for this kind of
analysis in the <a href="quality-control-and-manual-segmentation.html">manual
segmentation</a>, <a href="filters-and-thresholding.html">thresholding</a> and <a href="instance-segmentation-and-measurements.html">instance
segmentation</a> episodes.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="choose-a-light-microscopy-method"><h2 class="section-heading">Choose a light microscopy method<a class="anchor" aria-label="anchor" href="#choose-a-light-microscopy-method"></a>
</h2>
<hr class="half-width">
<p>For the final part of this episode, let’s look at how we can choose a
light microscopy method for our experiment. We’ll look at the last step
(choosing acquisition settings) in the <a href="choosing-acquisition-settings.html">next episode</a>.</p>
<p>Often the first step in choosing a light microscopy technique is
discussing with other members of your research group or department. What
are most of them using, for what kind of experiments? If you are working
with a microscopy core facility, then the people who work there are also
a great source of advice and recommendations. Finally, you can also
explore the literature for research questions and experiments similar to
yours to see which methods are used most often.</p>
<p>There’s a very wide variety of light microscopes available (from many
different manufacturers) that are specialised for different kinds of
samples. We’ll take a brief look at some popular options below, but
there are far too many to cover in a single episode. Bear this in mind
when choosing light microscopy techniques for your own experiments -
there are many more options to consider!</p>
<div class="section level3">
<h3 id="widefield">Widefield<a class="anchor" aria-label="anchor" href="#widefield"></a>
</h3>
<p>In a widefield microscope, the entire sample is illuminated by the
light source at once. The light source can illuminate the sample from
below (in an upright microscope) or from above (in an inverted
microscope).</p>
<p>In it’s simplest form, a widefield microscope can be used for
‘brightfield microscopy’. This is simply where the sample is illuminated
by a bright light from one side, and then imaged from the other. The
issue with this method is that it often produces low-contrast images,
where it’s difficult to see biological structures. This is because
biological structures are often quite transparent - they don’t absorb
much light or differ much in their density. For this reason, contrast
agents/staining are often used to increase contrast (i.e.  the addition
of dyes/chemicals that bind to specific structures).</p>
<figure><img src="../fig/skin.png" style="width:70.0%" alt="Screenshot of Napari's Skin sample image" class="figure mx-auto d-block"><div class="figcaption">The image above is Napari’s Skin (RGB) sample
image - it is a brightfield image of a hematoxylin and eosin stained
slide of dermis and epidermis.</div>
</figure><p>To increase contrast (especially for unstained samples), widefield
microscopes often support <a href="https://www.microscopyu.com/techniques/phase-contrast/introduction-to-phase-contrast-microscopy" class="external-link">‘phase-contrast’</a>
or <a href="https://www.leica-microsystems.com/science-lab/microscopy-basics/differential-interference-contrast-dic/" class="external-link">‘DIC
- Differential Interference Contrast’</a>. Both these methods make use
of slight changes in the ‘phase’ of light as it passes through a sample
to increase contrast.</p>
<figure><img src="../fig/phase-contrast.jpg" style="width:70.0%" alt="Phase gradient contrast image of SH-SY5Y cells" class="figure mx-auto d-block"><div class="figcaption">The image above is a phase gradient contrast
image of some SH-SY5Y cells (ZEISS Microscopy, <a href="https://creativecommons.org/licenses/by/2.0" class="external-link">CC BY 2.0</a>, via <a href="https://commons.wikimedia.org/wiki/File:SH-SY5Y_cells,_transmitted_light_phase_gradient_contrast_microscopy_with_ZEISS_Celldiscoverer_7_(30614936722).jpg" class="external-link">Wikimedia
Commons</a> )</div>
</figure><figure><img src="../fig/dic-example.jpg" style="width:50.0%" alt="DIC image of some yeast cells - Saccharomyces cerevisiae" class="figure mx-auto d-block"><div class="figcaption">The image above is a DIC image of some yeast
cells (<em>Saccharomyces cerevisiae</em>) from <a href="https://commons.wikimedia.org/wiki/File:S_cerevisiae_under_DIC_microscopy.jpg" class="external-link">Wikimedia
Commons</a>
</div>
</figure><p>Widefield microscopes can also be used for fluorescence microscopy.
In fluorescence microscopy, fluorescent labels are used that target
specific features (like the nucleus or cell membrane). These labels are
excited by a specific wavelength of light (the excitation wavelength),
and then emit light of a longer wavelength (the emission wavelength). By
illuminating the sample with light of the excitation wavelength, then
detecting light of the emission wavelength, a fluorescence microscope
can image the biological structures that the label is bound to.</p>
<p>It’s worth noting that optimising a fluorescence microscopy setup can
be quite complex! For example, depending on the structure you want to
image, it can be difficult to acquire labels that bind specifically and
don’t interfere with normal function and localisation. This requires
multiple initial tests to verify where a label binds, as well as the
appropriate conditions to use (like incubation time / temperature). In
addition, if you want to image multiple fluorescent labels at the same
time, then you have to ensure there is minimal overlap between their
excitation/emission wavelengths. Labels must also be chosen to match the
available lasers/filters on your microscope of choice - otherwise you
will be unable to properly excite and collect emitted light from them.
These considerations are true for all types of fluorescence microscope -
including widefield and also confocal (that will be discussed
below).</p>
<figure><img src="../fig/fluorescence-example.jpg" style="width:70.0%" alt="Fluorescence microscopy image of some LLC-PK1 cells" class="figure mx-auto d-block"><div class="figcaption">The image above is a fluorescence microscopy
image of some LLC-PK1 cells (ZEISS Microscopy, <a href="https://creativecommons.org/licenses/by/2.0" class="external-link">CC BY 2.0</a>, via <a href="https://commons.wikimedia.org/wiki/File:Mitotic_LLC-PK1_cells,_fluorescence_microscopy_(23700644352).jpg" class="external-link">Wikimedia
Commons</a> )</div>
</figure><p>The main issue with widefield microscopes is that, due to the whole
sample being illuminated, it can produce rather blurry images
(especially for thicker samples). For example, in widefield fluorescence
microscopy, fluorophores throughout the entire sample will be emitting
light. This means that the resulting image will represent light from the
focal plane, and also from out-of-focus planes above and below it. This
can result in details being obscured by out-of-focus light.</p>
</div>
<div class="section level3">
<h3 id="confocal">Confocal<a class="anchor" aria-label="anchor" href="#confocal"></a>
</h3>
<p>Confocal microscopes are capable of ‘optical sectioning’ meaning they
can detect light only from the plane of focus and exclude light from
out-of-focus planes. This is extremely useful, especially for thick
samples, to achieve clear images where details aren’t obscured by
out-of-focus light.</p>
<p>There are many different types of confocal microscopes, but one of
the most common is laser scanning confocal microscopes. These are also
known as point scanning confocal microscopes. In this microscope, a
laser is focused onto a single point of the specimen at a time (rather
than illuminating the entire sample). The point of illumination is then
moved across the sample in a raster pattern (i.e. line by line), with
light detected point by point. This gradually builds up the final
image.</p>
<p>Confocal microscopes are typically used for fluorescence microscopy,
especially when it is necessary to image the shape of structures in full
3D. However, this form of imaging also comes with some disadvantages.
For example, confocal microscopes will generally be more complex to use
and slower to acquire images than a widefield microscope. Although there
are other types of confocal that are faster than laser scanning systems,
such as ‘spinning disc confocal microscopes’. Spinning disc systems
split the laser into hundreds of focused beams using an array of
carefully arranged pinholes on a round disc. Spinning the disc causes
the beams to rapidly scan across the sample and acquire an image, much
faster than using a single beam as a standard laser-scanning confocal
does. For more information, <a href="https://www.nature.com/articles/s41596-020-0313-9" class="external-link">Jonkman et
al.’s review</a> gives a great summary of different confocal methods -
e.g. see figure 2 for a comparison of laser-scanning and spinning disc
confocals.</p>
<figure><img src="../fig/confocal-example.png" style="width:60.0%" alt="Screenshot of Napari's Kidney (3D + 3Ch) sample image" class="figure mx-auto d-block"><div class="figcaption">The image above is Napari’s Kidney (3D + 3Ch)
sample image. This was acquired with confocal fluorescence
microscopy.</div>
</figure>
</div>
<div class="section level3">
<h3 id="super-resolution">Super-resolution<a class="anchor" aria-label="anchor" href="#super-resolution"></a>
</h3>
<p>Before the invention of ‘super-resolution’ methods, it was thought
that light microscopes had a maximum resolution of around 200nm due to
the <a href="https://www.microscopyu.com/techniques/super-resolution/the-diffraction-barrier-in-optical-microscopy" class="external-link">‘diffraction
limit’</a> of light. Super-resolution methods allow microscopes to
surpass this limit, achieving resolutions down to tens of nanometres.
This allows many biological structures, previously only visible with
techniques like electron microscopy, to be viewed with light
microscopes. Many of these super-resolution systems are based on
modified widefield or confocal microscope setups. The increase in
resolution they provide usually comes at the cost of increased
complexity in experiment/microscope setup. There are far too many types
of super-resolution to cover in this episode, but <a href="https://www.nature.com/articles/s41556-018-0251-8_" class="external-link">Schermelleh et
al.</a> and <a href="https://royalsocietypublishing.org/doi/10.1098/rsta.2021.0110" class="external-link">Prakash
et al</a> provide useful reviews if you are interested.</p>
</div>
<div class="section level3">
<h3 id="choosing-a-method">Choosing a method<a class="anchor" aria-label="anchor" href="#choosing-a-method"></a>
</h3>
<p>Choosing a light microscopy method is about finding the simplest and
fastest approach that can address your research question. For example,
does it provide the resolution you need? Can it provide good images of
thick vs thin samples? Does it image a large enough area? Does it image
fast enough to provide the temporal resolution you need?… The online <a href="https://www.bioimagingguide.org/welcome.html" class="external-link">bioimaging guide</a>
provides a <a href="https://www.bioimagingguide.org/02_Sample_acquisition/Picking.html" class="external-link">useful
flowchart</a> to give you some ideas - as well as links to many other
great microscopy resources. We’ve also placed a small summary table
below of the microscopy techniques we covered in this episode.</p>
<p>If your research question can be solved by a simpler and less costly
method, then usually this is the way to go! It’s all about choosing the
right tool for the job - different approaches will be best for different
research questions. For example, you could count cell number with a
high-end super-resolution microscope, but this is also possible with a
standard widefield which will be simpler and faster. In general, only
move to more complex techniques when your research question really
requires it. This also holds true for later steps like choosing image
processing and statistical analysis methods.</p>
<table class="table">
<colgroup>
<col width="27%">
<col width="36%">
<col width="36%">
</colgroup>
<thead><tr class="header">
<th align="left">Technique</th>
<th align="left">What is it?</th>
<th align="left">Key points</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">Brightfield</td>
<td align="left">Illuminates the sample with light from one side, and
images on the other</td>
<td align="left">Hard to see many biological structures - usually
requires contrast agents/staining to increase contrast</td>
</tr>
<tr class="even">
<td align="left">Phase contrast / DIC</td>
<td align="left">Makes use of slight changes in the ‘phase’ of light as
it passes through a sample to increase contrast</td>
<td align="left">Allows unstained samples to be seen more easily</td>
</tr>
<tr class="odd">
<td align="left">Widefield flourescence</td>
<td align="left">Fluorescent labels (with specific excitation and
emission wavelengths) bind to specific biological structures</td>
<td align="left">Widefield illuminates the whole sample at once, which
can lead to blurry images in thicker samples</td>
</tr>
<tr class="even">
<td align="left">Laser scanning confocal</td>
<td align="left">A laser is scanned across the sample point by point in
a raster pattern</td>
<td align="left">Allows ‘optical sectioning’, giving clearer images in
full 3D. More complex to use than widefield, also slower to acquire
images.</td>
</tr>
<tr class="odd">
<td align="left">Spinning disc confocal</td>
<td align="left">An array of pinholes on a disc split the laser into
hundreds of beams that move across the sample</td>
<td align="left">Faster than standard laser scanning confocal</td>
</tr>
<tr class="even">
<td align="left">Super-resolution</td>
<td align="left">Wide range of methods that break the classic
‘diffraction limit’ of light, allowing resolutions down to tens of
nanometres</td>
<td align="left">More complex to use than standard widefield /
confocal</td>
</tr>
</tbody>
</table>
<div id="which-microscope" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="which-microscope" class="callout-inner">
<h3 class="callout-title">Which microscope?<a class="anchor" aria-label="anchor" href="#which-microscope"></a>
</h3>
<div class="callout-content">
<p>Which light microscopes could be used to answer our research question
- ‘Does the chemical affect the number, size or shape of cell nuclei
over time?’</p>
<p>Think about the points above and make a list of potential answers. It
may be best to discuss in a group, so you can share different ideas.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>Again, there’s no one solution here - it depends on which elements of
nucleus shape and size we’re focusing on. If we’re happy with 2D
measures, then widefield fluorescence microscopy would be a great option
to allow large numbers of nuclei to be imaged over time.</p>
<p>If we’re more interested in the precise 3D volume and shape, then
confocal fluorescence microscopy would be a better fit. We’d have to be
careful with how often we wanted to image the cells vs the speed of
acquisition though.</p>
<p>As we are using live cell imaging, we have to carefully consider how
often we need to take an image to answer our research question. For
example, if we want to track the rapid movement and divisions of
individual nuclei, then we will need to image quickly, with a small time
interval between each image. Alternatively, if we only need to measure
overall changes in mean nucleus number/size/shape (without exactly
tracking every nucleus) then we can allow much longer time intervals
between each image. For rapid imaging, it may be necessary to use a
microscope specialised for high speed (such as a spinning disc
confocal), otherwise slower methods (such as a standard laser scanning
confocal) can also work very well.</p>
<p>Super-resolution options wouldn’t be required here. Cell nuclei can
be easily visualised with standard widefield and confocal approaches, as
they have a large diameter of around 5-20 micrometre (depending on cell
type). This is well above the classic ‘diffraction limit’ of around
200nm that we discussed in the <a href="#super-resolution">super-resolution section</a>. There’s no need
to introduce any extra complexity, as our research question doesn’t
require that level of resolution.</p>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>
<p>A general workflow for designing a light microscopy experiment
should include:</p>
<ul>
<li>Define your research question</li>
<li>Define what you need to observe to answer that question</li>
<li>Define what you need to measure to answer that question</li>
<li>Choose a light microscopy method that fits your data needs</li>
<li>Choose acquisition settings that fit your data needs</li>
</ul>
</li>
<li><p>There are many different types of light microscope - including
widefield and confocal</p></li>
<li><p>You should choose the simplest methods (in acquisition,
processing, and statistical analysis) that allow you to address your
research question</p></li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</section></section><section id="aio-choosing-acquisition-settings"><p>Content from <a href="choosing-acquisition-settings.html">Choosing acquisition settings</a></p>
<hr>
<p> Last updated on 2024-04-26 | 
        
        <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/edit/main/episodes/choosing-acquisition-settings.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 55 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What are the key factors to consider when choosing acquisition
settings?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li><p>Explain some examples of acquisition settings, and factors to
consider when choosing them</p></li>
<li><p>Explain the difference between resolution and pixel size</p></li>
<li><p>Describe some quality control steps e.g. monitoring the image
histogram</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>In the last episode we looked at some of the main steps to designing
a light microscopy experiment:</p>
<ol style="list-style-type: decimal">
<li><p>Define your research question</p></li>
<li><p>Define what you need to observe to answer that question</p></li>
<li><p>Define what you need to measure to answer that question</p></li>
<li><p>Choose a light microscopy method that fits your data
needs</p></li>
<li><p>Choose acquisition settings that fit your data needs</p></li>
</ol>
<p>In this episode, we’ll focus on the last step - choosing acquisition
settings - as well as looking at some early quality control steps.</p>
<section id="acquisition-settings"><h2 class="section-heading">Acquisition settings<a class="anchor" aria-label="anchor" href="#acquisition-settings"></a>
</h2>
<hr class="half-width">
<p>Once you’ve chosen a light microscope to use, there are a wide
variety of acquisition settings that can be adjusted. Acquisition
settings commonly optimised for an experiment include magnification,
laser power, and exposure time, but there are many others. Different
combinations of settings will be best for different samples and research
questions. For example, one combination of settings may be best for
rapid live cell imaging, while another may be best for high resolution
imaging of fixed samples.</p>
<p>There are many different factors that are affected by acquisition
settings, but here are some of the main ones to consider.</p>
</section><section id="spatial-resolution"><h2 class="section-heading">Spatial resolution<a class="anchor" aria-label="anchor" href="#spatial-resolution"></a>
</h2>
<hr class="half-width">
<p>Spatial resolution is defined as the smallest distance between two
adjacent points on a sample that can still be seen as separate entities.
In 2D, this is measured in x and y, while in 3D it is measured in x, y
and z. For example, a resolution of one micrometre would mean that
objects less than one micrometre apart couldn’t be identified separately
(i.e. they would appear as one object).</p>
<p>Spatial resolution is affected by many factors including the
wavelength of light and the <a href="https://www.microscopyu.com/microscopy-basics/numerical-aperture" class="external-link">numerical
aperture (NA)</a> of the objective lens. Using a shorter wavelength of
light and higher NA lenses, can provide higher resolution images.</p>
<p>Spatial resolution can be isotropic (the same in all directions) or
anisotropic (different in different directions). Anisotropic resolution
is common with 3D datasets, where the x/y resolution tends to be better
than the z resolution.</p>
<p>We’ll look at spatial resolution in more detail <a href="#resolution-vs-pixel-size">later in this episode</a>.</p>
<div id="spatial-resolution-and-optical-resolution" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="spatial-resolution-and-optical-resolution" class="callout-inner">
<h3 class="callout-title">Spatial resolution and optical resolution<a class="anchor" aria-label="anchor" href="#spatial-resolution-and-optical-resolution"></a>
</h3>
<div class="callout-content">
<p>In this episode, we use ‘spatial resolution’ to refer to the
resolution limits of the light microscope. Note that sometimes ‘spatial
resolution’ is also used to refer to the final image pixel size (as we
will look at <a href="#resolution-vs-pixel-size">later in this
episode</a>). Due to this ambiguity, you may also see the microscope’s
resolution referred to as ‘optical resolution’ to clearly distinguish it
from pixel size.</p>
</div>
</div>
</div>
</section><section id="temporal-resolution"><h2 class="section-heading">Temporal resolution<a class="anchor" aria-label="anchor" href="#temporal-resolution"></a>
</h2>
<hr class="half-width">
<p>Temporal (or time) resolution, is how fast images can be acquired
(usually measured in seconds or milliseconds). This is mainly controlled
by the ‘exposure time’, which is the length of time over which the
detector collects light for each image. For a laser scanning confocal
microscope, you will instead see a ‘dwell time’ - this is the time the
laser spends illuminating each position on the sample. Decreasing
exposure time / dwell time will result in faster overall imaging
speeds.</p>
<p>The imaging speed will also depend on some of the mechanical
properties of your chosen light microscope. For example, the speed with
which a laser-scanning confocal can move the laser across a sample, or
(if imaging multiple locations) the speed of the microscope stage
movement.</p>
<p>To produce good quality images (with a high signal-to-noise ratio, <a href="#signal-to-noise-ratio">as we’ll look at later</a>), the detector
needs to collect as much light as possible in these short
timescales.</p>
</section><section id="field-of-view"><h2 class="section-heading">Field of view<a class="anchor" aria-label="anchor" href="#field-of-view"></a>
</h2>
<hr class="half-width">
<p>The field of view is the size of the area you can view with your
light microscope. It is often measured as a diameter in millimetres. A
larger field of view is useful if you want to, for example, observe a
large number of cells at the same time. This can quickly increase your
sample size, providing improved statistical power (as covered in the <a href="designing-a-light-microscopy-experiment.html#define-what-you-need-to-measure">last
episode</a>).</p>
<p>The field of view is affected by many factors but, importantly, is
directly related to the magnification of the objective lens. Higher
levels of magnification result in a smaller field of view.</p>
<p>To image areas larger than the field of view, you will need to
acquire multiple images at different positions by moving the
microscope’s stage. These images can then be stitched together (often
with the microscope manufacturer’s acquisition software), to give one
large final image. This method can provide high resolution images of
large areas, but also slows down overall acquisition times.</p>
</section><section id="depth-penetration"><h2 class="section-heading">Depth penetration<a class="anchor" aria-label="anchor" href="#depth-penetration"></a>
</h2>
<hr class="half-width">
<p>Depth penetration refers to the distance light can penetrate into
your sample (in the z direction). It therefore controls the maximum z
range that you can image over - for example, a specific confocal
microscope may be limited to a maximum depth of 50 micrometre.</p>
<p>Depth penetration is limited due to samples absorbing and scattering
light that passes through them. Various factors control the penetration
depth - for example, the density of the sample and the wavelength of
light used during imaging. Various <a href="https://www.nature.com/articles/s43586-021-00080-9" class="external-link">tissue
clearing</a> methods exist that can help to make a sample more
transparent, and allow imaging at greater depths. Also, the use of
longer wavelengths of light can reduce absorption/scattering and
increase depth penetration. This comes at the cost of decreased spatial
resolution (as we covered in the <a href="#spatial-resolution">spatial
resolution section</a>). For very thick samples, you will likely need to
use a microscope specialised for this task e.g. a <a href="https://www.microscopyu.com/techniques/multi-photon/multiphoton-microscopy" class="external-link">multiphoton
microscope</a>.</p>
</section><section id="data-size"><h2 class="section-heading">Data size<a class="anchor" aria-label="anchor" href="#data-size"></a>
</h2>
<hr class="half-width">
<p>It’s important to consider the overall size of your imaging data
(e.g. in megabytes (MB), gigabytes (GB) or terabytes (TB)). Light
microscopy datasets can be extremely large in size, so you will have to
make sure you have appropriate file storage space that is regularly
backed up.</p>
<p>The overall size of your data will depend on many factors. For
example, increasing your resolution will increase the size of your final
images, especially if you are covering a large area by stitching
multiple images taken at different positions together.</p>
</section><section id="light-exposure-i.e-phototoxicity-photobleaching"><h2 class="section-heading">Light exposure (i.e phototoxicity / photobleaching)<a class="anchor" aria-label="anchor" href="#light-exposure-i.e-phototoxicity-photobleaching"></a>
</h2>
<hr class="half-width">
<p>Illuminating our samples with light is an essential part of
collecting light microscopy images. However, we must also be mindful of
the detrimental effects light can have on our samples, especially at
high intensity over long time periods.</p>
<p>Photobleaching is a key issue for fluorescence microscopy. If a
fluorophore is exposed to intense light, its structure can degrade over
time. This means that it will eventually stop fluorescing entirely.</p>
<p>Another issue is phototoxicity. Light exposure can damage cells
resulting in unexpected changes in cell behaviour, morphology and
eventually cell death.</p>
<p>Reducing the effects of photobleaching and phototoxicity requires
minimising the light exposure of our samples as much as possible. Light
exposure can be reduced through many methods, including reducing
light/laser power, reducing exposure time/dwell time, and imaging at
lower temporal resolution.</p>
</section><section id="signal-to-noise-ratio"><h2 class="section-heading">Signal to noise ratio<a class="anchor" aria-label="anchor" href="#signal-to-noise-ratio"></a>
</h2>
<hr class="half-width">
<p>Signal to noise ratio is a very useful measure of image quality - in
general, it will be easier to identify and measure features of interest
in images with a higher signal to noise ratio.</p>
<p>The ‘signal’ is what we really want to measure - for example, for a
fluorescence microscopy image, this would be the light emitted by
fluorophores in the sample. In an ideal world the detector would
perfectly measure the light intensity, but this isn’t really possible.
The values the detector records will always be affected by random
fluctuations called ‘noise’ (see the <a href="#where-does-noise-come-from">‘where does noise come from?’</a>
section for more information). This noise usually appears as a random
‘graininess’ over the image and can make details difficult to see.</p>
<figure><img src="../fig/image-with-noise.png" alt="Left - the nuclei from Napari's Cells  (3D+2Ch) sample image. Right - same image with added gaussian noise" class="figure mx-auto d-block"></figure><p>To identify and measure our features of interest, the absolute size
of the signal and noise are less important than the ratio between them.
For example, imagine we are trying to distinguish between two areas with
different brightness - one being 1.5x brighter than the other. In a low
signal setup where values are small (say 20 vs 30) adding noise can make
these areas very difficult to separate. For example, see the diagram
below which shows three histograms. The left is the ideal scenario,
where there is no noise, and we perfectly see one value of 20 and one of
30. The middle shows the addition of some noise to each value, resulting
in wider distributions that overlap extensively. This means that in the
combined histogram of these values (right) it becomes impossible to
clearly distinguish these two values. Note - here the added noise is a
gaussian distribution centred on each value (i.e. 20 and 30) with a
standard deviation of 10 and 1000 samples - you can read more about
gaussian noise in the <a href="https://bioimagebook.github.io/chapters/3-fluorescence/3-formation_noise/formation_noise.html" class="external-link">noise
chapter</a> of Pete Bankhead’s bioimage book.</p>
<figure><img src="../fig/snr-comparison-low.png" alt="Diagram of a low signal-to-noise scenario.  Left - histogram with no noise. Middle - histogram with added noise (separate  histograms). Right - histogram with added noise (combined histogram)." class="figure mx-auto d-block"></figure><p>If we imagine a scenario with higher signal, now values of 80 and 120
(one still 1.5x higher than the other), then we can see that adding the
same level of noise has less effect. As the values are now higher and
further apart, the broadening of their distributions only causes them to
overlap slightly. We can still clearly distinguish the values in the
combined histogram on the right. This demonstrates how increasing the
signal to noise ratio can improve our ability to recognise and measure
different features.</p>
<figure><img src="../fig/snr-comparison-high.png" alt="Diagram of a high signal-to-noise scenario.  Left - histogram with no noise. Middle - histogram with added noise (separate  histograms). Right - histogram with added noise (combined histogram)." class="figure mx-auto d-block"></figure><p>How can we improve the signal to noise ratio of our images? The main
solution is to increase the amount of light that we detect per image, by
adjusting our acquisition settings. For example:</p>
<ul>
<li>increase light/laser power</li>
<li>increase exposure time / dwell time</li>
<li>Use <a href="https://bioimagebook.github.io/chapters/3-fluorescence/4-microscope_types/microscope_types.html#sec-detectors-binning" class="external-link">binning</a>
(this combines multiple pixels into a single larger pixel)</li>
<li>Average multiple images</li>
</ul>
<p>If you want to find out more about noise, we recommend the <a href="https://bioimagebook.github.io/chapters/3-fluorescence/3-formation_noise/formation_noise.html" class="external-link">noise
chapter</a> of Pete Bankhead’s bioimage book.</p>
<div id="where-does-noise-come-from" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="where-does-noise-come-from" class="callout-inner">
<h3 class="callout-title">Where does noise come from?<a class="anchor" aria-label="anchor" href="#where-does-noise-come-from"></a>
</h3>
<div class="callout-content">
<p>Noise comes from many sources - for example, ‘photon noise’ (also
known as ‘shot noise’) is a key source. Photon noise is caused by
inherent statistical fluctuations in the emission of photons (individual
light ‘packets’). For example, consider a fluorescence microscopy
experiment where photons of light are being emitted from a fluorescent
label. Even though the average rate of emission is constant, the exact
number of photons emitted in a specific time interval (say 30 seconds)
will vary each time. Say 20 photons, then 18, then 22… The exact time a
photon is emitted is random, so there will always be random variations.
These follow a statistical distribution known as a ‘poisson
distribution’ - if you’re interested in the details Pete Bankhead’s
bioimage book has a great chapter on <a href="https://bioimagebook.github.io/chapters/3-fluorescence/3-formation_noise/formation_noise.html" class="external-link">noise</a>.</p>
<p>The important point to consider is that there will always be some
photon noise. It is inherit noise in the emission of light, and
therefore not dependent on a particular microscope or detector
setup.</p>
<p>Other types of noise are dependent on the microscope itself, often
associated with the kind of detector used. For example, ‘read noise’ and
‘dark noise’ are commonly associated with <a href="https://www.microscopyu.com/tutorials/ccd-signal-to-noise-ratio" class="external-link">CCD
(charge-coupled device) cameras</a>. These types of noise relate to
slight inaccuracies in quantifying the number of photons that hit the
detector - no detector is perfect, so there will always be some
variation.</p>
</div>
</div>
</div>
</section><section id="optimising-acquisition-settings"><h2 class="section-heading">Optimising acquisition settings<a class="anchor" aria-label="anchor" href="#optimising-acquisition-settings"></a>
</h2>
<hr class="half-width">
<p>In an ideal world we could produce the ‘perfect’ image that optimised
for all of the features above: high spatial resolution, high temporal
resolution, large field of view, large depth penetration, small data
size, low light exposure and high signal to noise ratio. This is never
possible though! There are always trade-offs and compromises to be made.
These different factors are inter-related, usually meaning that
optimising for one will be at the expense of the others. For example,
increasing spatial resolution requires a reduced field of view, and it
will increase the overall data size. As each pixel covers a smaller
area, resulting in less signal per pixel, you will likely need to
increase light exposure to achieve a comparable signal to noise ratio.
This may mean using higher light intensity, or imaging more slowly (with
increased dwell time/exposure time).</p>
<figure><img src="../fig/acquisition-tradeoffs.png" style="width:80.0%" alt="Diagram highlighting some of the  trade-offs of increasing spatial resolution" class="figure mx-auto d-block"></figure><p>Different microscopes will provide different ranges for these
features - for example, supporting different ranges of spatial or
temporal resolution, or providing detectors with different sensitivity
that require higher or lower light intensity. Acquisition settings then
allow you to fine-tune how that microscope functions, optimising within
these ranges for your particular sample and research question. For
example, do you need to prioritise temporal resolution for a highly
dynamic process? Or perhaps is spatial resolution the priority, as your
region of interest is very small?</p>
<p><a href="https://www.nature.com/articles/s41596-020-0313-9" class="external-link">Jonkman
et al.’s 2020 paper</a> provides some great advice for choosing confocal
microscopes and acquisition settings, with figure 5 as a great example
of the different trade-offs.</p>
<div id="choosing-acquisiton-settings" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="choosing-acquisiton-settings" class="callout-inner">
<h3 class="callout-title">Choosing acquisiton settings<a class="anchor" aria-label="anchor" href="#choosing-acquisiton-settings"></a>
</h3>
<div class="callout-content">
<p>In the previous episode, we used an example of investigating the
effects of a specific chemical on cells grown in culture. Continuing
with this, which acquisition settings would we need to prioritise to
answer our research question? - ‘Does the chemical affect the number,
size or shape of cell nuclei over time?’</p>
<p>Think about the points above and the trade-offs between them to make
a list of potential answers. It may be best to discuss in a group, so
you can share different ideas.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>There’s no one correct answer here, as it will depend on which type
of light microscope you are using and which aspects of nucleus size and
shape you want to focus on. Here are some points to consider though:</p>
<div class="section level3">
<h3 id="light-exposure">Light exposure<a class="anchor" aria-label="anchor" href="#light-exposure"></a>
</h3>
<p>As we are interested in how the nuclei number, shape and size changes
over time, we will need to use live cell imaging. Light exposure
(photobleaching / phototoxicity) can be a huge issue for these long
experiments where cells must be exposed to light again and again.
Therefore, to image for as long as possible, we should try to reduce
light exposure as much as possible. This may require using lower
resolution, lower light/laser power and shorter exposure times. We must
balance this against signal to noise ratio though.</p>
</div>
<div class="section level3">
<h3 id="data-size-and-spatial-resolution">Data size and spatial resolution<a class="anchor" aria-label="anchor" href="#data-size-and-spatial-resolution"></a>
</h3>
<p>As we are taking many images over a long timescale, our data size
could become very large. To minimise this, we should make sure we are
using the lowest spatial resolution possible (that still lets us see
individual nucleus size/shape). Usually this means selecting a lower NA
objective lens (which is usually also a lower magnification objective
lens).</p>
<p>Also, we should ensure we are using an appropriate interval between
our images. For example, do you need an image every minute? Or is every
ten minutes fine? Longer intervals will reduce the size of your final
dataset.</p>
</div>
<div class="section level3">
<h3 id="temporal-resolution-1">Temporal resolution<a class="anchor" aria-label="anchor" href="#temporal-resolution-1"></a>
</h3>
<p>If we are only interested in measuring overall changes in the mean
nucleus number / size / shape, we can afford to have quite large
intervals between our images (low temporal resolution). If we instead
wanted to track the progress of individual nuclei over time (e.g. their
movements and individual divisions), we would need much smaller
intervals (higher temporal resolution).</p>
</div>
<div class="section level3">
<h3 id="field-of-view-1">Field of view<a class="anchor" aria-label="anchor" href="#field-of-view-1"></a>
</h3>
<p>We will need to keep the field of view large enough to image many
nuclei at once - this will give us a good estimate of the average nuclei
number / shape / size. This means we should use the lowest magnification
objective lens that still provides enough spatial resolution to see
individual nuclei. Alternatively, we will need to stitch images together
from multiple positions - although this will slow down acquisition, and
must be balanced against the required temporal resolution!</p>
</div>
<div class="section level3">
<h3 id="depth-penetration-1">Depth penetration<a class="anchor" aria-label="anchor" href="#depth-penetration-1"></a>
</h3>
<p>As we are considering cells grown in culture (rather than in thick
tissues), depth penetration isn’t a key concern here. This means we
shouldn’t have to, for example, limit ourselves to fluorophores with
longer emission wavelengths, or require any additional preparation steps
like tissue clearing.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="resolution-vs-pixel-size"><h2 class="section-heading">Resolution vs pixel size<a class="anchor" aria-label="anchor" href="#resolution-vs-pixel-size"></a>
</h2>
<hr class="half-width">
<p>Let’s dive deeper into choosing an appropriate spatial resolution for
your experiment. First, let’s clarify the differences between
magnification, pixel size and spatial resolution.</p>
<div class="section level3">
<h3 id="spatial-resolution-1">Spatial resolution<a class="anchor" aria-label="anchor" href="#spatial-resolution-1"></a>
</h3>
<p>As <a href="#spatial-resolution">defined above</a>, spatial
resolution is the smallest distance between two points on a sample that
can still be seen as separate entities. It is a measure of the
microscope’s ability to resolve small details in an image.</p>
</div>
<div class="section level3">
<h3 id="magnification">Magnification<a class="anchor" aria-label="anchor" href="#magnification"></a>
</h3>
<p>Magnification describes how much larger an object appears through the
microscope vs its actual size e.g. 10x larger, 100x larger…</p>
<p><span class="math display">\[
\large \text{Magnification} =
\frac{\text{Size of object in microscopy image}}{\text{True size of
object}}
\]</span></p>
<p>Note that this is not the same as spatial resolution! Magnification
just describes how large an object appears, not the size of the details
it can resolve. For example, a particular light microscope may have a
maximum spatial resolution of one micrometre - meaning that it won’t be
able to discern features less than this distance apart. It may be
possible to magnify images far beyond this limit though, making them
appear larger and larger without allowing us to see any more detail.
This is known as ‘empty magnification’ and should be avoided.</p>
</div>
<div class="section level3">
<h3 id="pixel-size">Pixel size<a class="anchor" aria-label="anchor" href="#pixel-size"></a>
</h3>
<p>We discussed pixel size in the <a href="filetypes-and-metadata.html#pixel-size">filetypes and metadata
episode</a>, but let’s recap here. The pixel size states how large a
single image pixel is in physical units i.e. ‘real world’ units of
measurement like micrometre, or millimetre.</p>
<p>Note again that this is not the same as the spatial resolution! The
pixel size helps us understand how large a region each pixel of our
image covers, but (as with magnification) it doesn’t tell us the size of
details the microscope is really capable of resolving. For example, I
could process a microscopy image to artificially give it 4x as many
pixels as the original. This would reduce the pixel size without
allowing us to see any smaller details i.e. there’s no corresponding
improvement in spatial resolution!</p>
<div id="pixel-size-vs-resolution" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="pixel-size-vs-resolution" class="callout-inner">
<h3 class="callout-title">Pixel size vs resolution<a class="anchor" aria-label="anchor" href="#pixel-size-vs-resolution"></a>
</h3>
<div class="callout-content">
<p>Consider the 16x16 pixel image below of a circle. If the pixel size
in x/y is 1.5 micrometre:</p>
<ul>
<li>How wide is the circle (in micrometre)?</li>
<li>How wide is the entire image (in micrometre)?</li>
<li>If the circle is displayed as 9cm wide, then what is the
magnification?</li>
</ul>
<figure><img src="../fig/pixel-size-magnification-exercise-1.png" style="width:70.0%" alt="A 16x16 image of a  grayscale circle" class="figure mx-auto d-block"></figure><p>Below is a downsampled version of the same image (now 8x8
pixels).</p>
<ul>
<li>What is the new pixel size (in micrometre)?</li>
<li>How wide is the circle (in micrometre)?</li>
<li>How wide is the entire image (in micrometre)?</li>
<li>If the circle is displayed as 9cm wide, then what is the
magnification?</li>
</ul>
<figure><img src="../fig/pixel-size-magnification-exercise-2.png" style="width:70.0%" alt="An 8x8 image of a  grayscale circle" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="section level3">
<h3 id="x16-image">16x16 image<a class="anchor" aria-label="anchor" href="#x16-image"></a>
</h3>
<p>The circle is 12 pixels wide, which means it is (12 x 1.5) = 18
micrometre wide</p>
<p>The entire image is 16 pixels wide, which means it is (16 x 1.5) = 24
micrometre wide</p>
<p>9cm is equivalent to 90,000 micrometre. The magnification is
therefore 90,000 (the displayed size) divided by 18 micrometre (the
actual size) = 5000x</p>
</div>
<div class="section level3">
<h3 id="x8-image">8x8 image<a class="anchor" aria-label="anchor" href="#x8-image"></a>
</h3>
<p>The image still covers the same total area, but has been downsampled
2x. This means the pixel size will be doubled to 3 micrometre.</p>
<p>As this is an image of the exact same circle as the 16x16 image, the
circle width is unchanged. In this image it is 6 pixels wide, which
means it is (6 x 3) = 18 micrometre wide (the same as above).</p>
<p>Again, the image still covers the same total area, so its total width
will be unchanged too. Here, it’s 8 pixels wide, which means it is (8 x
3) = 24 micrometre wide.</p>
<p>The magnification is also unchanged in this case. Magnification
depends on the size of an object in the displayed image vs its actual
size. If it is still displayed at 9cm wide, then the result will be the
same, 5000x.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="choosing-magnification-spatial-resolution-and-pixel-size">Choosing magnification, spatial resolution and pixel size<a class="anchor" aria-label="anchor" href="#choosing-magnification-spatial-resolution-and-pixel-size"></a>
</h3>
<p>Now that we understand the difference between magnification,
resolution and pixel size - how do we go about choosing their values for
our experiments?</p>
<p>The most important thing is to only use the level of resolution that
you really require to answer your research question. For example, say
you were researching the width of different skin layers using tissue
sections on slides. You could image this at extremely high resolution,
to resolve individual cells and nuclei, but this wouldn’t really be
needed to answer the research question. The width of the larger layers
could be seen at much lower resolution, allowing you to image a larger
area faster, while also keeping your data size as small as possible.
Don’t increase your resolution without a good reason!</p>
<p><a href="#spatial-resolution">As discussed above</a>, a number of
factors affect the spatial resolution - but one of the most important is
your choice of objective lens. Each microscope will usually have a
selection of objective lenses available (each with a different
magnification and numerical aperture (NA)). In general, higher NA lenses
(which provide a higher spatial resolution) will also provide higher
levels of magnification. Choose the lowest NA/magnification objective
that still provides enough spatial resolution to resolve the smallest
structure you want to be able to see and measure. Table 1 of <a href="https://www.nature.com/articles/s41596-020-0313-9" class="external-link">Jonkman et
al.’s 2020 paper</a> provides a nice summary of some common objective
lenses.</p>
<p>The pixel size of your final image should be matched appropriately to
your chosen objective lens (and therefore the spatial
resolution/magnification used). This is usually set to satisfy ‘Nyquist
sampling’ which states that the pixel size should be two to three times
smaller than the smallest feature you want to capture in your images.
Let’s look at a quick example of why this is necessary.</p>
<p>Consider the diagram below - on the left is shown two round cells
(blue) that have a diameter of 10 micrometre and are overlaid by a pixel
grid with the same spacing. On the right is shown the equivalent final
image, using a rough grayscale colourmap (black = 0, white =
maximum).</p>
<figure><img src="../fig/nyquist-1.png" style="width:80.0%" alt="Left - a diagram of two round cells (blue, 10  micrometre wide) overlaid by a perfectly aligned 10 micrometre pixel grid.  Right - the equivalent image with a grayscale colormap" class="figure mx-auto d-block"></figure><p>When the cells lie at the centre of each pixel, we can easily detect
them as separate objects in our final image. The issue comes when the
cells are offset from the pixel grid - which will happen very regularly
in a real life scenario! Keeping the same 10 micrometre spacing, but
offsetting the grid slightly results in a final image where where we can
no longer separate the two cells:</p>
<figure><img src="../fig/nyquist-2.png" style="width:80.0%" alt="Left - a diagram of two round cells (blue, 10  micrometre wide) overlaid by a misaligned 10 micrometre pixel grid.  Right - the equivalent image with a grayscale colormap" class="figure mx-auto d-block"></figure><p>Only by decreasing the pixel size (e.g. to 5 micrometre) can we be
sure to capture the two cells, no matter their alignment with the
grid:</p>
<figure><img src="../fig/nyquist-3.png" style="width:80.0%" alt="Left - a diagram of two round cells (blue, 10  micrometre wide) overlaid by a 5 micrometre pixel grid.  Right - the equivalent image with a grayscale colormap" class="figure mx-auto d-block"></figure><p>Failing to meet the Nyquist critera can also result in various image
artifacts known as ‘aliasing’. For example, consider the digram below -
here we have 6 small cells (blue) that are evenly spaced in the x
direction. Using a pixel spacing of 10 micrometre, this produces an odd
effect in our final image, where we see a repeating pattern that is much
wider than the real spacing of our cells.</p>
<figure><img src="../fig/aliasing.png" style="width:80.0%" alt="Top - a diagram of six round cells (blue, 5  micrometre wide) overlaid by a 10 micrometre pixel grid.  Bottom - the equivalent image with a grayscale colormap" class="figure mx-auto d-block"></figure><p>This is an example of aliasing where a high frequency detail in the
image (the repeating pattern of small cells) is aliased to give a false
lower frequency pattern. Aliasing and Nyquist sampling are important
considerations for all fields where an analog signal is converted to a
digital one - for example, digital audio and video, as well as
images.</p>
<p>Most acquisition software will automatically provide sensible
defaults that meet the Nyquist criteria (taking into account the
configuration of your microscope and other acquisition settings). For
example, for a laser scanning confocal, this will adjust the z-step used
and the x/y spacing of points sampled with the laser. The main point is
to avoid:</p>
<ul>
<li><p>Undersampling: Our pixel size is too large to accurately preserve
the spatial resolution in the resulting digital image (i.e. we haven’t
achieved Nyquist sampling)</p></li>
<li><p>Oversampling: Our pixel size is too small, exceeding that
required for Nyquist sampling. This results in larger images, with far
more pixels, without providing any extra useful spatial
information.</p></li>
</ul>
</div>
</section><section id="initial-quality-control"><h2 class="section-heading">Initial quality control<a class="anchor" aria-label="anchor" href="#initial-quality-control"></a>
</h2>
<hr class="half-width">
<p>While acquiring your images, it’s good to keep an eye on the image
histogram to ensure your images are good quality. Most light
microscope’s acquisition software will allow you to view a histogram in
real time while imaging your sample. Recall from the <a href="image-display.html">image display episode</a> that histograms
provide a quick summary of pixel values in an image. This summary is a
useful guide to help us adjust acquisition settings like exposure time /
dwell time and laser power.</p>
<p>In an ideal scenario, we want our histogram to show pixel values over
most of the possible intensity range. For example, for an 8-bit image,
spread over most of the range from 0 (minimum) to 255 (maximum). This
will ensure we are capturing as much information as possible about our
sample, and giving ourselves the best chance of distinguishing features
that only differ slightly in their brightness. The exercise below covers
more features to look for in your image histograms, with more details in
the exercise solution.</p>
<div id="image-histogram-quality-control" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="image-histogram-quality-control" class="callout-inner">
<h3 class="callout-title">Image histogram quality control<a class="anchor" aria-label="anchor" href="#image-histogram-quality-control"></a>
</h3>
<div class="callout-content">
<p>Look at the example 8-bit image histograms below. For each:</p>
<ul>
<li><p>Does it represent a good quality image, with an appropriate range
of pixel values?</p></li>
<li><p>If not, how might you adjust your acquisition settings to improve
it?</p></li>
</ul>
<figure><img src="../fig/exercise-qc-histograms.png" alt="Diagram of four example acquisition  image histograms (labelled a-d)" class="figure mx-auto d-block"></figure>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<div class="section level3">
<h3 id="a">a<a class="anchor" aria-label="anchor" href="#a"></a>
</h3>
<p>This image is ‘underexposed’ meaning its pixel values are clustered
at low intensity values, without using much of the intensity range. As
it uses such a small range of pixel values, it will be difficult to
distinguish features of similar brightness.</p>
<p>It could be improved by increasing the amount of light collected -
e.g.  increasing the exposure time / dwell time or increasing
light/laser power.</p>
</div>
<div class="section level3">
<h3 id="b">b<a class="anchor" aria-label="anchor" href="#b"></a>
</h3>
<p>This image is exposed well - it has pixel values spread throughout
most of the range from 0-255.</p>
</div>
<div class="section level3">
<h3 id="c">c<a class="anchor" aria-label="anchor" href="#c"></a>
</h3>
<p>This image is ‘overexposed’ meaning its pixel values are clustered at
high intensity values. It also shows ‘clipping’ (also known as
‘saturation’), as shown by the very tall peak at the right hand side of
the histogram. Clipping means that some pixels are recording light above
the maximum limit for the image (in this case 255). As no values beyond
this limit can be recorded, they are all ‘clipped’ to the maximum value,
resulting in the abnormally high peak you can see here. Clipping means
that information is being irretrievably lost and should be avoided!</p>
<p>It could be improved by reducing the amount of light collected -
e.g. reducing exposure time / dwell time or decreasing light/laser
power.</p>
</div>
<div class="section level3">
<h3 id="d">d<a class="anchor" aria-label="anchor" href="#d"></a>
</h3>
<p>This image is exposed well - it has pixel values spread throughout
most of the range from 0-255. Note that you will often have image
histograms with multiple peaks e.g. a low intensity one for the
background and a high intensity one for labelled cells.</p>
</div>
</div>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li><p>Choosing acquisition settings is a trade-off between many factors
including: spatial resolution, temporal resolution, field of view, data
size, light exposure and signal to noise ratio.</p></li>
<li><p>Signal to noise ratio is a useful measure of image
quality.</p></li>
<li><p>Magnification describes how much larger an object appears through
the microscope vs its actual size. This is not the same as the spatial
resolution!</p></li>
<li><p>Pixel size states how large a single image pixel is in physical
units e.g.  micrometre. Again, this is not the same as spatial
resolution.</p></li>
<li><p>Image histograms are a useful quality control measure during
acquisition. In general, we want our histogram to show pixel values
spread over most of the possible intensity range.</p></li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-quality-control-and-manual-segmentation"><p>Content from <a href="quality-control-and-manual-segmentation.html">Quality control and manual segmentation</a></p>
<hr>
<p> Last updated on 2024-04-26 | 
        
        <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/edit/main/episodes/quality-control-and-manual-segmentation.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 55 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is segmentation?</li>
<li>How do we manually segment images in Napari?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li><p>Explain some common quality control steps e.g. assessing an
image’s histogram, checking clipping/dynamic range</p></li>
<li><p>Explain what a segmentation, mask and label image are</p></li>
<li><p>Create a label layer in Napari and use some of its manual
segmentation tools (e.g. paintbrush/fill bucket)</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>For the next few episodes, we will work through an example of
counting the number of cells in an image.</p>
<p>First, let’s open one of Napari’s sample images with:<br><code>File &gt; Open Sample &gt; napari builtins &gt; Cells (3D + 2Ch)</code></p>
<figure><img src="../fig/cells-napari.png" alt="A screenshot of a flourescence microscopy image  of some cells in Napari" class="figure mx-auto d-block"></figure><section id="quality-control"><h2 class="section-heading">Quality control<a class="anchor" aria-label="anchor" href="#quality-control"></a>
</h2>
<hr class="half-width">
<p>The first step in any image processing workflow is always quality
control. We have to check that the images we acquired at the microscope
capture the features we are interested in, with a reasonable signal to
noise ratio (recall we discussed signal to noise ratio in the <a href="choosing-acquisition-settings.html#signal-to-noise-ratio">last
episode</a>).</p>
<p>In order to count our cells, we will need to see individual nuclei in
our images. As there is one nucleus per cell, we will use the nucleus
number as the equivalent of cell number. If we look at our cells image
in Napari, we can clearly see multiple cell nuclei in green. If we zoom
in we can see some noise (as a slight ‘graininess’ over the image), but
this doesn’t interfere with being able to see the locations or sizes of
all nuclei.</p>
<p>Next, it’s good practice to check the image histogram, as we covered
in the <a href="choosing-acquisition-settings.html#initial-quality-control">choosing
acquisition settings episode</a>. We’ll do this in the exercise
below:</p>
<div id="histogram-quality-control" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="histogram-quality-control" class="callout-inner">
<h3 class="callout-title">Histogram quality control<a class="anchor" aria-label="anchor" href="#histogram-quality-control"></a>
</h3>
<div class="callout-content">
<p>Plot the image histogram with <code>napari matplotlib</code>:</p>
<ul>
<li>Do you see any issues with the histogram?</li>
</ul>
<p>If you need a refresher on how to use <code>napari matplotlib</code>,
check out the <a href="image-display.html#napari-plugins">image display
episode</a>. It may also be useful to zoom into parts of the image
histogram by clicking the <img src="https://raw.githubusercontent.com/matplotlib/napari-matplotlib/main/src/napari_matplotlib/icons/black/Zoom.png" alt="A screenshot of napari-matplotlib's zoom button" height="30" class="figure">
icon at the top of histogram, then clicking and dragging a box around
the region you want to zoom into. You can reset your histogram by
clicking the <img src="https://raw.githubusercontent.com/matplotlib/napari-matplotlib/main/src/napari_matplotlib/icons/black/Home.png" alt="A screenshot of napari-matplotlib's home button" height="30" class="figure">
icon.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>In the top menu bar of Napari select:<br><code>Plugins &gt; napari Matplotlib &gt; Histogram</code></p>
<p>Make sure you have the ‘nuclei’ layer selected in the layer list
(should be highlighted in blue).</p>
<figure><img src="../fig/nuclei-histogram.png" alt="A histogram of the 29th z slice of Napari's  cell sample image" class="figure mx-auto d-block"></figure><div id="z-slices-and-contrast-limits" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="z-slices-and-contrast-limits" class="callout-inner">
<h3 class="callout-title">Z slices and contrast limits<a class="anchor" aria-label="anchor" href="#z-slices-and-contrast-limits"></a>
</h3>
<div class="callout-content">
<p>Note that as this image is 3D <code>napari matplotlib</code> only
shows the histogram of the current z slice (in this case z=29 as shown
at the top of the histogram). Moving the slider at the bottom of the
viewer, will update the histogram to show different slices.</p>
<p>The vertical white lines at the left and right of the histogram
display the current contrast limits as set in the layer controls. Note
that by default Napari isn’t using the full image range as the contrast
limits. We can change this by right clicking on the contrast limits
slider and selecting the ‘reset’ button on the right hand side. This
will set the contrast limits to the min/max pixel value of the current z
slice. If we instead want to use the full 16-bit range from 0-65535, we
can instead click the ‘full range’ button, then drag the contrast limit
nodes to each end of the slider.</p>
</div>
</div>
</div>
<p>First, are there pixel values spread over most of the possible range?
We can check what range is possible by printing the image’s data type to
the console:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>nuclei <span class="op">=</span> viewer.layers[<span class="st">"nuclei"</span>].data</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="bu">print</span>(nuclei.dtype)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="va">uint16</span></span></code></pre>
</div>
<p>This shows the image is an unsigned integer 16-bit image, which has a
pixel value range from 0-65535 (as we covered in the <a href="what-is-an-image.html#bit-depth">‘What is an image?’ episode</a>).
This matches the range on the x axis of our napari-matplotlib
histogram.</p>
<p>If we look at the brightest part of the image, near z=29, we can see
that there are indeed pixel values over much of this possible range. At
first glance, it may seem like there are no values at the right side of
the histogram, but if we zoom in using the <img src="https://raw.githubusercontent.com/matplotlib/napari-matplotlib/main/src/napari_matplotlib/icons/black/Zoom.png" alt="A screenshot of napari-matplotlib's zoom button" height="30" class="figure">
icon we can clearly see pixels at these higher values.</p>
<figure><img src="../fig/nuclei-histogram-zoom.png" alt="A histogram of the 29th z slice of  Napari's cell sample image - zoomed in to the range from 25000 to 60000" class="figure mx-auto d-block"></figure><p>Most importantly, we see no evidence for ‘clipping’, which means we
are avoiding any irretrievable loss of information this would cause.
Recall that clipping occurs when pixels are recording light above the
maximum limit for the image. Therefore, many values are ‘clipped’ to the
maximum value resulting in information loss.</p>
</div>
</div>
</div>
</div>
</section><section id="what-is-segmentation"><h2 class="section-heading">What is segmentation?<a class="anchor" aria-label="anchor" href="#what-is-segmentation"></a>
</h2>
<hr class="half-width">
<p>In order to count the number of cells, we must ‘segment’ the nuclei
in this image. Segmentation is the process of labelling each pixel in an
image e.g. is that pixel part of a nucleus or not? Segmentation comes in
two main types - ‘semantic segmentation’ and ‘instance segmentation’. In
this section we’ll describe what both kinds of segmentation represent,
as well as how they relate to each other, using some simple
examples.</p>
<p>First, let’s take a quick look at a rough semantic segmentation. Open
Napari’s console by pressing the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/console.svg" alt="A screenshot of Napari's console button" height="30" class="figure"> button,
then copy and paste the code below. Don’t worry about the details of
what’s happening in the code - we’ll look at some of these concepts like
gaussian blur and otsu thresholding in later episodes!</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">from</span> skimage.filters <span class="im">import</span> threshold_otsu, gaussian</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>image <span class="op">=</span> viewer.layers[<span class="st">"nuclei"</span>].data</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>blurred <span class="op">=</span> gaussian(image, sigma<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>threshold <span class="op">=</span> threshold_otsu(blurred)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a>semantic_seg <span class="op">=</span> blurred <span class="op">&gt;</span> threshold</span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a>viewer.add_labels(semantic_seg)</span></code></pre>
</div>
<figure><img src="../fig/semantic-seg-napari.png" alt="A screenshot of a rough semantic  segmentation of nuclei in Napari" class="figure mx-auto d-block"></figure><p>You should see an image appear that highlights the nuclei in brown.
Try toggling the ‘semantic_seg’ layer on and off multiple times, by
clicking the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility.svg" alt="A screenshot of Napari's eye button" height="30" class="figure"> icon next
to its name in the layer list. You should see that the brown areas match
the nucleus boundaries reasonably well.</p>
<p>This is an example of a ‘semantic segmentation’. In a semantic
segmentation, pixels are grouped into different categories (also known
as ‘classes’). In this example, we have assigned each pixel to one of
two classes: <code>nuclei</code> or <code>background</code>.
Importantly, it doesn’t recognise which pixels belong to different
objects of the same category - for example, here we don’t know which
pixels belong to individual, separate nuclei. This is the role of
‘instance segmentation’ that we’ll look at next.</p>
<p>Copy and paste the code below into Napari’s console. Again, don’t
worry about the details of what’s happening here - we’ll look at some of
these concepts in later episodes.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="im">from</span> skimage.morphology <span class="im">import</span> binary_erosion, ball</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="im">from</span> skimage.segmentation <span class="im">import</span> expand_labels</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="im">from</span> skimage.measure <span class="im">import</span> label</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>eroded <span class="op">=</span> binary_erosion(semantic_seg, footprint<span class="op">=</span>ball(<span class="dv">10</span>))</span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>instance_seg <span class="op">=</span> label(eroded)</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>instance_seg <span class="op">=</span> expand_labels(instance_seg, distance<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>viewer.add_labels(instance_seg)</span></code></pre>
</div>
<figure><img src="../fig/instance-seg-napari.png" alt="A screenshot of a rough instance  segmentation of nuclei in Napari" class="figure mx-auto d-block"></figure><p>You should see an image appear that highlights nuclei in different
colours. Let’s hide the ‘semantic_seg’ layer by clicking the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility.svg" alt="A screenshot of Napari's eye button" height="30" class="figure"> icon next
to its name in Napari’s layer list. Then try toggling the ‘instance_seg’
layer on and off multiple times, by clicking the corresponding <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility.svg" alt="A screenshot of Napari's eye button" height="30" class="figure"> icon. You
should see that the coloured areas match most of the nucleus boundaries
reasonably well, although there are some areas that are less well
labelled.</p>
<p>This image is an example of an ‘instance segmentation’ - it is
recognising which pixels belong to individual ‘instances’ of our
category (nuclei). This is the kind of segmentation we will need in
order to count the number of nuclei (and therefore the number of cells)
in our image.</p>
<p>Note that it’s common for instance segmentations to be created by
first making a semantic segmentation, then splitting it into individual
instances. This isn’t always the case though - it will depend on the
type of segmentation method you use.</p>
</section><section id="how-are-segmentations-represented"><h2 class="section-heading">How are segmentations represented?<a class="anchor" aria-label="anchor" href="#how-are-segmentations-represented"></a>
</h2>
<hr class="half-width">
<p>How are segmentations represented in the computer? What pixel values
do they use?</p>
<p>First, let’s focus on the ‘semantic_seg’ layer we created
earlier:</p>
<ul>
<li>Click the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility.svg" alt="A screenshot of Napari's eye button" height="30" class="figure"> icon next
to ‘semantic_seg’ in the layer list to make it visible.</li>
<li>Click the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility.svg" alt="A screenshot of Napari's eye button" height="30" class="figure"> icon next
to ‘instance_seg’ in the layer list to hide it.</li>
<li>Make sure the ‘semantic_seg’ layer is selected in the layer list. It
should be highlighted in blue.</li>
</ul>
<p>Try hovering over the segmentation and examining the pixel values
down in the bottom left corner of the viewer. Recall that we looked at
pixel values in the <a href="what-is-an-image.html#pixels">‘What is an
image?’ episode</a>.</p>
<figure><img src="../fig/pixel-value-segmentation.png" alt="A screenshot highlighting the pixel  value of a nuclei segmentation in Napari" class="figure mx-auto d-block"></figure><p>You should see that a pixel value of 0 is used for the background and
a pixel value of 1 is used for the nuclei. In fact, segmentations are
stored in the computer in the same way as a standard image - as an array
of pixel values. We can check this by printing the array into Napari’s
console, as we did in the <a href="what-is-an-image.html#images-are-arrays-of-numbers">‘What is an
image?’ episode</a>.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Get the semantic segmentation data</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>image <span class="op">=</span> viewer.layers[<span class="st">'semantic_seg'</span>].data</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a><span class="co"># Print the image values and type</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="bu">print</span>(image)</span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">type</span>(image))</span></code></pre>
</div>
<p>Note the output is shortened here to save space!</p>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[[[0 0 0 ... 0 0 0]
  [0 0 0 ... 0 0 0]
  [0 0 0 ... 0 0 0]
  ...

&lt;class 'numpy.ndarray'&gt;
</code></pre>
</div>
<p>The difference between this and a standard image, is in what the
pixel values represent. Here, they no longer represent the intensity of
light at each pixel, but rather the assignment of each pixel to
different categories or objects e.g.  0 = background, 1 = nuclei. You
can also have many more values, depending on the categories/objects you
are trying to represent. For example, we could have nucleus, cytoplasm,
cell membrane and background, which would give four possible values of
0, 1, 2 and 3.</p>
<p>Let’s take a look at the pixel values in the instance
segmentation:</p>
<ul>
<li>Click the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility.svg" alt="A screenshot of Napari's eye button" height="30" class="figure"> icon next
to ‘instance_seg’ in the layer list to make it visible.</li>
<li>Click the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility.svg" alt="A screenshot of Napari's eye button" height="30" class="figure"> icon next
to ‘semantic_seg’ in the layer list to hide it.</li>
<li>Make sure the ‘instance_seg’ layer is selected in the layer list. It
should be highlighted in blue.</li>
</ul>
<p>If you hover over the segmentation now, you should see that each
nucleus has a different pixel value. We can find the maximum value via
the console like so:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># Get the instance segmentation data</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>image <span class="op">=</span> viewer.layers[<span class="st">'instance_seg'</span>].data</span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="co"># Print the maximum pixel value</span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="bu">print</span>(image.<span class="bu">max</span>())</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">19</span></span></code></pre>
</div>
<p>Instance segmentations tend to have many more pixel values, as there
is one per object ‘instance’. When working with images with very large
numbers of cells, you can quickly end up with hundreds or thousands of
pixel values in an instance segmentation.</p>
<p>So to summarise - segmentations are just images where the pixel
values represent specific categories, or individual instances of that
category. You will sometimes see them referred to as ‘label images’,
especially in the context of Napari. Segmentations with only two values
(e.g. 0 and 1 for background vs some specific category) are often
referred to as ‘masks’.</p>
<p>Note that in this episode we focus on segmentations where every pixel
is assigned to a specific class or instance. There are many segmentation
methods though that will instead output the <em>probability</em> of a
pixel belonging to a specific class or instance. These may be stored as
float images, where each pixel has a decimal value between 0 and 1
denoting their probability.</p>
<div id="segmentation-shapes-and-data-types" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="segmentation-shapes-and-data-types" class="callout-inner">
<h3 class="callout-title">Segmentation shapes and data types<a class="anchor" aria-label="anchor" href="#segmentation-shapes-and-data-types"></a>
</h3>
<div class="callout-content">
<p>Copy and paste the following into Napari’s console:</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>nuclei <span class="op">=</span> viewer.layers[<span class="st">"nuclei"</span>].data</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>semantic_seg <span class="op">=</span> viewer.layers[<span class="st">"semantic_seg"</span>].data</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a>instance_seg <span class="op">=</span> viewer.layers[<span class="st">"instance_seg"</span>].data</span></code></pre>
</div>
<ul>
<li><p>What shape are the <code>nuclei</code>, <code>semantic_seg</code>
and <code>instance_seg</code> images?</p></li>
<li><p>What pattern do you see in the shapes? Why?</p></li>
<li><p>What bit-depth do the <code>semantic_seg</code> and
<code>instance_seg</code> images have?</p></li>
<li><p>If I want to make an instance segmentation of 100 nuclei - what’s
the minimum bit depth I could use? (assume we’re storing unsigned
integers) Choose from 8-bit, 16-bit or 32-bit.</p></li>
<li><p>If I want to make an instance segmentation of 1000 nuclei -
what’s the minimum bit depth I could use? (assume we’re storing unsigned
integers) Choose from 8-bit, 16-bit or 32-bit.</p></li>
</ul>
<p>Recall that we looked at image shapes and bit-depths in the <a href="what-is-an-image.html">‘What is an image?’ episode</a>.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" data-bs-parent="#accordionSolution2" aria-labelledby="headingSolution2">
<div class="accordion-body">
<div class="section level3">
<h3 id="shapes-pattern">Shapes + pattern<a class="anchor" aria-label="anchor" href="#shapes-pattern"></a>
</h3>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="bu">print</span>(nuclei.shape)</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="bu">print</span>(semantic_seg.shape)</span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="bu">print</span>(instance_seg.shape)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(60, 256, 256)
(60, 256, 256)
(60, 256, 256)</code></pre>
</div>
<p>All the images have the same shape of (60, 256, 256). This makes
sense as ‘semantic_seg’ and ‘instance_seg’ are segmentations made from
the nuclei image. Therefore, they need to be exactly the same size as
the nuclei image in order to assign a label to each pixel.</p>
<p>Note that even though the segmentation and image have the same shape,
they often won’t have the same filesize when saved. This is due to
segmentations containing many identical values (e.g. large parts of the
image might be 0, for background) meaning they can often be compressed
heavily without loss of information. Recall that we looked at
compression in the <a href="filetypes-and-metadata.html#compression">filetypes and metadata
episode</a>.</p>
</div>
<div class="section level3">
<h3 id="bit-depths">Bit depths<a class="anchor" aria-label="anchor" href="#bit-depths"></a>
</h3>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="bu">print</span>(semantic_seg.dtype)</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="bu">print</span>(instance_seg.dtype)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="va">int8</span></span>
<span><span class="va">int32</span></span></code></pre>
</div>
<p>The data type (<code>.dtype</code>) of ‘semantic_seg’ contains the
number 8, showing that it is an 8-bit image. The data type of
‘instance_seg’ contains the number 32, showing that it is a 32-bit
image.</p>
<p>Note that in this case, as the instance segmentation only contains 19
nuclei, it could also have been stored as 8-bit (and probably should
have been, as this would provide a smaller file size!). The bit depth
was increased by some of the image processing operations used to
generate the instance segmentation, which is a common side effect as
we’ll discuss in the <a href="filters-and-thresholding.html#thresholding-the-blurred-image">next
episode</a>. If we wanted to reduce the bit depth, we could right click
on the ‘instance_seg’ layer in the layer list, then select
<code>Convert data type</code>. For more complex conversions, we would
need to use python commands in Napari’s console - e.g. see the <a href="https://bioimagebook.github.io/chapters/1-concepts/3-bit_depths/python.html" class="external-link">‘Python:
Types and bit-depths’</a> chapter form Pete Bankhead’s bioimage
book.</p>
</div>
<div class="section level3">
<h3 id="bit-depth-for-100-nuclei">Bit depth for 100 nuclei<a class="anchor" aria-label="anchor" href="#bit-depth-for-100-nuclei"></a>
</h3>
<p>To store an instance segmentation of 100 nuclei, we need to store
values from 0-100 (with convention being that 0 represents the
background pixels, and the rest are values for individual nuclei). An
8-bit unsigned integer image can store values from 0-255 and would
therefore be sufficient in this case.</p>
</div>
<div class="section level3">
<h3 id="bit-depth-for-1000-nuclei">Bit depth for 1000 nuclei<a class="anchor" aria-label="anchor" href="#bit-depth-for-1000-nuclei"></a>
</h3>
<p>To store an instance segmentation of 1000 nuclei, we need to store
values from 0-1000 (with 0 being the background, and the rest values for
individual nuclei). A 16-bit unsigned integer image can store values
from 0-65535 and would therefore be sufficient in this case.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="how-to-create-segmentations"><h2 class="section-heading">How to create segmentations?<a class="anchor" aria-label="anchor" href="#how-to-create-segmentations"></a>
</h2>
<hr class="half-width">
<p>There are many, many different methods for creating segmentations!
These range from fully manual methods where you ‘paint’ the pixels in
each category, to complicated automated methods relying on classical
machine learning or deep learning models. The best method to use will
depend on your research question and the type of images you have in your
datasets. As we’ve mentioned before, it’s usually best to use the
simplest method that works well for your data. In this episode we’ll
look at manual methods for segmentation within Napari, before moving on
to more automated methods in later episodes.</p>
</section><section id="manual-segmentation-in-napari"><h2 class="section-heading">Manual segmentation in Napari<a class="anchor" aria-label="anchor" href="#manual-segmentation-in-napari"></a>
</h2>
<hr class="half-width">
<p>Let’s look at how we can create our own segmentations in Napari.
First remove the two segmentation layers:</p>
<ul>
<li>Click on ‘instance_seg’, then <kbd>shift</kbd> + click
‘semantic_seg’</li>
<li>Click the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/delete.svg" alt="A screenshot of Napari's delete layer button" height="30" class="figure">
icon to remove these layers.</li>
</ul>
<p>Then click on the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/new_labels.svg" alt="A screenshot of Napari's labels layer button" height="30" class="figure">
icon (at the top of the layer list) to create a new <code>Labels</code>
layer.</p>
<p>Recall from the <a href="imaging-software.html#layer-list">imaging
software episode</a>, that Napari supports different kinds of layers.
For example, <code>Image</code> layers for standard images,
<code>Point</code> layers for points, <code>Shape</code> layers for
shapes like rectangles, ellipses or lines etc… <code>Labels</code>
layers are the type used for segmentations, and provide access to many
new settings in the layer controls:</p>
<figure><img src="../fig/label-layer-controls.png" alt="A screenshot of the layer controls for  labels layers in Napari" class="figure mx-auto d-block"></figure><p>Let’s start by painting an individual nucleus. Select the paintbrush
by clicking the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/paint.svg" alt="A screenshot of Napari's paintbrush button" height="30" class="figure"> icon
in the top row of the layer controls. Then click and drag across the
image to label pixels. You can change the size of the brush using the
‘brush size’ slider in the layer controls. To return to normal movement,
you can click the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/pan_arrows.svg" alt="A screenshot of Napari's pan arrows button" height="30" class="figure"> icon
in the top row of the layer controls, or hold down spacebar to activate
it temporarily (this is useful if you want to pan slightly while
painting). To remove painted areas, you can activate the label eraser by
clicking the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/erase.svg" alt="A screenshot of Napari's erase button" height="30" class="figure"> icon.</p>
<figure><img src="../fig/single-painted-nucleus.png" alt="A screenshot of a single manually  painted nucleus in Napari" class="figure mx-auto d-block"></figure><p>If you hover over the image and examine the pixel values, you will
see that all your painted areas have a pixel value of 1. This
corresponds to the number shown next to ‘label:’ in the layer
controls.</p>
<p>To paint with a different pixel value, either click the + icon on the
right side of this number, or type another value into the box. Let’s
paint another nucleus with a pixel value of 2:</p>
<figure><img src="../fig/two-painted-nuclei.png" alt="A screenshot of two manually painted nuclei  in Napari" class="figure mx-auto d-block"></figure><p>When you paint with a new value, you’ll see that Napari automatically
assigns it a new colour. This is because <code>Labels</code> layers use
<a href="https://napari.org/stable/howtos/layers/labels.html#shuffling-label-colors" class="external-link">a
special colormap/LUT for their pixel values</a>. Recall from the <a href="image-display.html#colormaps-luts">image display episode</a> that
colormaps are a way to convert pixel values into corresponding colours
for display. The colormap for <code>Labels</code> layers will assign
random colours to each pixel value, trying to ensure that nearby values
(like 2 vs 3) are given dissimilar colours. This helps to make it easier
to distinguish different labels. You can shuffle the colours used by
clicking the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/shuffle.svg" alt="A screenshot of Napari's shuffle button" height="30" class="figure"> icon in
the top row of the layer controls. Note that the pixel value of 0 will
always be shown as transparent - this is because it is usually used to
represent the background.</p>
<div id="manual-segmentation-in-napari-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="manual-segmentation-in-napari-1" class="callout-inner">
<h3 class="callout-title">Manual segmentation in Napari<a class="anchor" aria-label="anchor" href="#manual-segmentation-in-napari-1"></a>
</h3>
<div class="callout-content">
<p>Try labelling more individual nuclei in this image, making sure each
gets its own pixel value (label). Investigate the other settings in the
layer controls:</p>
<ul>
<li>What does the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/fill.svg" alt="A screenshot of Napari's fill button" height="30" class="figure"> icon
do?</li>
<li>What does the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/picker.svg" alt="A screenshot of Napari's picker button" height="30" class="figure"> icon
do?</li>
<li>What does the ‘contour’ setting control?</li>
<li>What does the ‘n edit dim’ setting control?</li>
<li>What does the ‘preserve labels’ setting control?</li>
</ul>
<p>Remember that you can hover over buttons to see a helpful popup with
some more information. You can also look in <a href="https://napari.org/stable/howtos/layers/labels.html" class="external-link">Napari’s
documentation</a> on <code>Labels</code> layers.</p>
<p>Note that if you make a mistake you can use <kbd>Ctrl</kbd> +
<kbd>Z</kbd> to undo your last action. If you need to re-do it use
<kbd>Shift</kbd> + <kbd>Ctrl</kbd> + <kbd>Z</kbd>.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" data-bs-parent="#accordionSolution3" aria-labelledby="headingSolution3">
<div class="accordion-body">
<div class="section level3">
<h3 id="icon">
<img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/fill.svg" alt="A screenshot of Napari's fill button" height="30" class="figure"> icon<a class="anchor" aria-label="anchor" href="#icon"></a>
</h3>
<p>This icon activates the ‘fill bucket’. It will fill whatever label
you click on with the currently active label (as shown next to ‘label:’
in the layer controls). For example, if your active label is 2, then
clicking on a nucleus with label 1 will change it to 2. You can see more
details about the fill bucket in <a href="https://napari.org/stable/howtos/layers/labels.html#using-the-fill-bucket" class="external-link">Napari’s
documentation</a></p>
</div>
<div class="section level3">
<h3 id="icon-1">
<img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/picker.svg" alt="A screenshot of Napari's picker button" height="30" class="figure">
icon<a class="anchor" aria-label="anchor" href="#icon-1"></a>
</h3>
<p>This icon activates the ‘colour picker’. This allows you to click on
a pixel in the viewer and immediately select the corresponding label.
This is especially useful when you have hundreds of labels, which can
make them difficult to keep track of with the ‘label:’ option only.</p>
</div>
<div class="section level3">
<h3 id="contour">Contour<a class="anchor" aria-label="anchor" href="#contour"></a>
</h3>
<p>The contour setting controls how labels are displayed. By default
(when 0), labelled pixels are displayed as solid colours. Increasing the
contour value to one or higher, will change this to show the outline of
labelled regions instead. Higher values will result in a thicker
outline. This can be useful if you don’t want to obscure the underlying
image while viewing your segmentation.</p>
</div>
<div class="section level3">
<h3 id="n-edit-dim">n edit dim<a class="anchor" aria-label="anchor" href="#n-edit-dim"></a>
</h3>
<p>This controls the number of dimensions that are used for editing
labels. By default, this is 2 so all painting and editing of the
<code>Labels</code> layer will only occur in 2D. If you want to
paint/edit in 3D you can increase this to 3. Now painting will not only
affect the currently visible image slice, but slices above and below it
also. Be careful with the size of your brush when using this option!</p>
</div>
<div class="section level3">
<h3 id="preserve-labels">preserve labels<a class="anchor" aria-label="anchor" href="#preserve-labels"></a>
</h3>
<p>Activating ‘preserve labels’ will ensure that any painting/erasing
you do will only affect your currently active label. This is especially
useful if you need to paint very close to existing labels without
affecting them.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="manual-vs-automated"><h2 class="section-heading">Manual vs automated?<a class="anchor" aria-label="anchor" href="#manual-vs-automated"></a>
</h2>
<hr class="half-width">
<p>Now that we’ve done some manual segmentation in Napari, you can
probably tell that this is a very slow and time consuming process
(especially if you want to segment in full 3D!). It’s also difficult to
keep the segmentation fully consistent, especially if multiple people
are contributing to it. For example, it can be difficult to tell exactly
where the boundary of a nucleus should be placed, as there may be a
gradual fall off in pixel intensity at the edge rather than a sharp
drop.</p>
<p>Due to these challenges, full manual segmentation is generally only
suitable for small images in small quantities. If we want to efficiently
scale to larger images in large quantities, then we need more automated
methods. We’ll look at some of these in the next few episodes, where we
investigate some ‘classic’ image processing methods that generate
segmentations directly from image pixel values. These automated
approaches will also help us achieve a less variable segmentation, that
segments objects in a fully consistent way.</p>
<p>For more complex segmentation scenarios, where the boundaries between
objects are less clear, we may need to use machine learning or deep
learning models. These models learn to segment images based on provided
‘groundtruth’ data. This ‘groundtruth’ is usually many small, manually
segmented patches of our images of interest. While generating this
manual groundtruth is still slow and time consuming, the advantage is
that we only need to segment a small subset of our images (rather than
the entire thing). Also, once the model is trained, it can be re-used on
similar image data.</p>
<p>It’s worth bearing in mind that automated methods are rarely perfect
(whether they’re classic image processing, machine learning or deep
learning based). It’s very likely that you will have to do some manual
segmentation cleanup from time to time.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li><p>The first step in any image processing workflow is quality
control. For example, checking the histograms of your images.</p></li>
<li><p>Segmentation is the process of identifying what each pixel in an
image represents e.g. is that pixel part of a nucleus or not?</p></li>
<li><p>Segmentation can be broadly split into ‘semantic segmentation’
and ‘instance segmentation’. Semantic segmentation assigns pixels to
specific classes (like nuclei or background), while instance
segmentation assigns pixels to individual ‘instances’ of a class (like
individual nuclei).</p></li>
<li><p>Segmentations are represented in the computer in the same way as
standard images. The difference is that pixel values represent classes
or instances, rather than light intensity.</p></li>
<li><p>Napari uses <code>Labels</code> layers for segmentations. These
offer various annotation tools like the paintbrush, fill bucket
etc.</p></li>
<li><p>Fully manual segmentation is generally only suitable for small
images. More automated approaches (based on classic image processing,
machine learning or deep learning) are necessary to scale to larger
images. Even with automated methods, manual segmentation is often still
a key component - both for generating groundtruth data and cleaning up
problematic areas of the final segmentation.</p></li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-filters-and-thresholding"><p>Content from <a href="filters-and-thresholding.html">Filters and thresholding</a></p>
<hr>
<p> Last updated on 2024-04-30 | 
        
        <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/edit/main/episodes/filters-and-thresholding.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 60 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can thresholding be used to create masks?</li>
<li>What are filters and how do they work?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li><p>Threshold an image manually using its histogram, and display the
result in Napari</p></li>
<li><p>Use some Napari plugins to perform two commonly used image
processing steps: gaussian blurring and otsu thresholding</p></li>
<li><p>Explain how filters work - e.g. what is a kernel? What does the
‘sigma’ of a gaussian blur change?</p></li>
<li><p>Explain the difference between Napari’s
<code>.add_labels()</code> and <code>.add_image()</code></p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>In the previous episode, we started to develop an image processing
workflow to count the number of cells in an image. Our initial pipeline
relied on manual segmentation:</p>
<ol style="list-style-type: decimal">
<li>Quality Control</li>
<li>Manual segmentation of nuclei</li>
<li>Count nuclei</li>
</ol>
<p>There are a number of issues with relying on manual segmentation for
this task. For example, as covered in the <a href="quality-control-and-manual-segmentation.html#manual-vs-automated">previous
episode</a>, it is difficult to keep manual segmentation fully
consistent. If you ask someone to segment the same cell multiple times,
the result will always be slightly different. Also, manual segmentation
is very slow and time consuming! While it is feasible for small images
in small quantities, it would take much too long for large datasets of
hundreds or thousands of images.</p>
<p>This is where we need to introduce more automated methods, which we
will start to look at in this episode. Most importantly, using more
automated methods has the benefit of making your image processing more
<em>reproducible</em> - by having a clear record of every step that
produced your final result, it will be easier for other researchers to
replicate your work and also try similar methods on their own data.</p>
<section id="thresholding"><h2 class="section-heading">Thresholding<a class="anchor" aria-label="anchor" href="#thresholding"></a>
</h2>
<hr class="half-width">
<p>If you don’t have Napari’s ‘Cells (3D + 2Ch)’ image open, then open
it with:<br><code>File &gt; Open Sample &gt; napari builtins &gt; Cells (3D + 2Ch)</code></p>
<p>Make sure you only have ‘nuclei’ in the layer list. Select any
additional layers, then click the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/delete.svg" alt="A screenshot of Napari's delete layer button" height="30" class="figure">
icon to remove them. Also, select the nuclei layer (should be
highlighted in blue), and change its colormap from ‘green’ to ‘gray’ in
the layer controls.</p>
<figure><img src="../fig/nuclei-gray-napari.png" alt="A screenshot of nuclei in Napari using  the gray colormap" class="figure mx-auto d-block"></figure><p>Now let’s look at how we can create a mask of the nuclei. Recall from
the <a href="quality-control-and-manual-segmentation.html">last
episode</a>, that a ‘mask’ is a segmentation with only two labels -
background + your class of interest (in this case nuclei).</p>
<p>A simple approach to create a mask is to use a method called
‘thresholding’. Thresholding works best when there is a clear separation
in pixel values between the class of interest and the rest of the image.
In these cases, thresholding involves choosing one or more specific
pixel values to use as the cut-off between background and your class of
interest. For example, in this image, you could choose a pixel value of
100 and set all pixels with a value less than that as background, and
all pixels with a value greater than that as nuclei. How do we go about
choosing a good threshold though?</p>
<p>One common approach to choosing a threshold is to use the image
histogram - recall that we looked at histograms in detail in the <a href="image-display.html">image display episode</a>. Let’s go ahead and
open a histogram with:<br><code>Plugins &gt; napari Matplotlib &gt; Histogram</code></p>
<figure><img src="../fig/nuclei-histogram.png" alt="A histogram of the 29th z slice of Napari's  cell sample image" class="figure mx-auto d-block"></figure><p>From this histogram, we can see two peaks - a larger one at low
intensity, then a smaller one at higher intensity. Looking at the image,
it would make sense if the lower intensity peak represented the
background, with the higher one representing the nuclei. We can verify
this by adjusting the contrast limits in the layer controls. If we move
the left contrast limits node to the right of the low intensity peak
(around 8266), we can still clearly see the nuclei:</p>
<figure><img src="../fig/contrast-limit-8266-nuclei.png" alt="Left, nuclei with gray colormap.  Right, histogram of the same image. Both with left contrast limit set  to 8266." class="figure mx-auto d-block"></figure><p>If we move the left contrast limits node to the right of the higher
intensity peak (around 28263), most of the nuclei disappear from the
image:</p>
<figure><img src="../fig/contrast-limit-28263-nuclei.png" alt="Left, nuclei with gray colormap.  Right, histogram of the same image. Both with left contrast limit set  to 28263." class="figure mx-auto d-block"></figure><p>Recall that you can set specific values for the contrast limits by
right clicking on the contrast limits slider in Napari.</p>
<p>This demonstrates that the higher intensity peak is the one we are
interested in, and we should set our threshold accordingly.</p>
<p>We can set this threshold with some commands in Napari’s console:</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co"># Get the image data for the nuclei</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>nuclei <span class="op">=</span> viewer.layers[<span class="st">"nuclei"</span>].data</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># Create mask with a threshold of 8266</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>mask <span class="op">=</span> nuclei <span class="op">&gt;</span> <span class="dv">8266</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co"># Add mask to the Napari viewer</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>viewer.add_labels(mask)</span></code></pre>
</div>
<p>You should see a mask appear that highlights the nuclei in brown. If
we set the nuclei contrast limits back to normal (select ‘nuclei’ in the
layer list, then drag the left contrast limits node back to zero), then
toggle on/off the mask or nuclei layers with the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility.svg" alt="A screenshot of Napari's eye button" height="30" class="figure"> icon, you
should see that the brown areas match the nucleus boundaries reasonably
well. They aren’t perfect though! The brown regions have a speckled
appearance where some regions inside nuclei aren’t labelled and some
areas in the background are incorrectly labelled.</p>
<figure><img src="../fig/threshold-mask.png" style="width:60.0%" alt="Mask of nuclei (brown) overlaid on nuclei  image - created with manual thresholding" class="figure mx-auto d-block"></figure><p>This is mostly due to the presence of noise in our image (as we
looked at in the <a href="choosing-acquisition-settings.html#signal-to-noise-ratio">choosing
acquisition settings episode</a>). For example, the dark image
background isn’t completely uniform - there are random fluctuations in
the pixel values giving it a ‘grainy’ appearance when we zoom in. These
fluctuations in pixel value make it more difficult to set a threshold
that fully separates the nuclei from the background. For example,
background pixels that are brighter than average may exceed our
threshold of 8266, while some nuclei pixels that are darker than average
may fall below it. This is the main cause of the speckled appearance of
some of our mask regions.</p>
<p>To improve the mask we must find a way to reduce these fluctuations
in pixel value. This is usually achieved by blurring the image (also
referred to as smoothing). We’ll look at this in detail <a href="#filters">later in the episode</a>.</p>
<div id="add_labels-vs-.add_image" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="add_labels-vs-.add_image" class="callout-inner">
<h3 class="callout-title">
<code>.add_labels()</code>vs<code>.add_image</code><a class="anchor" aria-label="anchor" href="#add_labels-vs-.add_image"></a>
</h3>
<div class="callout-content">
<p>In the code blocks above, you will notice that we use
<code>.add_labels()</code>, rather than <code>.add_image()</code> as we
have in previous episodes. <code>.add_labels()</code> will ensure our
mask is added as a <code>Labels</code> layer, giving us access to all
the annotation tools and settings for segmentations (as covered in the
<a href="quality-control-and-manual-segmentation.html">manual
segmentation episode</a>). <code>add_image()</code> would create a
standard <code>Image</code> layer, which doesn’t give us easy access to
the annotation tools.</p>
</div>
</div>
</div>
<div id="manual-thresholds" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="manual-thresholds" class="callout-inner">
<h3 class="callout-title">Manual thresholds<a class="anchor" aria-label="anchor" href="#manual-thresholds"></a>
</h3>
<div class="callout-content">
<p>In this exercise, we’ll practice choosing manual thresholds based on
an image’s histogram. For this, we’ll use a simple test image containing
three shapes (rectangle, circle and triangle) with different mean
intensities.</p>
<p>Copy and paste the following into Napari’s console to generate and
load this test image:</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">import</span> skimage</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="im">from</span> skimage.draw <span class="im">import</span> disk, polygon</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="im">from</span> skimage.util <span class="im">import</span> random_noise, img_as_ubyte</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="im">from</span> packaging <span class="im">import</span> version</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>image <span class="op">=</span> np.full((<span class="dv">100</span>, <span class="dv">100</span>), <span class="dv">10</span>, dtype<span class="op">=</span><span class="st">"uint8"</span>)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>image[<span class="dv">10</span>:<span class="dv">50</span>, <span class="dv">20</span>:<span class="dv">80</span>] <span class="op">=</span> <span class="dv">80</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>image[disk((<span class="dv">75</span>, <span class="dv">75</span>), <span class="dv">15</span>)] <span class="op">=</span> <span class="dv">140</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>rr, cc <span class="op">=</span> polygon([<span class="dv">90</span>, <span class="dv">60</span>, <span class="dv">90</span>], [<span class="dv">10</span>, <span class="dv">30</span>, <span class="dv">50</span>])</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>image[rr, cc] <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="cf">if</span> version.parse(skimage.__version__) <span class="op">&lt;</span> version.parse(<span class="st">"0.21"</span>):</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>  image <span class="op">=</span> img_as_ubyte(random_noise(image, var<span class="op">=</span><span class="fl">0.0005</span>, seed<span class="op">=</span><span class="dv">6</span>))</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>  image <span class="op">=</span> img_as_ubyte(random_noise(image, var<span class="op">=</span><span class="fl">0.0005</span>, rng<span class="op">=</span><span class="dv">6</span>))</span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>viewer.add_image(image)</span></code></pre>
</div>
<figure><img src="../fig/manual-thresholding-exercise-shapes.png" alt="Test image containing a  rectangle, circle and triangle" class="figure mx-auto d-block"></figure><p>Create a mask for each shape by choosing thresholds based on the
image’s histogram. You can set a threshold as all pixels above a certain
value:</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>mask <span class="op">=</span> image <span class="op">&gt;</span> <span class="dv">40</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>viewer.add_labels(mask)</span></code></pre>
</div>
<p>Or all pixels below a certain value:</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>mask <span class="op">=</span> image <span class="op">&lt;</span> <span class="dv">40</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>viewer.add_labels(mask)</span></code></pre>
</div>
<p>Or all pixels between two values:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>mask <span class="op">=</span> (image <span class="op">&gt;</span> <span class="dv">20</span>) <span class="op">&amp;</span> (image <span class="op">&lt;</span> <span class="dv">40</span>)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>viewer.add_labels(mask)</span></code></pre>
</div>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>First, we show a histogram for the image by selecting the ‘image’
layer, then:<br><code>Plugins &gt; napari Matplotlib &gt; Histogram</code></p>
<figure><img src="../fig/manual-thresholding-exercise-histogram.png" alt="Histogram of the  shape image" class="figure mx-auto d-block"></figure><p>By moving the left contrast limits node we can figure out what each
peak represents. You should see that the peaks from left to right
are:</p>
<ul>
<li>background</li>
<li>rectangle</li>
<li>circle</li>
<li>triangle</li>
</ul>
<p>Then we set thresholds for each (as below). Note that you may not
have exactly the same threshold values as we use here! There are many
different values that can give good results, as long as they fall in the
gaps between the peaks in the image histogram.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>rectangle <span class="op">=</span> (image <span class="op">&gt;</span> <span class="dv">53</span>) <span class="op">&amp;</span> (image <span class="op">&lt;</span> <span class="dv">110</span>)</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>viewer.add_labels(rectangle)</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>circle <span class="op">=</span> (image <span class="op">&gt;</span> <span class="dv">118</span>) <span class="op">&amp;</span> (image <span class="op">&lt;</span> <span class="dv">166</span>)</span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>viewer.add_labels(circle)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>triangle <span class="op">=</span> image <span class="op">&gt;</span> <span class="dv">171</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>viewer.add_labels(triangle)</span></code></pre>
</div>
</div>
</div>
</div>
</div>
</section><section id="adding-plugins-to-help-with-segmentation"><h2 class="section-heading">Adding plugins to help with segmentation<a class="anchor" aria-label="anchor" href="#adding-plugins-to-help-with-segmentation"></a>
</h2>
<hr class="half-width">
<p>Fortunately for us, segmenting images in the presence of noise is a
widely studied problem, so there are a many existing plugins we can
install to help. If you need a refresher on the details of how to find
and install plugins in Napari, see the <a href="image-display.html#napari-plugins">image display episode</a>.</p>
<p>We will use two plugins: <a href="https://www.napari-hub.org/plugins/napari-segment-blobs-and-things-with-membranes" class="external-link"><code>napari-segment-blobs-and-things-with-membranes</code></a>
and <a href="https://www.napari-hub.org/plugins/napari-simpleitk-image-processing" class="external-link"><code>napari-simpleitk-image-processing</code></a>.
Both of these plugins add many features that can aid with image
segmentation and other image processing tasks. Here, we’ve selected only
these two to keep installation fast and easy for this course. If you use
Napari more in your own work though, you may want to consider using <a href="https://www.napari-hub.org/plugins/devbio-napari" class="external-link"><code>devbio-napari</code></a>
instead. <code>devbio-napari</code> bundles many plugins into one useful
package (including the two plugins we are about to install!). There’s
detailed information on their documentation pages if you’re
interested.</p>
<p>In the top menu-bar of Napari select:<br><code>Plugins &gt; Install/Uninstall Plugins...</code></p>
<p>Then search for
<code>napari-segment-blobs-and-things-with-membranes</code> and click
the blue button labelled ‘install’. Wait for the installation to
complete.</p>
<figure><img src="../fig/napari-segment-blobs-installation.png" alt="Screenshot of plugin  installation window for napari-segment-blobs-and-things-with-membranes" class="figure mx-auto d-block"></figure><p>Then search for <code>napari-simpleitk-image-processing</code> and
click the blue install button. Wait for the installation to
complete.</p>
<figure><img src="../fig/napari-simple-itk-installation.png" alt="Screenshot of plugin  installation window for napari-simpleitk-image-processing" class="figure mx-auto d-block"></figure><p>Once both plugins are installed, <strong>you will need to close and
re-open Napari</strong>.</p>
</section><section id="filters"><h2 class="section-heading">Filters<a class="anchor" aria-label="anchor" href="#filters"></a>
</h2>
<hr class="half-width">
<p>Let’s open the cells image in Napari again:<br><code>File &gt; Open Sample &gt; napari builtins &gt; Cells (3D + 2Ch)</code></p>
<p>As before, make sure you only have ‘nuclei’ in the layer list. Select
any additional layers, then click the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/delete.svg" alt="A screenshot of Napari's delete layer button" height="30" class="figure">
icon to remove them. Also, select the nuclei layer (should be
highlighted in blue), and change its colormap from ‘green’ to ‘gray’ in
the layer controls.</p>
<figure><img src="../fig/nuclei-gray.png" style="width:40.0%" alt="Nuclei image with gray colormap" class="figure mx-auto d-block"></figure><p>With the new plugins installed, you should see many new options under
<code>Tools</code> in the top menu-bar of Napari. You can find out more
about these options using the plugin’s documentation which was linked in
the <a href="#adding-plugins-to-help-with-segmentation">previous
section</a>.</p>
<p>For now, we are interested in the options under:<br><code>Tools &gt; Filtering / noise removal</code></p>
<p>Specifically, the two options labelled ‘gaussian’ - you should see
one ending in ‘n-SimpleITK’ and another ending ‘scikit-image, nsbatwm’.
These are two slightly different implementations of a <em>gaussian
blur</em>, one from the <code>napari-simpleitk-image-processing</code>
plugin and the other from the
<code>napari-segment-blobs-and-things-with-membranes</code> plugin. Both
should work perfectly well for our image, but let’s use the one ending
with ‘scikit-image, nsbatwm’ for now.</p>
<p>If you click on this option, you should see a new panel appear on the
right side of Napari:</p>
<figure><img src="../fig/gaussian-options.png" style="width:40.0%" alt="Screenshot of settings for gaussian blur in  Napari" class="figure mx-auto d-block"></figure><p>Make sure you have ‘nuclei(data)’ selected on the ‘image’ row, then
click run:</p>
<figure><img src="../fig/nuclei-blurred-1.png" style="width:40.0%" alt="Nuclei image after gaussian blur with sigma  of 1" class="figure mx-auto d-block"></figure><p>You should see a new image appear in the layer list called ‘Result of
gaussian_blur’, which is a slightly blurred version of the original
nuclei image.</p>
<p>Try increasing the ‘sigma’ value to three and clicking run again:</p>
<figure><img src="../fig/nuclei-blurred-3.png" style="width:40.0%" alt="Nuclei image after gaussian blur with sigma  of 3" class="figure mx-auto d-block"></figure><p>You should see that the ‘Result of gaussian_blur’ layer is updated to
show a much more heavily blurred version of the original nuclei
image.</p>
<p>What’s happening here? What exactly does a gaussian blur do? A
gaussian blur is an example of a ‘linear filter’ which is used to
manipulate pixel values in images. When a filter is applied to an image,
each pixel value is replaced by some combination of the pixel values
around it. For example, below is shown a small, zoomed-in area of the
nucleus image. Here, the value of the red pixel could be affected by all
values within the displayed 3x3 box:</p>
<figure><img src="../fig/nuclei-kernel-area.png" style="width:60.0%" alt="Small zoomed-in area of the nucleus image  with a pixel highlighted in red. Around this pixel is shown a  3x3 box." class="figure mx-auto d-block"></figure><p>Exactly what this effect is, and the size of the region it involves,
is controlled by the filter’s ‘kernel’. The kernel can be thought of as
another very small image, where the pixel values control the effect of
the filter. An example 3x3 kernel is shown below:</p>
<figure><img src="../fig/nuclei-kernel.png" alt="Left - small area of the nucleus image with a  pixel highlighted in red. Around this pixel is shown a 3x3 box. Right - example  of a 3x3 kernel" class="figure mx-auto d-block"></figure><p>While the above image uses a 3x3 kernel, they can have many different
sizes! Kernels tend to be odd in size: 3x3, 5x5, 9x9, etc. This is to
allow the current pixel that you will be replacing to lie perfectly at
the centre of the kernel, with equal spacing on all sides.</p>
<p>To determine the new value of a pixel (like the red pixel above), the
kernel is placed over the target image centred on the pixel of interest.
Then, for each position in the kernel, we multiply the image pixel value
with the kernel pixel value. For the 3x3 kernel example above, we
multiply the image pixel value one above and one to the left of our
current pixel with the pixel value in the top left position of the
kernel. Then we multiply the value for the pixel just above our current
pixel with the top middle kernel value and so on… The result of the nine
multiplications in each of the nine positions of this kernel are then
summed together to give the new value of the central pixel. To process
an entire image, the kernel is moved across the image pixel by pixel
calculating the new value at each location. Pete Bankhead’s bioimage
book has a <a href="https://bioimagebook.github.io/chapters/2-processing/4-filters/filters.html#mean-filters" class="external-link">really
nice animation of this process</a> , which is worth taking a look
at.</p>
<p>For the gaussian filter we looked at above, the values in the kernel
are based on a ‘gaussian function’. A gaussian function creates a
central peak that reduces in height as we move further away from the
centre. The shape of the peak can be adjusted using different values of
‘sigma’ (the standard deviation of the function), with larger sigma
values resulting in a wider peak:</p>
<figure><img src="../fig/gaussian-1d-comparison.png" alt="Plot of a 1D gaussian function  comparing three different sigma values" class="figure mx-auto d-block"></figure><p>For a gaussian filter, the values are based on a 2D gaussian
function. This gives similar results to the plot above (showing a 1D
gaussian function) with a clear central peak, but now the height
decreases in both x and y:</p>
<figure><img src="../fig/gaussian-2d-comparison.png" alt="Plot of a 2D gaussian function  comparing three different sigma values" class="figure mx-auto d-block"></figure><p>Gaussian kernels show this same pattern, with values highest at the
centre that decrease as we move further away. See the example 5x5 kernel
below:</p>
<figure><img src="../fig/gaussian-kernel.png" style="width:40.0%" alt="An example of a 5x5 gaussian  kernel" class="figure mx-auto d-block"></figure><p>The gaussian filter causes blurring (also known as smoothing), as it
is equivalent to taking a weighted average of all pixel values within
its boundaries. In a weighted average, some values contribute more to
the final result than others. This is determined by their ‘weight’, with
a higher weight resulting in a larger contribution to the result. As a
gaussian kernel has its highest weights at the centre, this means that a
gaussian filter produces an average where central pixels contribute more
than those further away.</p>
<p>Larger sigma values result in larger kernels, which average larger
areas of the image. For example, the <a href="https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.gaussian" class="external-link"><code>scikit-image, nsbatwm</code></a>
implementation we are currently using truncates the kernel after four
standard deviations (sigma). This means that the blurring effect of the
gaussian filter is enhanced with larger sigma, but will also take longer
to calculate. This is a general feature of kernels for different types
of filter - larger kernels tend to result in a stronger effect at the
cost of increased compute time. This time can become significant when
processing very large images in large quantities.</p>
<p>If you want more information about how filters work, both the <a href="https://datacarpentry.org/image-processing/06-blurring.html" class="external-link">data
carpentry image processing lesson</a> and Pete Bankhead’s <a href="https://bioimagebook.github.io/chapters/2-processing/4-filters/filters.html" class="external-link">bioimage
book</a> have chapters going into great detail. Note that here we have
focused on ‘linear’ filters, but there are many types of ‘non-linear’
filter that process pixel values in different ways.</p>
<div id="sigma-vs-full-width-at-half-maximum-fwhm" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="sigma-vs-full-width-at-half-maximum-fwhm" class="callout-inner">
<h3 class="callout-title">Sigma vs full width at half maximum
(FWHM)<a class="anchor" aria-label="anchor" href="#sigma-vs-full-width-at-half-maximum-fwhm"></a>
</h3>
<div class="callout-content">
<p>In this episode, we have focused on the gaussian function’s standard
deviation (sigma) as the main way to control the width of the central
peak. Some image analysis packages will instead use a measure called the
‘full width at half maximum’ or FWHM. The FWHM is the width of the curve
at half of its maximum value (see the diagram below):</p>
<figure><img src="../fig/gaussian-FWHM.png" style="width:80.0%" alt="Diagram of gaussian function with FWHM labelled" class="figure mx-auto d-block"></figure><p>Similar to increasing sigma, a higher FWHM will result in a wider
peak. Note that there is a clear relationship between sigma and FWHM
where:</p>
<p><span class="math display">\[\large FWHM =
sigma\sqrt{8ln(2)}\]</span></p>
</div>
</div>
</div>
<div id="filters-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="filters-1" class="callout-inner">
<h3 class="callout-title">Filters<a class="anchor" aria-label="anchor" href="#filters-1"></a>
</h3>
<div class="callout-content">
<p>Try some of the other filters included with the
<code>napari-segment-blobs-and-things-with-membranes</code> and
<code>napari-simpleitk-image-processing</code> plugins.</p>
<p>For example:</p>
<ul>
<li><p>What does
<code>Tools &gt; Filtering / noise removal &gt; Median (scipy, nsbatwm)</code>
do? How does it compare to the gaussian filter we used earlier?</p></li>
<li><p>What does
<code>Tools &gt; Filtering / edge enhancement &gt; Sobel (n-SimpleITK)</code>
do?</p></li>
<li><p>What does
<code>Tools &gt; Filtering / background removal &gt; Maximum (scipy, nsbatwm)</code>
do?</p></li>
<li><p>What does
<code>Tools &gt; Filtering / background removal &gt; Minimum (scipy, nsbatwm)</code>
do?</p></li>
</ul>
<p>If you’re not sure what the effect is, try searching for the filter’s
name online.</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<div class="section level3">
<h3 id="median">Median<a class="anchor" aria-label="anchor" href="#median"></a>
</h3>
<p>The median filter creates a blurred version of the input image.
Larger values for the ‘radius’ result in a greater blurring effect.</p>
<p>The results of the median filter are similar to a gaussian blur, but
tend to preserve the edges of objects better. For example, here you can
see that it preserves the sharp edges of each nucleus. You can read more
about how the median filter works in Pete Bankhead’s <a href="https://bioimagebook.github.io/chapters/2-processing/4-filters/filters.html#rank-filters" class="external-link">bioimage
book</a>.</p>
</div>
<div class="section level3">
<h3 id="sobel">Sobel<a class="anchor" aria-label="anchor" href="#sobel"></a>
</h3>
<p>Sobel is a type of filter used to detect the edges of objects in an
image. For example, for the nuclei image it gives bright rings around
each nucleus, as well as around some of the brighter patches inside the
nuclei.</p>
</div>
<div class="section level3">
<h3 id="maximum">Maximum<a class="anchor" aria-label="anchor" href="#maximum"></a>
</h3>
<p>A maximum filter replaces each pixel value with the maximum value in
a certain area around it. For example, for the nuclei, you should see
the bright areas expand in size. Increasing the ‘radius’ of the filter
enhances the effect.</p>
</div>
<div class="section level3">
<h3 id="minimum-filter">Minimum filter<a class="anchor" aria-label="anchor" href="#minimum-filter"></a>
</h3>
<p>A minimum filter replaces each pixel value with the minimum value in
a certain area around it. For example, for the nuclei, you should see
the bright areas shrink in size. Increasing the ‘radius’ of the filter
enhances the effect.</p>
</div>
</div>
</div>
</div>
</div>
</section><section id="thresholding-the-blurred-image"><h2 class="section-heading">Thresholding the blurred image<a class="anchor" aria-label="anchor" href="#thresholding-the-blurred-image"></a>
</h2>
<hr class="half-width">
<p>First, let’s clean up our layer list. Make sure you only have the
‘nuclei’ and ‘Result of gaussian_blur’ layers in the layer list - select
any others and remove them by clicking the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/delete.svg" alt="A screenshot of Napari's delete layer button" height="30" class="figure">
icon. Also, close all filter settings panels on the right side of Napari
(apart from the gaussian settings) by clicking the tiny <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility_off.svg" alt="A screenshot of Napari's hide button" height="20" class="figure"> icon at
their top left corner.</p>
<p>Now let’s try thresholding our image again. Make sure you set your
gaussian blur sigma to three, then click ‘Run’.</p>
<p>Then, we’ll apply the same threshold as before, now to the ‘Result of
gaussian_blur’ layer:</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="co"># Get the image data for the blurred nuclei</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>blurred <span class="op">=</span> viewer.layers[<span class="st">"Result of gaussian_blur"</span>].data</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="co"># Create mask with a threshold of 8266</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a>blurred_mask <span class="op">=</span> blurred <span class="op">&gt;</span> <span class="dv">8266</span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co"># Add mask to the Napari viewer</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a>viewer.add_labels(blurred_mask)</span></code></pre>
</div>
<p>You’ll see that we get an odd result - a totally empty image! What
went wrong here? Let’s look at the blurred image’s shape and data type
in the console:</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="bu">print</span>(blurred.shape)</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="bu">print</span>(blurred.dtype)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>(60, 256, 256)
float64</code></pre>
</div>
<p>The shape is as expected, but what happened to the data type! It’s
now a 64-bit float image (rather than the usual unsigned integer
8-bit/16-bit for images). Recall from the <a href="what-is-an-image.html#type">‘What is an image?’ episode</a> that
float images allow values with a decimal point to be stored
e.g. 3.14.</p>
<p>Many image processing operations will convert images to
<code>float32</code> or <code>float64</code>, to provide the highest
precision possible. For example, imagine you have an unsigned-integer
8-bit image with pixel values of all 75. If you divided by two to give
37.5, then these values would all be rounded to 38 when stored (as
<code>uint8</code> can’t store decimal numbers). Only by converting to a
different data type (float) could we store the exact value of this
operation. In addition, by having the highest bit-depth (64-bit), we
ensure the highest possible precision which can be useful to avoid
slight errors after multiple operations are applied one after the other.
In summary, changes in data type are expected in image processing, and
it’s good to keep an eye out for when this is required!</p>
<p>Let’s return to thresholding our image. Close the gaussian panel by
clicking the tiny <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility_off.svg" alt="A screenshot of Napari's hide button" height="20" class="figure"> icon at
its top left corner. Then select the ‘blurred_mask’ in the layer list
and remove it by clicking the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/delete.svg" alt="A screenshot of Napari's delete layer button" height="30" class="figure">
icon. Finally, open the <code>napari-matplotlib</code> histogram again
with:<br><code>Plugins &gt; napari Matplotlib &gt; Histogram</code></p>
<figure><img src="../fig/blurred-nuclei-histogram.png" alt="A histogram of the 29th z slice of  the nuclei image after a gaussian blur. The left contrast limit is set  to 0.134." class="figure mx-auto d-block"></figure><p>Make sure you have ‘Result of gaussian_blur’ selected in the layer
list (should be highlighted in blue).</p>
<p>You should see that this histogram now runs from 0 to 1, reflecting
the new values after the gaussian blur and its new data type. Let’s
again choose a threshold between the two peaks - around 0.134:</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="co"># Get the image data for the blurred nuclei</span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a>blurred <span class="op">=</span> viewer.layers[<span class="st">"Result of gaussian_blur"</span>].data</span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="co"># Create mask with a threshold of 0.134</span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a>blurred_mask <span class="op">=</span> blurred <span class="op">&gt;</span> <span class="fl">0.134</span></span>
<span id="cb10-7"><a href="#cb10-7" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" tabindex="-1"></a><span class="co"># Add mask to the Napari viewer</span></span>
<span id="cb10-9"><a href="#cb10-9" tabindex="-1"></a>viewer.add_labels(blurred_mask)</span></code></pre>
</div>
<figure><img src="../fig/threshold-blurred-mask.png" style="width:60.0%" alt="Mask of nuclei (brown) overlaid on  nuclei image - created with manual thresholding after gaussian blur" class="figure mx-auto d-block"></figure><p>Now we see a much more sensible result - a brown mask that again
highlights the nuclei in brown. To compare to the result with no
smoothing, let’s run our previous threshold again (from the <a href="#thresholding">earlier thresholding section</a>):</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="co"># Get the image data for the nuclei</span></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>nuclei <span class="op">=</span> viewer.layers[<span class="st">"nuclei"</span>].data</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="co"># Create mask with a threshold of 8266</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>mask <span class="op">=</span> nuclei <span class="op">&gt;</span> <span class="dv">8266</span></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a><span class="co"># Add mask to the Napari viewer</span></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a>viewer.add_labels(mask)</span></code></pre>
</div>
<p>If we compare ‘blurred_mask’ and ‘mask’, we can see that blurring
before choosing a threshold results in a much smoother result that has
fewer areas inside nuclei that aren’t labelled, as well as fewer areas
of the background that are incorrectly labelled.</p>
<p>This workflow of blurring an image then thresholding it is a very
common method to provide a smoother, more complete mask. Note that it
still isn’t perfect! If you browse through the z slices with the slider
at the bottom of the image, it’s clear that some areas are still missed
or incorrectly labelled.</p>
</section><section id="automated-thresholding"><h2 class="section-heading">Automated thresholding<a class="anchor" aria-label="anchor" href="#automated-thresholding"></a>
</h2>
<hr class="half-width">
<p>First, let’s clean up our layer list again. Make sure you only have
the ‘nuclei’, ‘mask’, ‘blurred_mask’ and ‘Result of gaussian_blur’
layers in the layer list - select any others and remove them by clicking
the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/delete.svg" alt="A screenshot of Napari's delete layer button" height="30" class="figure">
icon. Then, if you still have the <code>napari-matplotlib</code>
histogram open, close it by clicking the tiny <code>x</code> icon in the
top left corner.</p>
<p>So far we have chosen our threshold manually by looking at the image
histogram, but it would be better to find an automated method to do this
for this. Many such methods exist - for example, see the various options
starting with ‘Threshold’ under
<code>Tools &gt; Segmentation / binarization</code>. One of the most
common methods is <em>Otsu thresholding</em>, which we will look at
now.</p>
<p>Let’s go ahead and apply this to our blurred image:<br><code>Tools &gt; Segmentation / binarization &gt; Threshold (Otsu et al 1979, scikit-image, nsbatwm)</code></p>
<p>This will open a panel on the right side of Napari. Select ‘Result of
gaussian_blur(data)’ in the image row, then click Run:</p>
<figure><img src="../fig/threshold-blurred-otsu-mask.png" style="width:60.0%" alt="Mask of nuclei (brown) overlaid on  nuclei image - created with Otsu thresholding after gaussian blur" class="figure mx-auto d-block"></figure><p>This should produce a mask (in a new layer called ‘Result of
threshold_otsu’) that is very similar to the one we created with a
manual threshold. To make it easier to compare, we can rename some of
our layers by double clicking on their name in the layer list - for
example, rename ‘mask’ to ‘manual_mask’, ‘blurred_mask’ to
‘manual_blurred_mask’, and ‘Result of threshold_otsu’ to
‘otsu_blurred_mask’. Recall that you can change the colour of a mask by
clicking the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/shuffle.svg" alt="A screenshot of Napari's shuffle button" height="30" class="figure"> icon in
the top row of the layer controls. By toggling on/off the relevant <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility.svg" alt="A screenshot of Napari's eye button" height="30" class="figure"> icons, you
should see that Otsu chooses a slightly different threshold than we did
in our ‘manual_blurred_mask’, labelling slightly smaller regions as
nuclei in the final result.</p>
<p>How is Otsu finding the correct threshold? The details are rather
complex, but to summarise - Otsu’s method is trying to find a threshold
that minimises the spread of pixel values in both the background and
your class of interest. In effect, it’s trying to find two peaks in the
image histogram and place a threshold in between them.</p>
<p>This means it’s only suitable for images that show two clear peaks in
their histogram (also known as a ‘bimodal’ histogram). Other images may
require different automated thresholding methods to produce a good
result.</p>
<div id="automated-thresholding-1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="automated-thresholding-1" class="callout-inner">
<h3 class="callout-title">Automated thresholding<a class="anchor" aria-label="anchor" href="#automated-thresholding-1"></a>
</h3>
<div class="callout-content">
<p>For this exercise, we’ll use the same example image as the <a href="#manual-thresholds">manual thresholding exercise</a>. If you don’t
have that image open, run the top code block in that exercise to open
the image:</p>
<figure><img src="../fig/manual-thresholding-exercise-shapes.png" alt="Test image containing a  rectangle, circle and triangle" class="figure mx-auto d-block"></figure><p>Try some of the other automatic thresholding options provided by the
<code>napari-segment-blobs-and-things-with-membranes</code> and
<code>napari-simpleitk-image-processing</code> plugins.</p>
<p>For example:</p>
<ul>
<li><code>Tools &gt; Segmentation / binarization &gt; Threshold (Triangle method, Zack et al 1977, scikit-image, nsbatwm)</code></li>
<li><code>Tools &gt; Segmentation / binarization &gt; Threshold (Yen et al 1995, scikit-image, nsbatwm)</code></li>
<li><code>Tools &gt; Segmentation / binarization &gt; Threshold (Li et al 1993, scikit-image, nsbatwm)</code></li>
<li><code>Tools &gt; Segmentation / binarization &gt; Threshold Otsu, multiple thresholds (n-SimpleITK)</code></li>
</ul>
<p>How do they compare to standard Otsu thresholding?<br><code>Tools &gt; Segmentation / binarization &gt; Threshold (Otsu et al 1979, scikit-image, nsbatwm)</code></p>
<p>Recall that you can change the colour of a mask by clicking the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/shuffle.svg" alt="A screenshot of Napari's shuffle button" height="30" class="figure"> icon in
the top row of the layer controls.</p>
</div>
</div>
</div>
<div id="accordionSolution3" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution3" aria-expanded="false" aria-controls="collapseSolution3">
  <h4 class="accordion-header" id="headingSolution3">Show me the solution</h4>
</button>
<div id="collapseSolution3" class="accordion-collapse collapse" aria-labelledby="headingSolution3" data-bs-parent="#accordionSolution3">
<div class="accordion-body">
<p>Standard Otsu thresholding chooses a threshold that separates the
background from the three shapes:</p>
<figure><img src="../fig/otsu-shapes.png" alt="Mask of shapes (brown) overlaid on shapes image -  made with Otsu thresholding" class="figure mx-auto d-block"></figure><p>Li thresholding gives a very similar result.</p>
<p>Triangle also gives a similar result, but includes some isolated
pixels from the noisy background:</p>
<figure><img src="../fig/triangle-shapes.png" alt="Mask of shapes (brown) overlaid on shapes  image - made with triangle thresholding" class="figure mx-auto d-block"></figure><p>Yen gives a different result - isolating the triangle and circle from
the rest of the image. Some of the pixels in the rectangle are also
labelled, but only in patchy areas:</p>
<figure><img src="../fig/yen-shapes.png" alt="Mask of shapes (brown) overlaid on shapes image -  made with Yen thresholding" class="figure mx-auto d-block"></figure><p>Finally, the ‘Threshold Otsu, multiple thresholds’ option gives a
completely different result. Using ‘number of thresholds’ of 3, a ‘label
offset’ of 0 and ‘number of histogram bins’ of 256, we create a
segmentation where each shape gets its own label:</p>
<figure><img src="../fig/otsu-multiple-shapes.png" alt="Mask of shapes (brown) overlaid on shapes  image - made with multiple thresholds Otsu method" class="figure mx-auto d-block"></figure><p>The ‘Threshold Otsu, multiple thresholds’ option is an extension of
the standard Otsu thresholding method. This allows it to choose multiple
thresholds at once (rather than only one as standard). This is useful if
you have multiple different classes to segment in your images, that each
have a clear peak in the image histogram.</p>
<p>The important point is that different automatic thresholding methods
will work well for different kinds of images, and depending on which
part of an image you are trying to segment. It’s worth trying a variety
of options on your images to see which performs best for your specific
dataset.</p>
</div>
</div>
</div>
</div>
</section><section id="the-napari-assistant"><h2 class="section-heading">The Napari assistant<a class="anchor" aria-label="anchor" href="#the-napari-assistant"></a>
</h2>
<hr class="half-width">
<p>In this episode, we have directly applied methods from the
<code>Tools</code> menu to our images. It’s worth mentioning though that
these methods are also available via the ‘Napari assistant’.</p>
<p>First, let’s clean up our layer list again. Make sure you only have
the ‘nuclei’ layer in the layer list - select any others and remove them
by clicking the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/delete.svg" alt="A screenshot of Napari's delete layer button" height="30" class="figure">
icon. Also, close all settings panels on the right side of Napari by
clicking the tiny <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/visibility_off.svg" alt="A screenshot of Napari's hide button" height="20" class="figure"> icon at
their top left corner.</p>
<p>We can open the assistant with:<br><code>Tools &gt; Utilities &gt; Assistant (na)</code></p>
<figure><img src="../fig/napari-assistant.png" alt="Screenshot of the Napari assistant user  interface" class="figure mx-auto d-block"></figure><p>The assistant splits image processing operations into broad
categories such as ‘Remove noise’ and ‘Filter’. Let’s recreate our
workflow of a gaussian blur followed by otsu thresholding in the
assistant.</p>
<p>First, click the ‘Remove noise’ button in the assistant panel. In the
settings that appear below, select the ‘Operation’ as ‘Gaussian
(scikit-image, nsbatwm)’ and set the ‘sigma’ to three.</p>
<figure><img src="../fig/remove-noise-options.png" alt="Screenshot of settings for removing noise  in Napari" class="figure mx-auto d-block"></figure><p>This will create a new layer called ‘Result of Gaussian
(scikit-image, nsbatwm)’. If you can’t see it in the viewer, try
adjusting the contrast limits so the right node is at 1 (remember this
is a float image that runs from 0-1, as we discussed in an <a href="#thresholding-the-blurred-image">earlier section</a>).</p>
<p>With the ‘Result of Gaussian (scikit-image, nsbatwm)’ layer selected,
click the ‘Binarize’ button in the assistant panel. Change the
‘Operation’ to ‘Threshold (Otsu et al 1979, scikit-image, nsbatwm)’.</p>
<figure><img src="../fig/binarize-options.png" alt="Screenshot of settings for binarize in  Napari" class="figure mx-auto d-block"></figure><p>You should see a nuclei mask similar to the one we created
before.</p>
<p>Note that you can change the settings of a particular operation and
all the steps after it will also update. For example, select the ‘Result
of Gaussian (scikit-image, nsbatwm)’ layer again, then change the sigma
to 1. You should see the mask automatically updates, becoming more
patchy and noisy.</p>
<p>The napari assistant also has useful options to export and save
workflows for re-use. This is important to make our image processing
<em>reproducible</em> - it creates a clear record of every step that
produced the final result. This makes it easy for other researchers to
replicate our work and try the same methods on their own data. The
napari-assistant can export workflows as python scripts or jupyter
notebooks, which you can read more about in their <a href="https://www.napari-hub.org/plugins/napari-assistant" class="external-link">online
documentation</a>.</p>
</section><section id="summary"><h2 class="section-heading">Summary<a class="anchor" aria-label="anchor" href="#summary"></a>
</h2>
<hr class="half-width">
<p>To summarise, here are some of the main steps you can use to make a
mask:</p>
<ol style="list-style-type: decimal">
<li>Quality control</li>
<li>Blur the image</li>
<li>Threshold the image</li>
</ol>
<p>There are many variations on this standard workflow e.g. different
filters could be used to blur the image (or in general to remove noise),
different methods could be used to automatically find a good threshold…
There’s also a wide variety of post processing steps you could use to
make your mask as clean as possible. We’ll look at some of these options
in the next episode.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li><p>Thresholding is simply choosing a specific pixel value to use as
the cut-off between background and your class of interest.</p></li>
<li><p>Thresholds can be chosen manually based on the image
histogram</p></li>
<li><p>Thresholds can be chosen automatically with various methods
e.g. Otsu</p></li>
<li><p>Filters modify pixel values in an image by using a specific
‘kernel’</p></li>
<li><p>Gaussian filters are useful to blur an image before thresholding.
The ‘sigma’ controls the strength of the blurring effect.</p></li>
<li><p>Image processing operations may modify the data type of your
image - often converting it to <code>float32</code> or
<code>float64</code>.</p></li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-instance-segmentation-and-measurements"><p>Content from <a href="instance-segmentation-and-measurements.html">Instance segmentation and measurements</a></p>
<hr>
<p> Last updated on 2024-05-01 | 
        
        <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/edit/main/episodes/instance-segmentation-and-measurements.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 60 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do we perform instance segmentation in Napari?</li>
<li>How do we measure cell size with Napari?</li>
<li>How do we save our work to create re-usable workflows?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li><p>Use simple operations (like erosion and dilation) to clean up a
segmentation.</p></li>
<li><p>Use connected components labelling on a thresholded
image.</p></li>
<li><p>Calculate the number of cells and average cell volume.</p></li>
<li><p>Save and edit your workflow to re-use on subsequent
images.</p></li>
<li><p>Perform more complex cell shape analysis using the regionprops
plugin.</p></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>In this lesson we’ll continue to work with the Cells (3D + 2Ch) image
we’ve been using in past lessons. We will expand our use of Napari’s
Python console to perform the work and save our workflow to a Python
script. We chose to use the Python console for this lesson to further
develop skills in script writing and automating the image analysis
pipeline. Being able to use the console and save the script will help
with automation and repetition in later work.</p>
<p>We have kept the level of programming knowledge required to the
minimum possible and all code can be run by copy and pasting, so don’t
worry if you don’t understand it all yet. Most, if not all, of the
functions we will use in this lesson are also accessible via various
Napari plugins, so the analysis pipeline could also be assembled with
the Napari assistant if you prefer.</p>
<section id="before-you-begin"><h2 class="section-heading">Before you begin<a class="anchor" aria-label="anchor" href="#before-you-begin"></a>
</h2>
<hr class="half-width">
<p>We’ll be using the <a href="https://www.napari-hub.org/plugins/napari-skimage-regionprops" class="external-link">napari-skimage-regionprops</a>
plugin in this lesson. If it is not already installed you should do that
now. Use the tool bar to navigate to
<code>Plugins &gt; Install/Uninstall Plugins...</code>. Type
<code>region</code> into the filter bar at the top left and you should
see <code>napari-skimage-regionprops</code> in the dialog like the image
below. <img src="../fig/install-region-props.png" alt="A screenshot of the plugin installation dialog for napari-skimage-regionprops" class="figure">
If it is already installed, then nothing else needs to be done. If it is
not installed, press install, and when finished, restart Napari.</p>
</section><section id="loading-an-image-and-creating-a-mask"><h2 class="section-heading">Loading an image and creating a mask<a class="anchor" aria-label="anchor" href="#loading-an-image-and-creating-a-mask"></a>
</h2>
<hr class="half-width">
<p>We recommend starting a new session in Napari in order to make sure
the variable names in the console are correct. If you have come straight
from the last lesson the first few steps will be familiar, but can be
quickly repeated by copy and pasting into the console.</p>
<p>First, let’s open one of Napari’s sample images with:</p>
<p><code>File &gt; Open Sample &gt; napari builtins &gt; Cells (3D + 2Ch)</code></p>
<p>Open Napari’s console by pressing the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/console.svg" alt="A screenshot of Napari's console button" height="30" class="figure"> button,
then copy and paste the code below.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">from</span> skimage.filters <span class="im">import</span> threshold_otsu, gaussian</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>image <span class="op">=</span> viewer.layers[<span class="st">"nuclei"</span>].data</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>blurred <span class="op">=</span> gaussian(image, sigma<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>threshold <span class="op">=</span> threshold_otsu(blurred)</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>semantic_seg <span class="op">=</span> blurred <span class="op">&gt;</span> threshold</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>viewer.add_labels(semantic_seg)</span></code></pre>
</div>
<figure><img src="../fig/semantic-seg-napari.png" alt="A screenshot of a rough semantic segmentation of nuclei in Napari" class="figure mx-auto d-block"></figure><p>And you should see the image above. You are now ready to begin this
lesson.</p>
</section><section id="our-first-measurement"><h2 class="section-heading">Our first measurement<a class="anchor" aria-label="anchor" href="#our-first-measurement"></a>
</h2>
<hr class="half-width">
<p>We now have a mask image with each pixel classified as either cell
nuclei (pixel value 1) or not (pixel value 0). Try running the following
code in the console.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># We're going to need some functions from the Numpy library.</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co"># How many pixels are there in total in the image?</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>total_pixels <span class="op">=</span> semantic_seg.size</span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="co"># How many pixels are labelled as cell nuclei (pixel value = 1)?</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="co"># We'll use Numpy's count_nonzero method.</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>nuclei_pixels <span class="op">=</span> np.count_nonzero(semantic_seg)</span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a><span class="co"># Now we can work out what percentage of the image is cell nuclei</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>nuclei_percent <span class="op">=</span> nuclei_pixels <span class="op">/</span> total_pixels <span class="op">*</span> <span class="dv">100</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a><span class="co"># And write the results to the console with some formatting.</span></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a><span class="co"># Python's f-string format allows us to easily mix text and</span></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="co"># code in our print statements. The curly brackets contain</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a><span class="co"># code and the ":2f" provides formatting instructions, here</span></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a><span class="co"># telling Python to only print 2 decimal points.</span></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Percent Nuclei = </span><span class="sc">{</span>nuclei_percent<span class="sc">:.2f}</span><span class="ss">%"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Percent Nuclei = 19.47%</code></pre>
</div>
<p>Is knowing the percentage of pixels that are classed as nuclei
sufficient for our purposes? Thinking back to some of the research
questions we discussed in the <a href="designing-a-light-microscopy-experiment.html#define-your-research-question">episode
on designing an experiment</a> , if the percentage changes over time we
can infer that something is happening but what? We can’t say whether the
nuclei are changing in number or in size or shape. For most research
questions we will need a more informative measurement.</p>
<div id="saving-and-repeating-your-work" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="saving-and-repeating-your-work" class="callout-inner">
<h3 class="callout-title">Saving and repeating your work<a class="anchor" aria-label="anchor" href="#saving-and-repeating-your-work"></a>
</h3>
<div class="callout-content">
<p>Let’s assume that measuring percentage of nuclei is sufficient for
your research question. How do we automate and repeat this workflow on
new images? The Napari Python console has a built in save function.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># Save current session to a file called measure_percent.py</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="op">%</span>save measure_percent <span class="op">~</span><span class="dv">0</span><span class="op">/</span></span></code></pre>
</div>
<p>Delete the semantic_seg layer from the viewer and run:</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>load measure_percent.py</span></code></pre>
</div>
<p>After pressing enter you should see the calculated percent nuclei and
the semantic_seg layer should reappear. We will reuse the save function
at the end of this lesson.</p>
</div>
</div>
</div>
</section><section id="counting-the-nuclei"><h2 class="section-heading">Counting the nuclei<a class="anchor" aria-label="anchor" href="#counting-the-nuclei"></a>
</h2>
<hr class="half-width">
<p>We now need to count the number of nuclei in the image. We can use
the the <a href="https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.label" class="external-link">label</a>
function from scikit-image. The label function is an example of <a href="https://datacarpentry.org/image-processing/08-connected-components.html#connected-component-analysis" class="external-link">connected
component analysis</a> . Connected component analysis will go through
the entire image, determine which parts of the segmentation are
connected to each other and form separate objects. Then it will assign
each connected region a unique integer value. Let’s try it.</p>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Import the label function</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="im">from</span> skimage.measure <span class="im">import</span> label</span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="co"># Run the label function on the mask image</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>instance_seg <span class="op">=</span> label(semantic_seg)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a><span class="co"># Add the result to the viewer</span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>viewer.add_labels(instance_seg)</span></code></pre>
</div>
<p><img src="../fig/instance_segmentation_wrong.png" alt="A screenshot of an instance segmentation of nuclei with some incorrectly joined instances." class="figure">
You should see the above image in the Napari viewer. The different
colours are used to represent different nuclei. The instance
segmentation assigns a different integer value to each nucleus, so
counting the number of nuclei can be done very easily by taking the
maximum value of the instance segmentation image.</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="co"># Calculate number of nuclei from instance segmentation</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of Nuclei = </span><span class="sc">{</span>instance_seg<span class="sc">.</span><span class="bu">max</span>()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Number of Nuclei = 18</code></pre>
</div>
<p>We can reuse Numpy’s <code>count_nonzero</code> function on an
individual nucleus by specifying an integer value between 1 and 18</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="co"># How many pixels are there in nucleus 1</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>nucleus_id <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"There are </span><span class="sc">{</span>np<span class="sc">.</span>count_nonzero(instance_seg <span class="op">==</span> nucleus_id)<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a>      <span class="ss">f"pixels in nucleus </span><span class="sc">{</span>nucleus_id<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>There are 43945  pixels in nucleus 1</code></pre>
</div>
<p>Congratulations, you’ve measured the size (in pixels) of the first
nucleus. Later in this lesson, we’ll cover how to convert the size in
pixels to volume in cubic micrometres and how to get statistics on the
sizes of all the nuclei. Before we do that, we’ll use the <a href="https://www.napari-hub.org/plugins/napari-skimage-regionprops" class="external-link">napari-skimage-regionprops</a>
plugin to interactively examine the size and shape of individual
nuclei.</p>
</section><section id="using-napari-skimage-regionprops-plugin-to-measure-nuclei-size"><h2 class="section-heading">Using napari-skimage-regionprops plugin to measure nuclei size<a class="anchor" aria-label="anchor" href="#using-napari-skimage-regionprops-plugin-to-measure-nuclei-size"></a>
</h2>
<hr class="half-width">
<p>If you followed the instructions <a href="#before-you-begin">above</a> the napari-skimage-regionprops plugin
should already be installed. If not then do it now and restart Napari.
If the plugin is installed you can use the toolbar to open
<code>tools &gt; measurement tables &gt; Regionsprops(skimage, nsr)</code>.
You should see a dialog like this: <img src="../fig/region_props_before.png" alt="A screenshot of the napari-skimage-regionprops plugin at startup." class="figure"></p>
<p>Select <code>nuclei(data)</code> in the image drop down box and
<code>instance_seg(data)</code> in the labels drop down box. You can
choose to measure various shape properties with this plugin but for now
let’s keep it simple, making sure that only the <code>size</code> and
<code>position</code> tick boxes are selected. Click <code>run</code>. A
table of numeric values should appear under the plugin dialog box, like
the image below. <img src="../fig/region_props_after.png" alt="A screenshot of the numeric value table created by the napari-skimage-regionprops plugin" class="figure"></p>
<p>The regionprops plugin can generate a lot of information on the shape
and size of each connected region. You can use the horizontal scroll bar
to move across the table and see more. For now we will focus only on the
second column, headed <code>area</code>, which shows the size (in
pixels). Let’s look more closely at some the extreme values.</p>
<p>Let’s start with label 3 which is the largest labelled nucleus. <img src="../fig/region_props_after_3.png" alt="A screenshot of the region-props dialog highlighting the largest nucleus." class="figure">
According to the table, nucleus 3 is larger than the other nuclei
(202258 pixels). In the <a href="what-is-an-image.html#pixels">what is
an image</a> lesson, we learnt to use the mouse pointer to find
particular values in an image. Hovering the mouse pointer over the light
purple nuclei at the bottom left of the image we see that these
apparently four separate nuclei have been labelled as a single nucleus.
Before we examine the reasons for this we’ll look at the other extreme
value, the smallest nucleus.</p>
<p>The smallest nucleus is labelled 18, at the bottom of the table with
a size of 7 pixels. We can use the position data (the
<code>centroid</code> and <code>bbox</code> columns) in the table to
help find this nucleus. We need to navigate to slice 33 and get the
mouse near the top left corner (33 64 0) to find label 18 in the image.
<img src="../fig/region_props_after_18.png" alt="A screenshot region-props dialog highlighting the smallest nucleus." class="figure">
Nucleus 18 is right at the edge of the image, so is only a partial
nucleus. Partial nuclei will need to be excluded from our analysis.
We’ll do this later in the lesson with a <a href="#removing-border-cells">clear border</a> filter. However, first we
need to solve the problem of joined nuclei.</p>
</section><section id="separating-joined-nuclei"><h2 class="section-heading">Separating joined nuclei<a class="anchor" aria-label="anchor" href="#separating-joined-nuclei"></a>
</h2>
<hr class="half-width">
<p>Our first problem is how to deal with four apparently distinct nuclei
(labelled with a light purple colour) being segmented as a single
nucleus. Referring to the images above, three of the light purple nuclei
are visibly touching, so it is not surprising that they have been
considered as a single <code>connected component</code> and thus
labelled as a single nucleus. What about the fourth apparently separate
nucleus? It is important to remember that this is a three-dimensional
image and so pixels will be considered as “connected” if they are
adjacent to another segmented pixel in any of the three dimensions (and
not just in the two-dimensional slice that you are looking at).</p>
<p>You may remember from our <a href="imaging-software.html#d3d">first
lesson</a> that we can change to 3D view mode by pressing the <img src="https://raw.githubusercontent.com/napari/napari/main/napari/resources/icons/2D.svg" alt="Napari's 2D/3D toggle" height="30" class="figure"> button. Try it now.</p>
<p><img src="../fig/instance_segmentation_wrong3d.png" alt="A screenshot of an instance segmentation of nuclei in 3D mode with some incorrectly joined instances." class="figure">
You should see the image rendered in 3D, with a clear join between the
upper most light purple nucleus and its neighbour. So now we understand
why the instance labelling has failed, what can we do to fix it?</p>
<div id="erode-the-semantic-segmentation-so-all-nuclei-are-separate" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="erode-the-semantic-segmentation-so-all-nuclei-are-separate" class="callout-inner">
<h3 class="callout-title">Erode the semantic segmentation so all nuclei
are separate<a class="anchor" aria-label="anchor" href="#erode-the-semantic-segmentation-so-all-nuclei-are-separate"></a>
</h3>
<div class="callout-content">
<p>In order to use the label function to count the cell nuclei we first
need to make sure all the nuclei are separate. We can do this by
reducing the apparent size of the nuclei by eroding the image. Image
erosion is an image filter, similar to those we covered in the <a href="filters-and-thresholding.html">filters and thresholding</a>
lesson. We will use scikit-image’s <a href="https://scikit-image.org/docs/stable/api/skimage.morphology.html#skimage.morphology.binary_erosion" class="external-link">binary_erosion</a>
function. In this lesson we will run the binary erosion function using
the Napari console to help develop our scripting skills. It is also
possible to run the binary erosion function through a plugin:
<code>Tools &gt; Segmentation post-processing &gt; Binary erosion (scikit-image, nsbatwm)</code>
if you prefer.</p>
<p>The binary erosion function sets a pixel to the minimum value in the
neighbourhood defined by a <code>footprint</code> parameter. We’ll use
scikit-image’s <a href="https://scikit-image.org/docs/stable/api/skimage.morphology.html#skimage.morphology.ball" class="external-link">ball</a>
function to generate a sphere to use as the footprint. Image erosion has
the effect of making bright areas of the image smaller. In this case the
labelled (non-zero) nuclei will become smaller, as any pixels closer to
the edge of the nucleus than the radius of the footprint will be set to
zero. We can change the radius of the footprint to control the amount of
erosion. Try eroding the <code>semantic_seg</code> layer with different
integer values for the radius. What radius do you need to ensure all
nuclei are separate?</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">from</span> skimage.morphology <span class="im">import</span> binary_erosion, ball</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="co"># With radius = 1</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a>radius <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>eroded_mask <span class="op">=</span> binary_erosion(semantic_seg, footprint <span class="op">=</span> ball(radius))</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a>viewer.add_labels(eroded_mask, name <span class="op">=</span> <span class="ss">f'eroded ball </span><span class="sc">{</span>radius<span class="sc">}</span><span class="ss">'</span>)</span></code></pre>
</div>
<p>Note that larger radius values will take longer to run on your
computer. Keep your radius values &lt;= 10.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1">Show me the solution</h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p>To test different values of radius, you can assign a different value
to radius, e.g. <code>radius = 5</code> and rerun the last two lines
from above. Or you can try with a Python <a href="https://swcarpentry.github.io/python-novice-inflammation/05-loop.html" class="external-link">for
loop</a> which enables us to test multiple values of radius quickly.</p>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="co"># The for loop will repeat the indented lines of codes for each value</span></span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="co"># of radius in the list (1, 5, 10).</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="cf">for</span> radius <span class="kw">in</span> <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">10</span>:</span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a>  eroded_mask <span class="op">=</span> binary_erosion(semantic_seg, footprint <span class="op">=</span> ball(radius))</span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a>  viewer.add_labels(eroded_mask, name <span class="op">=</span> <span class="ss">f'eroded ball </span><span class="sc">{</span>radius<span class="sc">}</span><span class="ss">'</span>)</span></code></pre>
</div>
<p><img src="../fig/binary_mask_no_erosion.png" alt="A screenshot of a semantic segmentation mask before erosion." class="figure">
The first image shows the mask without any erosion for comparison.</p>
<p><img src="../fig/binary_mask_erosion_1.png" alt="A screenshot of a semantic segmentation mask eroded with a ball of radius 1." class="figure">
Erosion with a radius of 1 makes a small difference, but the nuclei
remain joined.</p>
<p><img src="../fig/binary_mask_erosion_5.png" alt="A screenshot of a semantic segmentation mask eroded with a ball of radius 5." class="figure">
Erosion with a radius of 5 makes a more noticeable difference, but some
nuclei remain joined.</p>
<p><img src="../fig/binary_mask_erosion_10.png" alt="A screenshot of a semantic segmentation mask eroded with a ball of radius 10." class="figure">
Erosion with a radius of 10 separates all nuclei.</p>
<p>An alternative to performing a single large erosion (radius = 10) is
to perform a small erosion (radius = 1) 10 times. Doing this will give
subtly different results. As an extra activity you could try doing this
using a <code>for loop</code> and comparing the results?</p>
</div>
</div>
</div>
</div>
<p>Now we have separate nuclei, lets try creating instance labels
again.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># Remove the incorrect instance segmentation from the viewer</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>viewer.layers.remove(<span class="st">'instance_seg'</span>)</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>eroded_semantic_seg <span class="op">=</span> viewer.layers[<span class="st">'eroded ball 10'</span>].data</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a><span class="co"># Create a new instace segmentation using the eroded mask</span></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>instance_seg <span class="op">=</span> label(eroded_semantic_seg)</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a>viewer.add_labels(instance_seg)</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of nuclei after erosion  = </span><span class="sc">{</span>instance_seg<span class="sc">.</span><span class="bu">max</span>()<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Number of nuclei after erosion  = 19</code></pre>
</div>
<p><img src="../fig/instance_segmentation_eroded.png" alt="Instance segmentation on the eroded segmentation mask" class="figure"> Looking
at the image above, there are no longer any incorrectly joined nuclei.
The absolute number of nuclei found hasn’t changed much as the erosion
process has removed some partial nuclei around the edges of the
image.</p>
<p>Performing any size or shape analysis on these nuclei will be flawed,
as they are heavily eroded. We can largely undo much of the erosion by
using the scikit-image’s <a href="https://scikit-image.org/docs/stable/api/skimage.segmentation.html#skimage.segmentation.expand_labels" class="external-link">expand
labels</a> function. The expand labels function is a filter which
performs a <code>dilation</code> , expanding the bright (non-zero) parts
of the image. The expand labels function adds an extra step to stop the
dilation when two neighbouring labels meet, preventing overlapping
labels.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="im">from</span> skimage.segmentation <span class="im">import</span> expand_labels</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="co"># Remove the eroded instance segmentation from the viewer</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a>viewer.layers.remove(<span class="st">'instance_seg'</span>)</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="co"># Expand the labels using the same radius we used when eroding them (10)</span></span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a>instance_seg <span class="op">=</span> expand_labels(instance_seg, <span class="dv">10</span>)</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a><span class="co"># Put the new instance segmentation back in the viewer</span></span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a>viewer.add_labels(instance_seg)</span></code></pre>
</div>
<p><img src="../fig/instance_segmentation_expanded.png" alt="Expanded instance segmentation on the eroded segmentation mask" class="figure">
There are now 19 apparently correctly labelled nuclei that appear to be
the same shape as in the original mask image.</p>
<div id="is-the-erosion-completely-reversible" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="is-the-erosion-completely-reversible" class="callout-inner">
<h3 class="callout-title">Is the erosion completely reversible?<a class="anchor" aria-label="anchor" href="#is-the-erosion-completely-reversible"></a>
</h3>
<div class="callout-content">
<p>In order to create a correct instance segmentation we have performed
a mask erosion followed by a label expansion. This is a common image
operation often used to remove background noise, known as as
<code>opening</code>, or an erosion followed by a dilation. In addition
to helping us separate instances it will have the effect of removing
objects smaller than the erosion footprint, in this case a sphere with
radius 10 pixels. If we compare the eroded and expanded image with the
original mask, what will we see?</p>
</div>
</div>
</div>
<div id="accordionSolution2" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution2" aria-expanded="false" aria-controls="collapseSolution2">
  <h4 class="accordion-header" id="headingSolution2">Show me the solution</h4>
</button>
<div id="collapseSolution2" class="accordion-collapse collapse" aria-labelledby="headingSolution2" data-bs-parent="#accordionSolution2">
<div class="accordion-body">
<p><img src="../fig/instance_segmentation_vs_semantic_segmentation.png" alt="A comparison between the expanded instance segmentation and the original semantic segmentation showing some mismatch between the borders." class="figure">
Looking at the above image we can see some small mismatches around the
edges of most of the nuclei. It should be remembered when looking at
this image that it is a single slice though a 3D image, so in some cases
where the differences look large (for example the nucleus at the bottom
right) they may still be only one pixel deep. Will the effect of this on
the accuracy of our results be significant?</p>
</div>
</div>
</div>
</div>
</section><section id="removing-border-cells"><h2 class="section-heading">Removing Border Cells<a class="anchor" aria-label="anchor" href="#removing-border-cells"></a>
</h2>
<hr class="half-width">
<p>Now we return to the second problem with our initial instance
segmentation, the presence of partial nuclei around the image borders.
As we’re measuring nuclei size, the presence of any partially visible
nuclei could substantially bias our statistics. We can remove these from
our analysis using scikit-image’s <a href="https://scikit-image.org/docs/stable/api/skimage.segmentation.html#skimage.segmentation.clear_border" class="external-link">clear
border</a> function.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="im">from</span> skimage.segmentation <span class="im">import</span> clear_border</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a>viewer.layers.remove(<span class="st">'instance_seg'</span>)</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a>instance_seg <span class="op">=</span> clear_border(instance_seg)</span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a>viewer.add_labels(instance_seg)</span></code></pre>
</div>
<p><img src="../fig/instance_segmentation_clear_border.png" alt="The instance segmentation with any nuclei crossing the image boundary removed" class="figure">
We now have an image with 11 clearly labelled nuclei. You may notice
that the smaller nucleus (dark orange) near the top left of the image
has been removed even though we can’t see where it touches the image
border. Remember that this is a 3D image and clear border removes nuclei
touching any border. This nucleus has been removed because it touches
the top or bottom (z axis) of the image. Let’s check the nuclei count as
we did above.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co"># First count the nuclei</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>number_of_nuclei <span class="op">=</span> instance_seg.<span class="bu">max</span>()</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"There are </span><span class="sc">{</span>number_of_nuclei<span class="sc">}</span><span class="ss"> individual nuclei"</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>There are 19 individual nuclei</code></pre>
</div>
<p>Why are there still 19 nuclei? When we ran <code>clear_borders</code>
the pixels corresponding to border nuclei were set to zero, however the
total number of labels in the image was not changed, so whilst there are
19 labels in the image some of them have no corresponding pixels. The
easiest way to correct this is to re label the image (and replace the
old instance segmentation in the viewer.)</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>viewer.layers.remove(<span class="st">'instance_seg'</span>)</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>instance_seg <span class="op">=</span> label(instance_seg)</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a>number_of_nuclei <span class="op">=</span> instance_seg.<span class="bu">max</span>()</span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"There are </span><span class="sc">{</span>number_of_nuclei<span class="sc">}</span><span class="ss"> individual nuclei"</span>)</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>viewer.add_labels(instance_seg)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>There are 11 individual nuclei</code></pre>
</div>
<p>You now have a correct instance segmentation. You could return to
using the napari-skimage-regionprops plugin to calculate the sizes of
each nucleus and export the results to a speadsheet or your preferred
analysis software using the <code>save as csv</code> function. However
you’ve probably picked up enough Python during this course to complete
the analysis you need with just the Napari console. Let’s give it a try.
The following commands should work with copy and paste, so don’t worry
too much if you don’t think you’ve quite mastered Python for loops
yet.</p>
<p>Earlier in the lesson we used a Python <a href="https://swcarpentry.github.io/python-novice-inflammation/05-loop.html" class="external-link">for
loop</a> to try out three different erosion radii. Now let’s use a for
loop to count the number of pixels in each of the 11 nuclei. For this
we’ll take advantage of Python’s <code>range</code> function which
returns a list of numbers.</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a><span class="co"># Create a list of label values for each label in the instance segmentation</span></span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="co"># We need to add 1 to the number of nuclei as python's range function</span></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a><span class="co"># includes the lower limit (1) but not the upper limit.</span></span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>labels <span class="op">=</span> <span class="bu">range</span>(<span class="dv">1</span>, number_of_nuclei <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a><span class="co"># Print it to the console to check it's right</span></span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> labels:</span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>  <span class="bu">print</span> (label)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code><span><span class="fl">1</span></span>
<span><span class="fl">2</span></span>
<span><span class="fl">3</span></span>
<span><span class="fl">4</span></span>
<span><span class="fl">5</span></span>
<span><span class="fl">6</span></span>
<span><span class="fl">7</span></span>
<span><span class="fl">8</span></span>
<span><span class="fl">9</span></span>
<span><span class="fl">10</span></span>
<span><span class="fl">11</span></span></code></pre>
</div>
<p>Earlier in the lesson we used Numpy’s <code>count_nonzero</code>
function to find the size of a single nucleus. Let’s put it inside the
for loop to find the size of every nucleus. We’ll use a Python list to
store the results.</p>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="co"># Create an empty list</span></span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>nucleus_pixels <span class="op">=</span> []</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a><span class="co"># Go through each nucleus,</span></span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a><span class="cf">for</span> label <span class="kw">in</span> labels:</span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a>  <span class="co"># Append the number of pixels to the list</span></span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a>  nucleus_pixels.append(np.count_nonzero(instance_seg <span class="op">==</span> label))</span></code></pre>
</div>
<p>We now have a list of nuclei sizes in pixels. We can check what is in
the list with a print statement.</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="bu">print</span>(nucleus_pixels)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>[40960, 26847, 60268, 46545, 51409, 46866, 53108, 37869, 36523, 40809, 14548]</code></pre>
</div>
<p>We see 11 values within the list. Each value tells us the number of
pixels in a particular nucleus. Before we do too much analysis on the
nuclei sizes we should convert them to a physical value, rather than
pixels.</p>
</section><section id="size-in-pixels-to-cell-volume"><h2 class="section-heading">Size in pixels to cell volume<a class="anchor" aria-label="anchor" href="#size-in-pixels-to-cell-volume"></a>
</h2>
<hr class="half-width">
<p>To convert to volumes we need to know the pixel size. In the lesson
on <a href="filetypes-and-metadata.html#pixel-size">filetypes and
metadata</a> we learnt how to inspect the image metadata to determine
the pixel size. Unfortunately the sample image we’re using in this
lesson has no metadata. Fortunately the image pixel sizes can be found
in the <a href="https://scikit-image.org/docs/stable/api/skimage.data.html#skimage.data.cells3d" class="external-link">scikit-image
documentation</a> . So we can assign a pixel size of 0.26μm (x axis),
0.26μm (y axis) and 0.29μm (z axis). Using this pixel size, we can then
calculate the nucleus volume in cubic micrometres.</p>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="co"># Let's keep everything in micrometres</span></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>pixel_volume <span class="op">=</span> <span class="fl">0.26</span> <span class="op">*</span> <span class="fl">0.26</span> <span class="op">*</span> <span class="fl">0.29</span></span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a><span class="co"># We can multiply all nuclei by the pixel volume by first converting the</span></span>
<span id="cb26-5"><a href="#cb26-5" tabindex="-1"></a><span class="co"># nucleus_pixels to a numpy array.</span></span>
<span id="cb26-6"><a href="#cb26-6" tabindex="-1"></a>nucleus_volume <span class="op">=</span> pixel_volume <span class="op">*</span> np.array(nucleus_pixels)</span></code></pre>
</div>
<p>We can now use some simple functions to find the range, mean, and
standard deviation of the nuclei in micrometers.</p>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a></span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a><span class="co"># Find the range of nucleus sizes (maximum - minimum).</span></span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Range of nucleus volumes = </span><span class="sc">{</span>nucleus_volume<span class="sc">.</span><span class="bu">max</span>() <span class="op">-</span> nucleus_volume<span class="sc">.</span><span class="bu">min</span>()<span class="sc">:.2f}</span><span class="ss"> cubic micrometres."</span>)</span>
<span id="cb27-4"><a href="#cb27-4" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" tabindex="-1"></a><span class="co"># Find the mean nuclei volume</span></span>
<span id="cb27-6"><a href="#cb27-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Nucleus volume mean = </span><span class="sc">{</span>np<span class="sc">.</span>mean(nucleus_volume)<span class="sc">:.2f}</span><span class="ss"> cubic micrometres."</span>)</span>
<span id="cb27-7"><a href="#cb27-7" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" tabindex="-1"></a><span class="co"># And the standard deviation</span></span>
<span id="cb27-9"><a href="#cb27-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Nucleus volume standard dev. = </span><span class="sc">{</span>np<span class="sc">.</span>std(nucleus_volume)<span class="sc">:.2f}</span><span class="ss"> cubic micrometres."</span>)</span></code></pre>
</div>
<div class="codewrapper">
<h3 class="code-label">OUTPUT<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="output" tabindex="0"><code>Range of nucleus volumes = 579.10 cubic micrometres.
Nucleus volume mean = 855.98 cubic micrometres.
Nucleus volume standard dev. = 170.19 cubic micrometres.</code></pre>
</div>
<p>These numbers provide a good quantitative measure of the quantity and
volume of cell nuclei suitable for an experiment investigating how these
quantities change over time.</p>
<p>We can save our work from the console for re-use on data from
subsequent time points, creating a repeatable measurement workflow.</p>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="op">%</span>save measurement_workflow <span class="op">~</span><span class="dv">0</span><span class="op">/</span></span></code></pre>
</div>
<p>This will create a Python file <code>measurement_pipepine.py</code>
that we can load into the Napari console and re-run. You may choose to
edit the file with any text editor to remove some of the redundant steps
we’ve made whilst learning.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points<a class="anchor" aria-label="anchor" href="#keypoints1"></a>
</h3>
<div class="callout-content">
<ul>
<li>Connected component analysis (the label function) was used to assign
each connected region of a mask a unique integer value. This produces an
instance segmentation from a semantic segmentation.</li>
<li>Erosion and dilation filters were used to correct the instance
segmentation. Erosion was used to separate individual nuclei. Dilation
(or expansion) was used to return the nuclei to their (approximate)
original size.</li>
<li>Partial nuclei at the image edges can be removed with the
clear_border function.</li>
<li>The napari-skimage-regionprops plugin can be used to interactively
examine the nuclei shapes.</li>
<li>The Python console can be used to automate and save the image
analysis pipeline.</li>
</ul>
</div>
</div>
</div>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</section></section><section id="aio-additional-resources"><p>Content from <a href="additional-resources.html">Additional resources</a></p>
<hr>
<p> Last updated on 2024-03-07 | 
        
        <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/edit/main/episodes/additional-resources.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time <i aria-hidden="true" data-feather="clock"></i> 10 minutes </p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right"> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>Where can I find further information?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Find additional resources for image analysis / microscopy
online</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section id="additional-resources"><h2 class="section-heading">Additional resources<a class="anchor" aria-label="anchor" href="#additional-resources"></a>
</h2>
<hr class="half-width">
<p>If you want to learn more about image processing and microscopy,
we’ve linked some useful resources below:</p>
<div class="section level3">
<h3 id="bioimage-analysis">Bioimage analysis<a class="anchor" aria-label="anchor" href="#bioimage-analysis"></a>
</h3>
<ul>
<li><p><a href="https://forum.image.sc/" class="external-link">image.sc forum</a><br>
A general forum for image analysis advice, covering many popular pieces
of software e.g. ImageJ, Napari, QuPath and many more.</p></li>
<li><p><a href="https://bioimagebook.github.io/" class="external-link">Pete Bankhead’s
bioimage book</a><br>
A free, online book providing in-depth discussion of biological image
analysis. A great resource for delving deeper into the principles
discussed during this course.</p></li>
<li><p><a href="https://datacarpentry.org/image-processing/" class="external-link">Data
Carpentry: Image Processing with Python course</a><br>
An online course covering image analysis with python. This is
particularly useful if you want to start writing your own scripts to use
with Napari.</p></li>
<li><p><a href="https://www.bioimagingguide.org" class="external-link">Bioimaging
guide</a><br>
An online guide covering principles for design of quantitative
microscopy experiments, as well as how to analyse the resulting image
data.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PL5ESQNfM5lc7SAMstEu082ivW4BDMvd0U" class="external-link">Robert
Haase’s lectures on bioimage analysis</a><br>
Recordings of Robert Haase’s 2020 lectures on bioimage analysis. Useful
for learning general image processing principles, in various pieces of
software e.g. ImageJ, python, QuPath…</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="napari">Napari<a class="anchor" aria-label="anchor" href="#napari"></a>
</h3>
<ul>
<li><p><a href="https://napari.org" class="external-link">Napari documentation</a><br>
Napari’s main documentation website, including installation instructions
and tutorials.</p></li>
<li><p><a href="https://scads.github.io/napari-tutorial-2023/intro.html" class="external-link">Napari
tutorial from ScaDS.AI</a><br>
Online tutorial showing use of Napari from jupyter notebooks, with
various plugins.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="microscopy">Microscopy<a class="anchor" aria-label="anchor" href="#microscopy"></a>
</h3>
<ul>
<li><p><a href="https://www.microscopyu.com/" class="external-link">Nikon’s Microscopy
U</a><br>
Website with articles on microscopy principles, covering many different
types of microscope.</p></li>
<li><p><a href="https://www.nature.com/articles/s41596-020-0313-9" class="external-link">Tutorial:
guidance for quantitative confocal microscopy</a><br>
Useful paper giving an overview of how to design and setup a
quantitative light microscopy experiment.</p></li>
<li><p><a href="https://www.youtube.com/playlist?list=PLoemaChWEBWHINjLnWFIFOZKTE13mm3ac" class="external-link">Royal
Microscopical Society technical tea break talks</a><br>
Recorded short talks from the Royal Microscopical Society on various
microscopy principles.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="talksseminars-about-image-processing">Talks/seminars about image processing<a class="anchor" aria-label="anchor" href="#talksseminars-about-image-processing"></a>
</h3>
<ul>
<li><p><a href="https://www.ucl.ac.uk/lmcb/ucl-biig" class="external-link">UCL Bioimage
interest group</a><br>
Monthly talks (in person and online) about image analysis of biological
images. Open to all.</p></li>
<li><p><a href="https://www.eurobioimaging.eu/about-us/virtual-pub" class="external-link">Eurobioimaging
virtual pub</a><br>
Weekly online talks on various topics related to microscopy and image
analysis. Open to all.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="python">Python<a class="anchor" aria-label="anchor" href="#python"></a>
</h3>
<ul>
<li>
<a href="https://swcarpentry.github.io/python-novice-inflammation/" class="external-link">Software
carpentry python course</a><br>
Online course providing an introduction to the python programming
language.</li>
</ul>
</div>
<div class="section level3">
<h3 id="statistics">Statistics<a class="anchor" aria-label="anchor" href="#statistics"></a>
</h3>
<ul>
<li><p><a href="https://www.nature.com/collections/qghhqm/pointsofsignificance" class="external-link">Nature’s
‘Points of significance’ series</a><br>
A series of articles covering basic principles of statistics.</p></li>
<li><p><a href="https://www.huber.embl.de/msmb/" class="external-link">Modern statistics for
modern biology book</a><br>
A free online book covering statistics for biology in depth.</p></li>
</ul>
<!-- 
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use. 
 -->
</div>
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
				<p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>
        
        <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/edit/main/README.md" class="external-link">Edit on GitHub</a>
        
	
        | <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a> 
        | <a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/" class="external-link">Source</a></p>
				<p><a href="https://github.com/HealthBioscienceIDEAS/microscopy-novice/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">
        
        <p>Materials licensed under <a href="../LICENSE.html">CC-BY 4.0</a> by the authors</p>
        
        <p><a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">Template licensed under CC-BY 4.0</a> by <a href="https://carpentries.org" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/HealthBioscienceIDEAS/sandpaper/tree/IDEAS" class="external-link">sandpaper (0.16.4)</a>,
        <a href="https://github.com/carpentries/pegboard/tree/a32a7836d4455f407c3cafe8ab95edc636e5e919" class="external-link">pegboard (0.7.5)</a>,
      and <a href="https://github.com/HealthBioscienceIDEAS/varnish/tree/IDEAS" class="external-link">varnish (0.3.3.9000)</a>.</p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
			<i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back to top"></i><br><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://HealthBioscienceIDEAS.github.io/microscopy-novice/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://HealthBioscienceIDEAS.github.io/microscopy-novice/instructor/aio.html",
  "identifier": "https://HealthBioscienceIDEAS.github.io/microscopy-novice/instructor/aio.html",
  "dateCreated": "2023-10-26",
  "dateModified": "2024-05-14",
  "datePublished": "2024-05-14"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo
    2022-11-07: we have gotten a notification that we have an overage for our
    tracking and I'm pretty sure this has to do with Workbench usage.
    Considering that I am not _currently_ using this tracking because I do not
    yet know how to access the data, I am turning this off for now.
  <script>
    var _paq = window._paq = window._paq || [];
    /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
    _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
    _paq.push(["setDomains", ["*.preview.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
    _paq.push(["setDoNotTrack", true]);
    _paq.push(["disableCookies"]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
          var u="https://carpentries.matomo.cloud/";
          _paq.push(['setTrackerUrl', u+'matomo.php']);
          _paq.push(['setSiteId', '1']);
          var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
          g.async=true; g.src='https://cdn.matomo.cloud/carpentries.matomo.cloud/matomo.js'; s.parentNode.insertBefore(g,s);
        })();
  </script>
  End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

